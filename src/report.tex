\documentclass[10pt,oneside]{report}

% Encoding and font packages
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{multicol}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[nottoc]{tocbibind} % If you don't want the main TOC itself listed
\usepackage{setspace}
\onehalfspacing
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}
\usepackage[numbers]{natbib}
\renewcommand{\citet}[1]{\citeauthor{#1}, \citeyear{#1}}

\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes}
\usepackage{adjustbox}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{tabularx}
\usepackage{anyfontsize}
\usepackage{lmodern}
% Dynamically scale font sizes (80% of original)
\let\oldsmall\small
\renewcommand{\small}{\fontsize{7}{8}\selectfont}
\let\oldnormalsize\normalsize
\renewcommand{\normalsize}{\fontsize{8}{10}\selectfont}
\let\oldlarge\large
\renewcommand{\large}{\fontsize{10}{12}\selectfont}
\let\oldLarge\Large
\renewcommand{\Large}{\fontsize{11.5}{13.8}\selectfont}
\let\oldLARGE\LARGE
\renewcommand{\LARGE}{\fontsize{13}{15.6}\selectfont}
\let\oldhuge\huge
\renewcommand{\huge}{\fontsize{16}{19.2}\selectfont}
\let\oldHuge\Huge
\renewcommand{\Huge}{\fontsize{20}{24}\selectfont}
\usepackage{subcaption}
\usepackage[font={normalsize},labelfont={bf}]{caption}

\DeclareCaptionType{equ}[][]
%\captionsetup[equ]{labelformat=empty}

\usepackage{color}
\usepackage{listings}
\usepackage{lipsum}


% Your provided environment setup:
\newcounter{nalg}[chapter] % defines algorithm counter for chapter-level
\renewcommand{\thenalg}{\thechapter .\arabic{nalg}} %defines appearance of the algorithm counter
\DeclareCaptionLabelFormat{algocaption}{Algorithm \thenalg} % defines a new caption label as Algorithm x.y
\lstnewenvironment{algorithm}[1][] %defines the algorithm listing environment
{
    \refstepcounter{nalg} %increments algorithm number
    \captionsetup{labelformat=algocaption,labelsep=colon} %defines the caption setup
    \lstset{ %this is the style
        mathescape=true, % Allows math mode with $...$
        frame=tB, % Top and bottom frame lines
        numbers=left,
        numberstyle=\tiny,
        basicstyle=\fontsize{6}{7}\ttfamily, % Use typewriter font for code feel
        keywordstyle=\color{black}\bfseries, % Keywords bold (removed \em)
        keywords={input, output, return, function, in, if, else, foreach, for, while, begin, end, then, repeat, until, compute, select, assign, perform, update}, % Added more keywords
        commentstyle=\color{gray}\itshape, % Style for comments
        morecomment=[l]{\#}, % Define # as line comment start
        breaklines=true, % Allow line breaks
        breakatwhitespace=true,
        numbers=left,
        xleftmargin=.04\textwidth,
        #1 % this is to add specific settings to an usage of this environment
    }
}
{} % End of environment definition
% Begin document
\begin{document}

% -------------------------
% Title Page
% -------------------------
\begin{titlepage}
    \centering
    % Title (two lines)
    {\fontsize{12}{14.6}\selectfont Auto-Categorisation of ISIS Neutron and Muon Source Fault Logs\\\par}
    \vspace{2cm}
    % Subtitle/description
    {\large A report submitted to the University of Manchester for the degree of
    Bachelor's of Science in the Faculty of Science and Engineering\par}
    \vspace{2cm}
    \vspace{1cm}

    {\large Author: Sai Putravu\par}
    {\large Student ID: 10829976\par}
    {\large Supervisor: Professor Thomas Thomson\par}
    \vfill
    % Department
    % Date
    {\large 2025\par}
    \vspace{1cm}
    {\large School of Computer Science\par}
\end{titlepage}

% Roman page numbering for preliminary pages
\pagenumbering{roman}


% \chapter*{Abstract}
% The ISIS Neutron and Muon Source is a large-scale research facility requiring complex machinery and sophisticated maintenance strategies to ensure operational availability for international scientific users \cite{thomason2019isis}. Predictive maintenance (PdM) policies offer potential for optimising maintenance and reducing costly downtime \cite{carvalho2019systematic, mobley2002introduction}, but leveraging the facility's text-based operational fault log (Operalog) is hindered by its unstructured nature \cite{nota2022text}. This project addresses the challenge of automatically categorising a subsection of log entries, the ion source failures, by developing and evaluating an unsupervised machine learning pipeline based on modern Natural Language Processing (NLP) techniques. The pipeline involves: (1) pre-processing of the \texttt{FaultDescription} field; (2) generating semantic vector representations using state-of-the-art sentence embedding models (MPNet \cite{song2020mpnet} and Nomic \cite{nussbaum2024nomic}); (3) applying dimensionality reduction (UMAP \cite{mcinnes2018umap}); (4) performing unsupervised clustering (k-Medoids \cite{kmedoids}, DBSCAN \cite{ester1996density}, HDBSCAN \cite{campello2013density}); (5) optimising hyperparameters using Optuna \cite{akiba2019optuna} with multiple clustering validity heuristics \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}; and (6) automatically generating descriptive cluster labels using SpaCy \cite{spacy2}. 
%
% The optimisation process, favouring HDBSCAN \cite{campello2013density} clustering on UMAP-reduced \cite{mcinnes2018umap} Nomic \cite{nussbaum2024nomic} embeddings with default text pre-processing, suggested between 5 and 25 distinct fault categories within the ion source data.

\chapter*{Abstract}

The ISIS Neutron and Muon Source is a large-scale research facility requiring complex machinery and sophisticated maintenance strategies to ensure operational availability for international scientific users. Predictive maintenance (PdM) policies offer potential for optimising maintenance and reducing costly downtime, but leveraging the facility's text-based operational fault log (Operalog) is hindered by its unstructured nature. This project addresses the challenge of automatically categorising a subsection of log entries, the ion source failures, by developing and evaluating an unsupervised machine learning pipeline based on modern Natural Language Processing (NLP) techniques. The pipeline involves: (1) pre-processing of the \texttt{FaultDescription} field; (2) generating semantic vector representations using state-of-the-art sentence embedding models (MPNet and Nomic); (3) applying dimensionality reduction (UMAP); (4) performing unsupervised clustering (k-Medoids, DBSCAN, HDBSCAN); (5) optimising hyperparameters using Optuna with multiple clustering validity heuristics; and (6) automatically generating descriptive cluster labels using SpaCy. 

The optimisation process identified HDBSCAN clustering on UMAP-reduced Nomic embeddings (favouring Euclidean distance and 5-6 components) with default text pre-processing as a particularly effective configuration, suggesting between 5 and 25 distinct fault categories within the ion source data.

Given the complexity and novelty of this task, this work focuses on demonstrating the feasibility of this approach and represents significant progress in developing tools for analysing specialised industrial logs. The primary contribution is the pipeline methodology itself. Future work is essential for refining the automatically generated labels to maximise their operational utility. This research paves the way for enhanced data-driven predictive maintenance at complex scientific facilities.

\chapter*{Acknowledgements}
I would like to express my sincere gratitude to my supervisor, Professor Thomas Thomson, for his invaluable support throughout this project. His excellent advice and guidance were instrumental in shaping this work and motivating me during the research process.

I would also like to thank my parents for their unwavering support during my university years.

% -------------------------
% Table of Contents
% -------------------------
\tableofcontents
\clearpage

% -------------------------
% List of Figures
% -------------------------
\listoffigures
\clearpage

% -------------------------
% List of Tables
% -------------------------
\listoftables
\clearpage


% -------------------------
% Abbreviations and Acronyms
% -------------------------
% \phantomsection
\addcontentsline{toc}{chapter}{Abbreviations and Acronyms}
\chapter*{Abbreviations and Acronyms}

\begin{multicols}{2}
\begin{description}
    % Using \item[<Term>] <Definition> format
    \small

    \item[\textbf{AE}] Auto-encoding.
    \item[\textbf{AR}] Auto-regressive.
    \item[\textbf{BERT}] Bidirectional Encoder Representation from Transformers.
    \item[\textbf{CBM}] Condition-based maintenance policy.
    \item[\textbf{CLI}] Command Line Interface.
    \item[\textbf{CNN}] Convolutional Neural Network.
    \item[\textbf{CPU}] Central Processing Unit.
    \item[\textbf{CUDA}] Compute Unified Device Architecture.
    \item[\textbf{DBSCAN}] Density-Based Spatial Clustering of Application with Noise.
    \item[\textbf{DL}] Deep Learning.
    \item[\textbf{ELMo}] Embeddings from Language Models.
    \item[\textbf{EPB1}] Extracted proton beam line 1.
    \item[\textbf{EPB2}] Extracted proton beam line 2.
    \item[\textbf{F4A}] Free-For-All.
    \item[\textbf{FAP}] Fault Analysis Pathway.
    \item[\textbf{FLD}] First-line diagnosis system.
    \item[\textbf{GPT}] Generative Pre-trained Transformer.
    \item[\textbf{GPU}] Graphics Processing Unit.
    \item[\textbf{H2H}] Head-to-Head.
    \item[\textbf{HDBSCAN}] Hierarchical Density-Based Spatial Clustering of Application with Noise.
    \item[\textbf{HDD}] Hard-Disk Drive.
    \item[\textbf{HEDS}] High energy drift space.
    \item[\textbf{IoT}] Internet of Things.
    \item[\textbf{LEBT}] Low energy beam transport line.
    \item[\textbf{LINAC}] Linear Accelerator.
    \item[\textbf{LSA}] Latent Semantic Analysis.
    \item[\textbf{MCR}] Main Control Room.
    \item[\textbf{ML}] Machine Learning.
    \item[\textbf{MLM}] Masked Language Modelling.
    \item[\textbf{MRL}] Matryoshka Representation Learning.
    \item[\textbf{MST}] Minimum Spanning Tree.
    \item[\textbf{MTEB}] Massive Text Embedding Benchmark.
    \item[\textbf{NLP}] Natural Language Processing.
    \item[\textbf{Operalog}] The ISIS Operational Log.
    \item[\textbf{PAM}] Partitioning Around Medoids.
    \item[\textbf{PCA}] Principle Component Analysis.
    \item[\textbf{PdM}] Predictive maintenance policy.
    \item[\textbf{PLM}] Pre-trained Language Model.
    \item[\textbf{PvM}] Preventative maintenance policy.
    \item[\textbf{R2F}] Run-to-failure maintenance policy.
    \item[\textbf{RAM}] Random Access Memory.
    \item[\textbf{RFQ}] Radio-frequency quadrupole.
    \item[\textbf{RF}] Random Forest.
    \item[\textbf{RoPE}] Rotary Positional Embeddings.
    \item[\textbf{SAFE}] Supervised Aggregative Feature Extraction.
    \item[\textbf{SMART}] Self-monitoring and reporting technology.
    \item[\textbf{STFC}] Science and Technology Facilities Council.
    \item[\textbf{SVD}] Singular Value Decomposition.
    \item[\textbf{SVM}] Support Vector Machine.
    \item[\textbf{TS-1}] Target Station 1.
    \item[\textbf{TS-2}] Target Station 2.
    \item[\textbf{t-SNE}] t-distributed Stochastic Neighbour Embedding.
    \item[\textbf{UKRI}] United Kingdom Research and Innovation.
    \item[\textbf{UMAP}] Uniform Manifold Approximation and Projection.
    \item[\textbf{VRAM}] Video Random Access Memory.

\end{description}
\end{multicols}

% --- End of List ---

\clearpage

% Switch to Arabic numbering after preliminary pages
\pagenumbering{arabic}

% -------------------------
% Chapters
% -------------------------

\chapter{Introduction}

The Science and Technology Facilities Council's (STFC) ISIS Neutron and Muon Source, located at the Rutherford Appleton Laboratory, is a research centre enabling studies across physical and life sciences by producing beams of neutrons and muons \cite{thomason2019isis}. Generating these beams requires large, intricate accelerator systems operating under extreme and demanding conditions \cite{2021practicalguide}. The inherent complexity means component failures are inevitable, making effective maintenance crucial for minimising downtime and maximising scientific output \cite{thomason2019isis}.

While the ISIS Neutron and Muon Source (hereafter, referred to as ISIS for brevity) employs various maintenance strategies, leveraging predictive maintenance (PdM) techniques holds significant promise \cite{thomason2019isis, susto2012predictive}. PdM aims to predict failures based on data analysis, enabling proactive intervention \cite{carvalho2019systematic}. The facility's Operational Log (Operalog) contains decades of valuable text entries detailing faults and their remediation. However, its unstructured nature, domain-specific terminology, and variability currently hinder systematic analysis for PdM \cite{usuga2022using}.


This project tackles this challenge by exploring the application of modern Natural Language Processing (NLP) and unsupervised machine learning techniques to automatically categorise these textual entries. It is crucial to recognise the scale and open-ended nature of this research endeavour - there is no established `industry standard' for automatically categorising such specialised industrial operational logs \cite{carvalho2019systematic}. Therefore, this project's primary aim is not to deliver a final, production-ready system, but rather to progress the state-of-the-art by developing and rigorously evaluating an innovative pipeline for this task. The core contribution lies in the methodology itself - through exploring the feasibility, comparing different technical approaches, performing detailed parameter analysis, and building a tool that demonstrates a pathway towards automated insight extraction from these complex logs. This work constitutes one part of a two-part collaborative project; the partner project focuses on analysing structured data from specialised ion source logs.

The specific objectives of this project, framed within this exploratory context, are:
\begin{enumerate}
    \item To pre-process and clean the unstructured text data from the Operalog (the \\ \texttt{FaultDescription} field) with a focus on `ion source' entries.
    \item To investigate and compare the effectiveness of different state-of-the-art sentence embedding models (MPNet \cite{song2020mpnet}, Nomic \cite{nussbaum2024nomic}).
    \item To evaluate dimensionality reduction techniques (PCA \cite{pearson1901liii} vs. UMAP \cite{mcinnes2018umap}).
    \item To apply and compare different unsupervised clustering algorithms (k-Medoids \cite{kmedoids}, DBSCAN \cite{ester1996density}, HDBSCAN \cite{campello2013density}) to identify potential fault categories.
    \item To implement a systematic hyperparameter optimisation strategy, exploring variations in dimensionality reduction and clustering parameters using Optuna \cite{akiba2019optuna}, based on internal clustering evaluation metrics \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite} to identify promising pipeline configurations.
    \item To develop and demonstrate a method for automatically generating initial descriptive labels for the discovered clusters.
\end{enumerate}

% The evaluation strategy involves assessing the technical success of the pipeline components and the relative performance of different configurations. As ground truth fault categories are unavailable, the evaluation relies on internal clustering validation metrics, a standard approach in unsupervised learning . Specifically, the Silhouette coefficient, Davies-Bouldin index and Calinski-Harabasz index metrics were chosen based on their widespread use and ability to capture different aspects of cluster cohesion and separation \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}. To systematically identify optimal pipeline parameters under these multiple objectives, the Optuna framework \cite{akiba2019optuna} was utilised, leveraging its capabilities for efficient multi-objective search. Alongside these quantitative indicators, qualitative assessment will consider the coherence of the discovered clusters with domain knowledge of the Operalog and of the ISIS facility.

The evaluation strategy involves assessing the technical success of the pipeline components and the relative performance of different configurations. As ground truth fault categories are unavailable, the evaluation relies on internal clustering validation metrics, a standard approach in unsupervised learning. Specifically, the Silhouette coefficient, Davies-Bouldin index, and Calinski-Harabasz index metrics were chosen based on their widespread use and ability to capture different aspects of cluster cohesion and separation \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}. To systematically identify optimal pipeline parameters under these multiple objectives, the Optuna framework \cite{akiba2019optuna} was utilised, leveraging its capabilities for efficient multi-objective search.
This involved a detailed exploration of how choices in embedding models, distance metrics, dimensionality reduction settings, and clustering algorithm parameters influenced the final clustering quality based on these heuristics. Alongside these quantitative indicators, qualitative assessment will consider the coherence of the discovered clusters with domain knowledge of the Operalog and of the ISIS facility.

% \todo {Explain the setting, explain the project achievement, Explain the setting, Derive aims from the literature. Explain how the evaluation strategy was chosen and why.}

This report is structured as follows:

\begin{itemize}
    \item \textbf{Chapter 2 (Natural Language Processing Background)} - Provides the necessary theoretical background on the core NLP and machine learning techniques employed.
    \item \textbf{Chapter 3 (Automatic Categorisation and Label Generation)} - Details the developed methodology, data handling, specific tool choices, hyperparameter optimisation framework, and the label generation approach.
    \item \textbf{Chapter 4 (Results and Discussion)} - Presents experimental results demonstrating the pipeline's capability to automatically structure the log entries and compares the performance of different embedding, dimensionality reduction, and clustering choices. This chapter additionally discusses the implications of these findings, including the hyperparameter optimisation outcomes, particularly favouring the HDBSCAN algorithm \cite{campello2013density} under specific parametrisations for this dataset, and reflects on the overall feasibility of the approach.
    \item \textbf{Chapter 5 (Conclusion)} - Summarises the project's progress, highlighting the successful development and evaluation of the auto-categorisation pipeline, reflects on limitations, and outlines crucial next steps for future research, particularly concerning label refinement and validation.
\end{itemize}

\section{Motivation}

The primary motivation for this research is the potential to gain insight on information within the ISIS Operalog's extensive unstructured text data. Manual analysis is labour-intensive and potentially misses subtle patterns which are crucial for effective predictive maintenance (PdM) \cite{usuga2022using, borgi2017data, susto2012predictive, susto2016dealing}. Automating the categorisation of fault reports promises to reduce this burden, enable faster diagnostics, and facilitate the identification of recurring failure modes, directly supporting data-driven PdM strategies \cite{nota2022text, abijith2023large, carvalho2019systematic}.

However, the specific characteristics of the Operalog - its unstructured format, domain-specific language, semantic nuance, and variable entry length - make this auto-categorisation task particularly challenging. Simple methods are insufficient, necessitating the exploration of advanced NLP techniques capable of understanding semantic meaning. This project is therefore motivated by the opportunity to significantly advance the methodology for analysing such specialised operational logs. By constructing and evaluating a novel pipeline incorporating state-of-the-art sentence embeddings \cite{song2020mpnet, nussbaum2024nomic} and unsupervised clustering \cite{kmedoids, ester1996density, campello2013density}, we aim to demonstrate the potential of these techniques and provide a foundational tool for future, more refined systems. The progress achieved in developing and evaluating this pipeline constitutes the core technological advancement delivered by this work.

The specific research aims stemming from this motivation are:

\begin{itemize}
    \item To determine if modern NLP techniques can effectively group semantically similar fault descriptions from the unstructured Operalog text.
    \item To compare the performance of different embedding models, dimensionality reduction techniques, and clustering algorithms within this specific application context.
    \item To develop a methodology for systematically optimising the parameters of this unsupervised pipeline to achieve the most coherent and meaningful categorisation possible within this exploratory phase.
    \item To explore the possibility of automatically generating human-interpretable labels for the discovered fault categories, acknowledging this as an initial step requiring further development.
    \item The input data is the pre-processed \texttt{FaultDescription} text which is scoped to `ion source' during this project, with the hope to extend the pipeline more generally in future work.
\end{itemize}

The expected output is a version of this dataset augmented with cluster IDs and preliminary generated labels, alongside a comparative evaluation of the pipeline configurations explored. The evaluation approach, detailed in Chapter~\ref{chap:results}, relies on internal clustering heuristics (Silhouette coefficient, Davies-Bouldin index, Calinski-Harabasz index) to guide hyperparameter optimisation and assess cluster quality quantitatively \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}. Successfully demonstrating the pipeline's ability to structure this complex data represents significant progress towards leveraging the Operalog for improved maintenance and operational reliability at ISIS.

% Sources and related content
% \todo[inline,caption={}]{
% The things in this section will include
% \begin{itemize}
%     \item Introduce the problem: Auto-categorisation and label inference
%     \item Highlight the research aims. This should highlight the vastly open nature of the project and then hone in on the particular issue I am tackling. (i.e. to progress the state of PdM ...)
%     \item Identify the data input, expected output, data shape and explain why this motivates the project
% \end{itemize}
% }


\section{The ISIS Facility's Complexity and Operational Challenges}\label{sec:isisbg}

% \subsection{Overview of the ISIS Research Facility}\label{sec:isisbg} % Retain original sectioning if needed
% The STFC Rutherford Appleton Laboratory's ISIS Neutron and Muon research facility is a research centre for physical and life sciences, operated by the STFC under UKRI funding \cite{thomason2019isis}. Established based on designs from the 1970s and 80s, ISIS utilises complex accelerator technology to produce neutron and muon beams, enabling detailed studies of materials at the atomic level \cite{thomason2019isis}. This process, however, is inherently complex and presents significant operational challenges.

% \subsubsection{The ISIS Accelerator Chain and Operational Complexity} 
% \textit{% A brief introductory sentence to replace the very first paragraph of original text}

\todo[inline]{
    Discuss this by referencing the figures
}

The ISIS Neutron and Muon Source at STFC Rutherford Appleton Laboratory is a large-scale research facility enabling physical and life sciences study through the production of neutron and muon beams \cite{thomason2019isis}. Achieving this requires a complex, multi-stage accelerator system, the operation and maintenance of which present significant challenges relevant to this project.

The core process, described in \textit{A Practical Guide to the ISIS Neutron and Muon Source} \cite{2021practicalguide}, involves accelerating negative hydrogen ($H^-$) ions through four major stages: an ion source, a radio-frequency quadrupole (RFQ), a linear accelerator (LINAC), and finally a large synchrotron ring. This chain ultimately produces an 800~MeV proton beam directed towards one of two target stations (TS-1 or TS-2) for neutron or muon generation \cite{2021practicalguide}. A schematic overview is shown in Figure~\ref{fig:isis}, with the particle path illustrated in Figure~\ref{fig:isis2}.

Successful operation across these stages demands extreme precision and stability. Throughout the entire acceleration process, maintaining an ultra-high vacuum (between $10^{-8}$ and $10^{-9}$ atmospheric pressure) is crucial, requiring tens of vacuum pumps and continuous monitoring, as leaks can disrupt the beam and damage sensitive components \cite{2021practicalguide}. The initial $H^-$ ion source must provide a stable, consistent beam, as inconsistencies can propagate and amplify downstream; component wear within the source itself is a common maintenance task requiring immediate intervention \cite{2021practicalguide}. Accelerator structures, like those in the LINAC which utilise high-Q radio-frequency fields \cite{michael2006electronic}, require exceptional stability achieved through precise temperature regulation (significantly under $1^\circ C$ variation) to mitigate thermal effects of expansion and contraction \cite{2021practicalguide}.

Within the 163m circumference synchrotron, magnetic fields and accelerating voltages must be ramped in precise synchronism over nearly 8000 revolutions to boost the beam energy to 800~MeV. Any deviation risks beam oscillation, particle loss, and component damage which necessitates continuous, precise monitoring of beam position and machine parameters via sensor networks \cite{2021practicalguide}. Even the injection mechanism, which uses charge-exchange via a thin foil to convert $H^-$ ions to protons \cite{2021practicalguide, ankenbrandt1980h}, relies on the integrity of this consumable foil, whose replacement is a scheduled maintenance activity and whose damage causes immediate performance loss \cite{2021practicalguide}.

The complexity extends to beam transport and target stations. Extracted proton beams must be accurately steered by magnets towards the target stations; mis-steering can damage equipment, hence the need for heavy shielding and beam loss monitors \cite{2021practicalguide}. At the target stations, the high-energy beam strikes targets to produce neutrons or muons \cite{sharma2001nuclear, 2021practicalguide}. These target systems operate under extreme conditions and require careful monitoring and management to prevent overheating or material damage, representing another major operational and maintenance concern \cite{2021practicalguide}.

\noindent This overview highlights the key takeaway: the ISIS facility involves an intricate sequence of precisely controlled stages, creating numerous potential points of failure. While precautions like beam loss monitors and physical shielding exist, the inherent complexity and harsh operating environment (radiation, high-power) mean that component degradation and operational faults are inevitable. Effectively managing the facility therefore necessitates robust maintenance strategies and sophisticated diagnostic capabilities. Documenting fault events, diagnoses, and repairs in operational logs (like the Operalog studied here) is essential, and the technical complexity of the facility directly influences the nature and variety of entries found in these logs, motivating the need for advanced analysis techniques like those explored in this report.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{ISIS.png}
    \caption{The schematic representation of the physical layout of ISIS. The light grey areas are footprints of the buildings. Source: (\citet{thomason2019isis}).}\label{fig:isis}
\end{figure}

\subsection{The end-to-end production of neutrons and muons}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{ISIS2.png}
    \caption{ISIS ion source and chain of accelerators, with $H^-$ ions in blue and protons in red. Not to scale. Source: (\citet{2021practicalguide}).}\label{fig:isis2}
\end{figure}

% The STFC Rutherford Appleton Laboratory's ISIS Neutron and Muon research facility is a research centre for physical and life sciences. It is owned and operated by the STFC, a council within the United Kingdom Research and Innovation (UKRI). The UKRI is a public body of the United Kingdom's Government that directs funding for research and innovation through the science budget of the Department for Science, Innovation and Technology. ISIS was designed in the 1970s and early 1980s, with the core of the design being a strong-focusing machine with six radio frequency accelerating cavities to provide an average beam current of $200\mu A$ \cite{thomason2019isis}. According to (\citet{thomason2019isis}), with the introduction of ISIS and other neutron sources, rapid development of neutron instrumentation was stimulated. Over the years, the facility has been augmented with numerous components and instruments, such as the second target station. The current schematic representation of the facility can be seen in Figure~\ref{fig:isis}.
%
% The ISIS facility, described in A Practical Guide to the ISIS Neutron and Muon Source (\citet{2021practicalguide}), is comprised of four major stages: (1) the ion source, (2) the radio-frequency quadrupole (RFQ), (3) linear accelerator (LINAC) and (4) the synchrotron. These components combine to produce a 800 MeV proton beam that is directed to either target station 1 (TS-1) or target station 2 (TS-2). Starting with $H^-$ ions (protons with two orbiting electrons), the path taken through the four stages is shown in Figure~\ref{fig:isis2}. Throughout the entire $H^-$ or proton acceleration process, the beam must be kept under tight vacuum and, to do so, many tens of vacuum pumps are maintaining the vacuum between $10^{-8}$ and $10^{-9}$ of atmospheric pressure. Maintaining this vacuum integrity is a constant operational challenge as leaks may disrupt the beam and potentially damage sensitive components, necessitating numerous pumps and continuous monitoring system \cite{2021practicalguide}. The rest of this section explains the various stages in a more detail with the purpose of highlighting the complex infrastructure in place and critical control required for the reliable operation of the facility.
%
% The first stage of the ISIS machine is the ion source, which generates the negative hydrogen ($H^-$) ions, which are then accelerated through the RFQ and LINAC. This ion source is a pulsed source that ionises hydrogen gas via an electric discharge between an internal anode and cathode \cite{2021practicalguide}. As detailed in the guide, around 20ml of hydrogen gas per minute is continuously delivered to the ion source from a hydrogen gas bottle. Once the $H^-$ ions emerge from the ion source, they have an energy of 35 keV and are presented to the RFQ via the Low Energy Beam Transport line (LEBT). The LEBT prevents the low-energy $H^-$ beam from increasing in size (due to the mutual repulsion of the ions) and also incorporates a beam stop. This is essentially a remotely removable sheet of metal that physically blocks the beam. Precise control and monitoring of the beam intensity are required here since inconsistencies originating from the ion source affect the overall performance of the machine. This is because these issues can propagate and amplify through the subsequent stages. Additionally, the failure of the source itself, for example due to component wear, requires immediate intervention and is a common maintenance task \cite{2021practicalguide}.
%
% Following the ion source and LEBT, the $H^-$ beam enters the RFQ. This section of the machine utilises high-intensity radio frequency electric fields that contain (focus), group (bunch) and begin accelerating the particles in this beam up to an energy of 665 keV. A key function of the RFQ is to isolate downstream accelerators from variations originating in the ion source, thereby improving beam stability in the LINAC, synchrotron and target stations \cite{2021practicalguide}. However, the stability of the RFQ's own high-intensity beam is a primary concern as, again, fluctuations can lead to poorly formed bunches.
%
% The subsequent stage is the LINAC, which increases the beam energy from 665 keV to 70 MeV. This acceleration is achieved via high-intensity radio-frequency fields arranged in structures that possess a very high Q factor \cite{michael2006electronic} (around 50,000). This means their geometry must be exceptionally stable. Therefore the precise temperature regulation of the cooling water tank is critical in ensuring it is significantly under $1^\circ C$ so that the effects of thermal expansion and contraction are countered \cite{2021practicalguide}. Furthermore, maintaining a high vacuum is critical throughout the LINAC. Failure to do so can lead to premature stripping of loosely bound electrons in the $H^-$ ions, consequently leading to an increase in radioactivity and particle loss (i.e. beam loss). To detect and warn about such cases, beam loss monitors are installed throughout the LINAC, which provide an indication whenever there is an excessive loss in particles. \cite{2021practicalguide}.
%
% Once the $H^-$ particles leave the LINAC, they then pass through a beam transport line - the High Energy Drift Space (HEDS). The function of this is to mainly reduce the particles spreading and focus the beam to the synchrotron. To accumulate the high intensity beam in the synchrotron, ISIS employs charge-exchange injection. This technique allows incoming $H^-$ beam from the LINAC to be `layered' into the synchrotron's circumference over many turns. As the $H^-$ ions enter the synchrotron ring, they pass through a thin foil which strips away their two electrons, converting them to protons. The charge-exchange injection process is specifically chosen as it is the most efficient way of taking and wrapping a long string of particles around the circumference of a synchrotron \cite{2021practicalguide, ankenbrandt1980h}. The integrity of this thin foil is vital for efficient injection and represents a consumable component requiring periodic replacement, which is a scheduled maintenance activity. Damage to the foil will lead directly to beam loss and reduced performance.
%
% The 70 MeV beam is then injected into the synchrotron, which is the primary accelerator ring at ISIS. This ring boosts the (now proton) beam energy to 800 MeV. For an idea of scale, the machine has a circumference of around 163m (corresponding to a radius of approximately 26m). It features a repeating structure of ten `superperiods'. Each superperoid uses dipole magnets to bend the proton path by $36^\circ$ and quadrupole magnets to maintain beam focus \cite{2021practicalguide}. The protons gain energy over just under 8000 revolutions, as each turn gives a proton 0.1 MeV. Achieving this final energy requires extreme precision over each and every revolution. While the protons gain energy, the magnetic fields and accelerating radio-frequency voltages must also very precisely be increased in synchronism. Any deviations can cause the beam to oscillate and strike the vacuum vessel walls which results in particle loss and potential component damage. Therefore, continuous precise monitoring of beam position, magnet currents and more via a network of sensors is imperative for controlling the beam orbit and ensuring successful acceleration \cite{2021practicalguide}. The initially continuous ring of injected protons is gathered into two distinct bunches, a little more than $0.1\mu s$ apart, using a low-level radio-frequency voltage before the main acceleration begins \cite{2021practicalguide}.
%
% Once accelerated, in order to direct the proton bunches towards a pre-selected target station (TS-1 or TS-2) three kicker magnets are used which deflect the bunches upwards. These bunches are then bent into the respective extracted proton beam lines (EPB1 or EPB2). These lines use further magnets for steering. Safe transport relies on accurate magnet performance as mis-steering could direct the high-energy beam into the pipe walls. Additionally, as a safety precaution, both lines are heavily shielded in thick steel and concrete and beam loss monitors are installed along their length to detect any errant particles and provide crucial, real-time feedback to operators \cite{2021practicalguide}.
%
% The primary purpose of the ISIS facility culminates at the target stations. The high-energy proton beam collides with a dense, high atomic number target (tantalum-clad tungsten at ISIS), designed to produce a large number of neutrons via spallation, within a compact region of space \cite{2021practicalguide}. These energetic neutrons are then slowed down (moderated) by surrounding materials (liquid nitrogen, liquid hydrogen and water at ISIS) and reflected back towards instruments by a beryllium reflector \cite{sharma2001nuclear, 2021practicalguide}. Additionally, ISIS produces muons for experiments by inserting a thin (roughly 1cm thick) graphite target into the proton beam around 20m away from the TS-1 neutron target. Collision in this target generates pions, which subsequently decay into muons \cite{2021practicalguide}.
% This entire system needs extremely careful control and monitoring capabilities, as failure due to overheating or material damage is a major operational concern which requires careful management and represents a major maintenance undertaking. Precise monitoring is required of metrics such as incoming beam profile, target temperature, cryogenic temperatures. \\
%
% \noindent This detailed description sourced from the guide highlights some key takeaways. There are an incredible number of points of failure in the facility due to the highly precise and radioactive nature of the task. The ISIS team has taken many precautions such as fault detection mechanisms (i.e. the beam loss monitors) and breakdown counter-measures (i.e. the thick layer of steel and concrete in the EPB1 and EPB2 lines). However, the inherent complexity and harsh operating environment (radiation, high-power) necessitate both robust preventative maintenance schedules and sophisticated diagnostic capabilities that address the component degradation and failures that will inevitably arise. 

\subsection{Maintenance at ISIS} 
As detailed in the 33-year historical account of the ISIS facility \cite{thomason2019isis}, the ISIS operations occur in cycles, periods of roughly 30-50 days where the machine runs constantly without breaks. Gaps between cycles typically range from 1 week to 3 months. In addition, typically every four years, shutdowns scheduled for 6-9 months occur for major maintenance and upgrade work. On the ground, day-to-day operations are run from the Main Control Room (MCR) by the ISIS crew which consists of 6 shift teams of the following roles: (1) duty officer, (2) assistant duty officer and (3) duty technician. The expertise and rapid response capabilities of these trained operational crews are vital for both routine operation and initial fault response.
Minimising this downtime is crucial for several reasons inherent to operating a large-scale user facility like ISIS \cite{thomason2019isis}. Primarily, unscheduled downtime directly impacts the international researchers allocated beam time, potentially jeopardising experiments planned months or years in advance and wasting valuable research opportunities. Furthermore, interruptions disrupt the tightly packed operational schedule, reduce the overall scientific output, and incur significant operational costs without delivering the facility's core research product \cite{thomason2019isis}. Therefore, understanding the factors that contribute to downtime and implementing strategies to mitigate it are paramount.
Many factors affect downtime such as having a robust plan, the day-to-day operators' knowledge of the system and adequate inventories of spares \cite{thomason2019isis}. Figure~\ref{fig:isisDowntime} shows the ratio of downtime to active machine operation since 2016, indicating this machine has been down for maintenance over half the time since 2016. In other words, over half the time, the machine is not active and is undergoing maintenance. This highlights the trade-off between maximising operational availability and minimizing failures through planned maintenance. The major maintenance strategies and their trade-offs are further explored in Section~\ref{sec:maintenanceTechniques}.

To help reduce downtime, over the last few years, a first-line diagnosis (FLD) system was introduced, presented in (\citet{fld2017}). The FLD system helps reduce downtime by providing expert guidance on fault diagnosis and resolution, which has been shown to improve the dissemination of knowledge from experts to operations. The FLD utilises Fault Analysis Pathways (FAPs), which provide structural links between ISIS subsystems. This allows users of the system to access granular subsystems' local documentation minimising file hunting and saving time and effort. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{isisDowntime.png}
    \caption{Active versus Downtime days (2016 - 2024), according to the ISIS operational cycle. Data source: (\citet{isisbeamoperations2024})}\label{fig:isisDowntime}
\end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\textwidth]{fap.png}
%     \caption{Example FAP1101, screenshotted from an iPAD version of FLD version 2.2, which shows the FAP for pre-injector, injector and High Energy Drift Space. Source: (\citet{fld2017})}\label{fig:fap}
% \end{figure}


% \textbf{}\\
% \todo[inline,caption={}]{
% Introduce the research topic. 
% The things in this section will include
% \begin{itemize}
%     \item Talk about the ISIS research facility - DONE
%     \item Talk about the Operational Cycle for ISIS - SORT OF DONE (graph too)
%     \item Talk about the ISIS Crew and importance of having trained staff on premises. - SORT OF
%     \item Talk about the Lost time and why it is important to minimise this for the ISIS research facility. - SORT OF
%     \item Describe the first-line diagnosis system (FLD) and FAPs. - DONE
%     \item Talk about the Datasets, operalog (MOVED)
% \end{itemize}
% }


\section{Maintenance Techniques}\label{sec:maintenanceTechniques}
In industry, the uptime of production systems is strongly coupled with the equipment maintenance. So much so that what was once considered a `necessary evil' is now seen as a `profit contributor' to be able to maintain a competitive edge \cite{waeyenbergh2002framework, faccio2014industrial}. For facilities aiming to provide systems for research, maintenance impacts the downtime and cost of running. As a result, both to minimise unexpected downtime and provide a competitive edge, many industrial applications collect vast quantities of data during the entire life cycle of the system. This large amount of data may include information about processes, events and alarms \cite{carvalho2019systematic} which occur along the industrial production line, collected by different equipment. The equipment may be located in different locations in the sub-components of the larger system or even different sub-components themselves. 

% \todo{talk about maintenance itself in a lot more depth}

In the literature, various terms and categories of maintenance arise each with differing strategies \cite{susto2012predictive, mobley2002introduction, susto2016dealing}. Thus, while there exists some disagreement in nomenclature, we consider the four categories presented in (\citet{susto2012predictive}). The four maintenance policy categories are as follows, noting that each policy has, uniquely, their own benefits and drawbacks:

\begin{enumerate}
    \item{\textbf{Run-to-failure (R2F) maintenance} - Continual usage of the system until failure. Restoration is performed at the point of noticing failure condition. This is the simplest approach and typically the most costly method as it reduces the facility's availability and requires a complete replacement of parts.} 
    \item{\textbf{Preventative maintenance (PvM)} - Otherwise referred to as scheduled maintenance, performing maintenance at regular intervals to increase longevity of the component or in anticipation of the end of expected life of the component. While this typically prevents many errors, it wastes maintenance cycles when systems are perfectly healthy. Hence, causing unnecessary downtime and cost.}
    \item{\textbf{Condition-based maintenance (CBM)} - Taking the action to perform maintenance on equipment through monitoring various health characteristics and metrics of the components of the system. This approach requires continuous monitoring and, thus, allows for close to instant response on maintenance only when required. However, a drawback of this policy is that one cannot plan maintenances in advance.}
    \item{\textbf{Predictive maintenance (PdM)} - Otherwise referred to as statistical-based maintenance, only performs maintenance actions when determined necessary. Prediction tools are utilised to implement forward-planning and scheduling systems, using statistical inference methods. However, if these statistical inferences are not accurate, the whole system suffers which inevitably leads to additional downtime and costs.}
\end{enumerate}
It should be noted that several sources conflate CBM and PdM \cite{mobley2002introduction}. As in (\citet{susto2012predictive}), where they are given as separate categories, we follow suit. \\ 

The PdM strategy stands out in the four categories presented as, given a statistical inference model that is able to detect faults accurately, this policy optimises the trade-off between improving equipment condition, reduce failure rates for equipment and minimising maintenance costs \cite{carvalho2019systematic}. This technique enables one to apply foresight for pre-emptive scheduling of large-scale maintenance. As pointed out in Section~\ref{sec:isisbg}, the ISIS facility aims to strike a balance between PvM, CBM and PdM through periods of large-scaled scheduled maintenance and collection of high quantities of metrics. This balance is achieved through the careful coordination between cycle scheduling, day-to-day crew-based monitoring and the FLD \cite{thomason2019isis}.

In industry, many maintenance strategies prefer using PdM whilst experimenting with a variety of statistical inference and artificial intelligence modelling approaches \cite{mobley2002introduction, jezzini2013effects}. Some examples from \cite{carvalho2019systematic} are listed in Table~\ref{tab:pdmPaperTable} which highlights the trend in the industry towards more accurate, ML-based approaches.

\rowcolors{2}{gray!10}{white}
\begin{table}[htbp]
    \fontsize{8}{12}\selectfont
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{p{1.5cm} p{8.5cm} p{3cm}}
        \toprule
        \textbf{Type} & \textbf{Description} & \textbf{Reference} \\
        \midrule
        Statistical & Application of SAFE to deal with PdM problems characterised by time-series data. The approach is tested on a real-life dataset of the semiconductor ion implantation process. & (\citet{susto2016dealing})\\
        ML & Application of SVM classification for fault prediction of rail networks, with discussion on using the model in optimising trade-offs related to maintenance schedule and costs. & (\citet{li2014improving})\\
        ML & Audio analysis on IoT devices, enabling acoustic event recognition for machine diagnosis. This paper describes designing an end-to-end system, utilising CNN-based classification. & (\citet{pan2017cognitive})  \\
        ML & Utilisation of RF decision trees trained on SMART data to predict reliability of HDD in real-time. & (\citet{su2018real}) \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Examples of applications of PdM for industrial maintenance strategies.}
    \label{tab:pdmPaperTable}
\end{table}

\section{Data}

Data relevant to the operational state and maintenance of the ISIS facility originates several sources. The ISIS team maintains proprietary internal datasets detailing specific component performance, which are available upon request. For this research, access was granted to historical logs for the ion source component, with entries dating back to 27 March 2003 and the Operalog, an operational log maintained by the operations crew, which documents facility faults and the corresponding remedial actions taken.

Given the breadth of available data and the project's scope constraints, a decision was made to focus the analysis presented in this report on the \textbf{Operalog}. This dataset provides a rich textual record of real-world failures across the facility, offering valuable insights despite its partially unstructured nature. Data from the specialised ion source logs is the focus of the partner project. 

\subsection{The Operalog}
The operational log, otherwise referred to as the `Operalog', is an Excel spreadsheet with entries documenting moments of machinery failure within the ISIS facility covering the period 1996 - 2023. Table~\ref{tab:operalog} documents the important features. Notably, the day-to-day operational crew have developed custom abbreviations, acronyms and terminology which may not immediately be clear. Additionally, different crew members have varying writing styles and levels of depth of information. Thus, this results in an unstructured text dataset which has a huge variance in quality and quantity of information. Furthermore, as noted in Table~\ref{tab:operalog}, \texttt{FaultRepair} was seemingly made redundant post-2017. Figure~\ref{fig:FaultDescriptionVSFaultRepairs} highlights the distribution in the text lengths of both unstructured text fields. Furthermore, around $0.02\%$ of \texttt{FaultDescription} fields are empty whereas around $66\%$ of \texttt{FaultRepair} fields are empty. The generally shorter entries in \texttt{FaultRepair}, with a large majority having fewer than or equal to 5 characters, suggest lower informational content compared to \texttt{FaultDescription}. This is further illustrated in the Wordcloud illustrations \cite{oesper2011wordcloud} (Figure~\ref{fig:wc}), where \texttt{FaultRepair} only has one extremely large word (`reset'). As the size of the word indicates the frequency, this means \texttt{FaultRepair} has the word `reset' in almost all non-empty entries. With `reset' being exactly 5 characters and most non-empty entries in \texttt{FaultRepair} being at most 5 characters (of which, those that contain reset are over 70\% of entries), it is easy to see that the \texttt{FaultDescription} field contains richer, more informative context. 

\rowcolors{2}{gray!10}{white}
\begin{table}[htbp]
    \fontsize{8}{11}\selectfont
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{p{3cm} p{3cm} p{7cm}}
        \toprule
        \textbf{Feature Name} & \textbf{Data Type} & \textbf{Description} \\
        \midrule
        FaultDate & Date-time & Date the fault occurred, with a precision up to the nearest second. \\
        UserRun & String & Operational cycle that this fault has occurred within. Cycle information up to 2016 can be found at (\citet{isisbeamoperations2024}). \\
        Downtime & Integer & The amount of time, in hours the downtime has occurred for. \\
        Group & String, Fixed Category & The group that the faulty equipment is part of. There are 13 unique equipment groups. \\
        Equipment & String, Fixed Category & The equipment type that failed. There are around 200 unique equipment types that have been logged to fail. \\
        FaultDescription & String, Free-form & This is a free-form, unstructured text field that allows the on-shift operational crew to note details about the problem diagnosis and remediation that has occurred. There is no constraint to the size of this text field.\\
        FaultRepair & String, Free-form & Another free-form, unstructured text field that allows the on-shift operational crew to note only the remediation steps. The crew seems to have stopped using this field after the end of 2017, preferring to put remediation steps in FaultDescription. \\
        ManagersComments & String, Free-form & A very rarely used free-form text field, where the manager in charge of the on-shift crew will input comments. \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Description of important features (columns) in the Operalog.}
    \label{tab:operalog}
\end{table}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{FaultDescriptionVSFaultRepairs.png}
%         \caption{Frequency distribution of the text lengths for non-blank \texttt{FaultDescription} and \texttt{FaultRepair} fields. Each bar represents a range of 5 characters.}\label{fig:FaultDescriptionVSFaultRepairs}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/ionSourceTokenCountDist.png}
%         \caption{Frequency distribution of the number of tokens for non-blank \texttt{FaultDescription} and \texttt{FaultRepair} entries. Each bar represents a range of around 2 tokens.}\label{fig:ionSourceTokenCountDist}
%     \end{subfigure}
%     \caption{Frequency distributions of characters and tokens.}
%     \label{fig:hists}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{FaultDescriptionVSFaultRepairs.png}
%         \caption{Distribution of text character lengths for non-empty \texttt{FaultDescription} and \texttt{FaultRepair} entries. Bin width is 5 characters.}
%         \label{fig:FaultDescriptionVSFaultRepairs}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/ionSourceTokenCountDist.png}
%         \caption{Distribution of token counts for non-empty \texttt{FaultDescription} and \texttt{FaultRepair} entries. Bin width is approximately 2 tokens.}
%         \label{fig:ionSourceTokenCountDist}
%     \end{subfigure}
%     \caption[Operalog Field Length Distributions]{Comparison of length distributions (character counts and token counts) for the \texttt{FaultDescription} and \texttt{FaultRepair} fields in the Operalog dataset.}
%     \label{fig:hists}
% \end{figure}


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FaultDescriptionVSFaultRepairs.png}
        \caption{Character length distribution.}
        \label{fig:FaultDescriptionVSFaultRepairs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/ionSourceTokenCountDist.png}
        \caption{Token count distribution.}
        \label{fig:ionSourceTokenCountDist}
    \end{subfigure}
    \caption[Operalog Field Length Distributions]{Frequency distributions comparing the non-empty \texttt{FaultDescription} and \texttt{FaultRepair} fields from the Operalog. Left shows the distribution of text lengths in characters (each bar represents a range of approximately 5 characters). Right shows the distribution of the number of tokens (each bar represents a range of approximately 2 tokens).}
    \label{fig:hists}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{wcFaultRepairs.png}
        \caption{FaultRepair}
        \label{fig:wcFaultRepairs}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{wcFaultDescription.png}
        \caption{FaultDescription}
        \label{fig:wcFaultDescription}
    \end{subfigure}
    \caption{Comparison of word clouds for the top 300 most common words for the FaultRepair field versus the FaultDescription field. Larger words are more common and collocations are enabled so both words and phrases are shown, leading to some repeating.}
    \label{fig:wc}
\end{figure}

% \subsection{Preliminary Data Analysis}
% \todo[inline, caption={}]{ 
%     \begin{itemize}
%         \item Talk about amount of issues per year
%         \item Talk about the distribution of text lengths across whole df
%         \item Talk about the distribution of text lengths across iondf
%     \end{itemize}
% }


\chapter{Natural Language Processing Background}\label{chap:nlpbg}

This chapter delves into the technical background required for the methodology proposed in Chapter \ref{chap:Methodology}. Firstly, we discuss various measures of text similarity in Section \ref{sec:sentenceSimilarity}, which motivates text embeddings and categorises similarity measures into different generations. Section \ref{sec:sentenceEmbedding} introduces the third‑generation model BERT \cite{devlin2019bert} and the fourth‑generation universal‐embedding architecture XLNet \cite{yang2019xlnet}. We specifically discuss the technical details of the BERT model and further iterations on it, then detail one improvement over BERT (XLNet) and outline the design principles that underpin contemporary fourth‑generation models. Afterwards, we cover two methods of dimensionality reduction,  PCA \cite{pearson1901liii,hotelling1933analysis} and UMAP \cite{mcinnes2018umap}, motivated by the need to visualise samples from the high‑dimensional embedding spaces of the aforementioned models, in Section \ref{sec:dimred}. Finally, we present three clustering algorithms - one partitioning‑based (k‑Medoids \cite{kmedoids}) and two density‑based (DBSCAN \cite{ester1996density} and HDBSCAN \cite{campello2013density}).

%together with four clustering‑evaluation metrics: (1) Silhouette \cite{rousseeuw1987silhouettes}, (2) Davies-Bouldin Index \cite{davies1979cluster}, and (4) Calinski-Harabasz Index \cite{calinski1974dendrite}. \todo{maybe talk about optuna, if we use it.} \todo{double check all citations here are not empty}\


% \todo[inline,caption={}]{
% Describe the various technical factors required before attempting to understand the methodology.
% The things in this section will include
% \begin{itemize}
%     \item Discuss sentence embedding, similarity measures: BERT, RoBERTA, MPNet, XLNet, NOMIC.
%     \item Dimensional reduction techniques and need for them (UMAP, PCA, t-SNE).
%     \item Clustering methods: kmedoids, DBSCAN, DBSCAN*/HDBCAN
%     \item Clustering evaluation methods: todo I don't remember these off the top of my head 
%     \item Maybe briefly touch on Optuna?
% \end{itemize}
% }

\section{Sentence Similarity}\label{sec:sentenceSimilarity}
Sentence similarity, otherwise referred to as document similarity, is the (NLP) task of computing the quantification of the similarities between two sentences, documents or texts. This task is motivated by the increasingly large amount of digitisation of human languages (and data, in general), calling for the need to understand similarity between various texts \cite{raju2022sentence}. Examples of the use-cases of sentence similarity include: detection of academic malpractice via plagiarism \cite{lukashenko2007computer, baba2017plagiarism}, and text summarisation \cite {aliguliyev2009new, kumar2020semantic, jo2017k}. According to \cite{raju2022sentence}, there are two main types of sentence similarities: (1) lexical similarity and (2) semantic similarity. The former is a computation of the equality between the lexicon of two sentences (i.e. a purely syntactical view). This contrasts with semantic similarity, which compares meaning. Further, the type we focus on, semantic similarity can be split into three types: 

\begin{itemize}
    \item \textbf{String-based similarity} - Measures similarity directly between two strings, accounting for string sequences and character composition. These can be fine-grained, i.e. character-based; coarse-grained, i.e. term-based; or a hybrid mixture of both \cite{yu2016string}.
    \item \textbf{Knowledge-based similarity} - Measures the degree to which two sentences are related, utilising semantic networks (i.e. knowledge graphs). Examples of Knowledge-based similarity approaches include WordNet \cite{budanitsky2001semantic}, the most popular type of approach.
    \item \textbf{Corpus-based similarity} - Premised on a provided corpus, a large database of text to derive inferences from. Methods of this type require the development statistical or DL models that train on the provided corpus and estimate the similarity between two sentence-pair inputs. Popular examples include traditional statistical models, such as LSA \cite{landauer1998introduction} and SVD \cite{steinberger2005text} as well as word embedding models (utilising ML), such as Word2Vec \cite{bojanowski2017enriching}, GloVe \cite{pennington2014glove} and fastText \cite{mikolov2013efficient}.
\end{itemize}

Most of the models mentioned above require some numerical representation of the text to be able to apply mathematical procedures for similarity calculation. Computing this representation involves converting unstructured textual data into one or more vectors. Typically, this process includes (1) general natural language pre-processing steps such as stop-word removal, case normalisation, parts-of-speech tagging, lemmatisation, and tokenisation \cite{tabassum2020survey}; and (2) applying an embedding model, either to a single token (word embedding) or to a sequence of tokens (sentence embedding). Step (1) can be seen as a feature extraction step applied on the unstructured textual data, where feature extraction is the process of extracting the most useful components of the data \cite{sammons2016edison}. For example, part-of-speech tagging can be seen as introducing non-trivial features to some token through extracting the surrounding context. 

This representation is known as an embedding, with the span of the possible vectors referred to as the embedding space. The dimension of the span is termed the embedding dimension. This is an important concept because the characteristics of the embedding space influence the model's ability to capture syntactic and semantic meaning in text, as the embedding space itself encodes this information. This can be seen in the Word2Vec model, described in (\citet{bojanowski2017enriching}), which shows different embedding dimensions produce different results. Another conclusion that can be drawn from this paper is that, if embedding space is not constructed to maximise the meaning of texts, the accuracy of model predictions tends to deteriorate. 

Therefore, the problem of sentence similarity can be directly mapped from the problem of sentence embedding (otherwise referred to as text embedding), where text embedding is the (NLP) task of learning a high-dimensional embedding space representation. Various aspects of text embedding are more thoroughly covered in Section~\ref{sec:sentenceEmbedding}. However, with the advent of the transformer architecture \cite{vaswani2017attention} and rise of the large language models, text embedding has been increasingly solved using DL models with high parameter counts \cite{cao2024recent} and considering extremely large token sequences. Nowadays, word embedding models are considered obsolete with (\citet{cao2024recent}) only considering these models second-generation. Further, the paper states newer generations fall into the following categories:
\begin{itemize}
    \item \textbf{Third-generation} - contextualised embeddings. These models dynamically account for contexts, encoding them into the embedding space. Examples of models include ELMo \cite{sarzynska2021detecting}, GPT \cite{radford2018improving} and BERT \cite{devlin2019bert}. As these models are trained to both understand some embedding space and generate natural language text, they are canonically referred to as language models.
    \item \textbf{Fourth-generation} - universal text embeddings. The generation which is currently state-of-the-art, with the aim of developing a unified model which is able to address multiple downstream tasks. Examples of models in this generation, making progress towards unification include Gecko \cite{lee2024gecko}, Multilingual e5 text embeddings \cite{wang2024multilingual}, Nomic \cite{nussbaum2024nomic} and many more. 
\end{itemize}

Second-, third- and fourth-generation text embedding models are used frequently in PdM for applications such as insight extraction \cite{abijith2023large,usuga2022using} and clustering intents from unstructured text data. Sources of natural language datasets, in industrial applications typically arise from operational or managerial log files which document aspects such as failures, resolutions and comments. Advanced text embedding models enable for semi- or fully automatic insight retrieval and auto-categorisation, enabling intuitive understanding of the textual datasets potentially highlighting patterns in failure \cite{nota2022text}.

\section{Sentence Embedding}\label{sec:sentenceEmbedding}
Briefly touched on in Section~\ref{sec:sentenceSimilarity}, sentence embedding (otherwise known as text embedding) is the NLP task of computing some high-dimensional embedding vector-space representation for unstructured text data. This vector-based representation should encode the semantic and syntactic meaning of the text and establish meaningful relationships. For example, the sentence `I like dogs' should have the opposite representation to `I hate dogs'.  However these sentences should be more related than to the sentence `My house was destroyed in an earthquake'. For a naive illustration of this, see Figure~\ref{fig:sentenceEmbeddingExample}, which shows how similar sentences should be grouped together. As Nomic requires the task explicitly in input text (more on this later), `clustering:' is appended to all sentences. Deep learning sentence embedding models are now seen to be state-of-the-art as they are able to extract features automatically and more effectively than manual efforts, when supported with large quantities of data \cite{liang2017text}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{sentenceEmbeddingExample1.png}
    \caption{Example text embedding with Nomic Text Embedding v1.5 \cite{nussbaum2024nomic} of random sentences generated using OpenAI's ChatGPT o3-mini-high and projected onto 2 dimensions using UMAP \cite{mcinnes2018umap} with a random seed of 42 and minimum neighbours of 15. Only some of the labels are highlighted for visual clarity.}\label{fig:sentenceEmbeddingExample}
\end{figure}

% \subsection{Transformers}\label{sec:BERT}
% Pre-trained language models (PLMs), such as BERT \cite{devlin2019bert}, have been widely successful in a wide range of NLP tasks through fine-tuning \cite{edunov2019pre, min2023recent}. Fine-tuning is the process of re-training a PLM on specialised tasks, leveraging the model's base knowledge, by applying perturbations to the pre-trained model parameters through gradient descent learning algorithms. These language models, trained on finding an embedding space for natural language as well as stochastically generating tokens to mimic natural language, often provide a great platform to perform task-specific model fine-tuning. 
% \todo{fix wording}
% Platforms such as HuggingFace \cite{wolf2019huggingface}, allow authors to upload these pre-trained model parameters which in-turn allows researchers to download them for them fine-tuning. Moreover, task-specific fine-tuned models parameters are uploaded, downloaded and shared on these platforms. These fine-tuned models are useful for researchers whose focus lies outside of optimising these model parameters.
% In this section, we talk about the transformer-family of language models.
%
% \subsubsection{BERT}
% The Bidirectional Encoder Representations from Transformer (BERT) language model introduced in 2018 by Google AI Language team in (\citet{devlin2019bert}) was designed to learn deep bidirectional representations from unlabelled text. It achieved this by learning the left and right context in every layer of the model. The model implementation can be split into two phases: \todo{sprinkle some images in here.} (1) the pre-training phase; and (2) the fine-tuning phase. This allows the model architecture to remain common, with many down-stream NLP tasks benefiting from a single PLM \cite{devlin2019bert}. An example illustration can be seen in Figure~\ref{fig:bert1}, where a single model can be fine-tuned on many down-stream tasks by simply replacing the output layer. The model architecture is a multi-layer transformer encoder based on the original paper (\citet{vaswani2017attention}) which is bidirectional by nature. As the transformer architecture is a very well researched architecture, well represented in the literature and slightly out-of-scope for this paper, we refer the reader to the original paper.
%
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\textwidth]{BERT1.png}
%     \caption{The overall pre-training and fine-tuning phases for BERT. For different down-stream tasks, notice the same architecture, except output layer, is used. Source: (\citet{devlin2019bert}).}\label{fig:bert1}
% \end{figure}
%
% \textbf{}\\ The pre-training phase can be split into two further training tasks (otherwise referred to as objectives):
% \begin{itemize}
% \item  Masked Language Modelling - During training, a portion of the input tokens are `masked' and the model predicts these masked tokens based on the remaining context, using cross-entropy loss \cite{zhang2018generalized}. In the literature, this is referred to as a Cloze task \cite{taylor1953cloze}. In the experiments conducted by (\citet{devlin2019bert}) 15\% of tokens in each sequence are randomly selected for masking. However, to mitigate the mismatch between pre-training and fine-tuning phases, a training data generator replaces each chosen masked tokens with: (1) the special token \texttt{[MASK]} 80\% of the time; (2) a random token 10\% of the time; and (3) the original token itself 10\% of the time \cite{devlin2019bert}.
%     \item Next Sentence Prediction - In this task, the model is trained on a binary classification task prediction the relationship between sentence pairs. Given two sentences \texttt{A} and \texttt{B} either (1) sentence \texttt{B} is the actual sentence that follows \texttt{A} 50\% of the time or (2) sentence \texttt{B} is randomly selected from the corpus 50\% of the time \cite{devlin2019bert}. 
% \end{itemize}
%
% \textbf{}\\ The Google AI Language team further fine-tune BERT across 11 NLP tasks, showing that BERT out-performed the state-of-the-art models at its point in time.
%
% As previously mentioned, a benefit of using PLMs such as BERT for NLP tasks (such as text embedding) is the ability to off-load the computationally intensive pre-training and any initial fine-tuning stages to another party. Although, this means that there now exists an implicit trust in that third-party's model parameters. For example, the third-party could be motivated by factors to censor certain phrases, artificially injecting testing data into training to increase performance metrics or maliciously change the embedding space. These are all factors which influence our decision on using a PLM and, thus, PLMs with transparently documented data sources, pre-training and fine-tuning should be preferred.
%
% Since the release of the Google AI Language team's BERT, there have been some developments on further improving the performance of PLMs based on techniques presented in the paper. Namely, we touch on two: XLNet \cite{yang2019xlnet} and MPNet \cite{song2020mpnet}. 
% XLNet's architecture is backed by the Transfomer-XL model \cite{dai2019transformer}. However, it is still briefly covered here to motivate MPNet. MPNet is a hybrid model that inherits XLNet's key feature, permutation language modelling, and BERT's masked language modelling, producing a `unified view'.
%
% \subsubsection{XLNet}\label{sec:xlnet}
%
% According to (\citet{yang2019xlnet}), the most successful pre-training objectives can broadly be split into two categories: (1) auto-regressive (AR) and auto-encoding (AE). AR language modelling, given a sequence of tokens $X = (x_1, ..., x_T)$, aims to factorize likelihood of a token into either a forward (Equation~\ref{eq:fwdprod}) or backward (Equation~\ref{eq:bckprod}) product. As an AR language model encodes context in a singular direction (only forward or backward, but not both), it has been shown to not be effective \cite{yang2019xlnet}. The notation $X_{<i}$ for some $1 \le i \le T$ means to include the first $i - 1$ elements (vice versa for $X_{>i}$).
%
% \begin{align}
%     p(x) &= \prod_{i=1}^{T} p(x_i | X_{<i})
%     \label{eq:fwdprod} \\
%     p(x) &= \prod_{i=T}^{i} p(x_i | X_{>i})
%     \label{eq:bckprod}
% \end{align}
%
% AE language modelling, such as BERT, on the other hand aims to construct the original data from masked or malformed input, without estimations of the probability densities \cite{yang2019xlnet}. 
% One downside of BERT, is the discrepancy between the pre-training and fine-tuning phases. Although BERT utilises bidirectional context for reconstruction of the special \texttt{[MASK]} token during pre-training, this special token does not exist at the fine-tuning phase. Additionally, the predicted tokens are masked in the \textit{input} and BERT assumes the predicted tokens are independent to the surrounding context as it is not able to model joint probabilities \cite{yang2019xlnet}. This assumption is an gross oversimplification, as longer term dependencies are highly prevalent in natural language applications \cite{schank1972conceptual, dai2019transformer}. 
%
% Therefore, XLNet proposes an AR language modelling solution, backed by Transfomer-XL architecture \cite{dai2019transformer}, that combines the benefits of both AR and AE language modelling whilst avoiding their downsides. It does this by introducing two methods, described below.
%
% \begin{itemize}
%     \item{Permutation language modelling - XLNet maximises the pre-training objective, the expected log-likelihood over all possible factorisation orders of a length-$T$ sequence. A factorisation order is simply one way of shuffling the token positions $1, ..., T$. Imagine picking a shuffle then walking through the sentence in that order - when we come to a token, we predict it only using the tokens that appeared earlier in the very same shuffle even if those tokens lie to its right in the original sequence. Since this procedure is repeated for all shuffles, every token eventually `sees' information from both sides of the input, which gives XLNet a BERT-like bidirectional context \cite{yang2019xlnet}. More formally, 
%             \begin{equation} \max_{\theta} \text{  } \mathbb{E}_{z\in\mathcal{Z}_T} \bigl[ \sum_{i=1}^{T} \log p_{\theta} \bigl(x_{z_i}\mid x_{z_{<i}}\bigr) \bigr], \label{eq:plm} \end{equation}
%             where $\mathcal{Z_T}$ is the set of all $T!$ permutations of the index list $[1, ... , T]$. For a given permutation $z = (z_1, ..., z_T)$, $X_{z_{<i}}$ denotes the tokens that appear before $z_i$ in that permutation. $\theta$ denotes the model parameters, and is shared across all orders \cite{yang2019xlnet}.
%         }
%
%     \item{Two-stream self-attention - A naive transformer would expose the model to the very token it is trying to predict. XLNet avoids this by splitting the hidden state at every layer into (1) a content stream $h$, which encodes both content and position for every input token, and (2) a query stream $g$ which is target-aware. That is, the query stream knows where in the sequence a prediction is to be made, through its positional embedding but never sees the token's content \cite{vaswani2017attention, yang2019xlnet}. This helps the model use a different probability density depending on where in the text it is predicting, which BERT was lacking. At layer $m$ and target position $z_i$, the update sequence for $h$ and $g$ look like the following, using the regular attention function from (\citet{vaswani2017attention}), 
%             \begin{align}
%                 g_{z_i}^{(m)} &\leftarrow \text{Attention} (Q = g_{z_i}^{(m - 1)}, KV = h_{z_{<i}}^{(m - 1)}) \\
%                 h_{z_i}^{(m)} &\leftarrow \text{Attention} (Q = h_{z_i}^{(m - 1)}, KV = h_{z_{\le i}}^{(m - 1)})
%             \end{align}
%             The query-stream vector $g_{(z_i)}^{(M)}$, after $M$ layers, is used to compute the soft-max \cite{gao2017properties} over some pre-determined vocabulary. This ensures the model predicts $x_{(z_i)}$ without ever having directly observed it. This is while the content stream continues to propagate contextual signals and positional signals.
%         the use of two sets of hidden representations, the content representation and query representation. The content representation encodes both the context and the permutation $x_{(z_i)}$ whereas the query representation only encodes the position $z_i$ and the contextual information $X_{(Z_{<i})}$ \cite{yang2019xlnet}.}
% \end{itemize}
%
% Through integration of permutation language modelling and two-stream self-attention, XLNet inherits the probability density estimation ability of AR models and bidirectional context of AE models. Additionally, it also side-steps BERT's pre-train and fine-tune discrepancy and independence assumption \cite{yang2019xlnet}. For further specific details on XLNet, the reader is referred to (\citet{yang2019xlnet}).

\subsection{Transformers and Foundational Pre-trained Language Models}\label{sec:BERT} % 

An important advancement in modern NLP is the use of Pre-trained Language Models (PLMs). Models like BERT \cite{devlin2019bert} demonstrate success and improvements in performance across various NLP tasks when adapted through a process called fine-tuning \cite{edunov2019pre, min2023recent}. Fine-tuning involves taking a general-purpose (termed `base') PLM, already trained on vast amounts of text data, and further training it on a smaller, task-specific dataset. This leverages the model's pre-existing knowledge of language structure and semantics, allowing for efficient adaptation to specialised tasks like the Operalog analysis in this report. These PLMs effectively learn an embedding space for language and can often generate text, providing a powerful foundation \cite{devlin2019bert}. The accessibility of these models has been greatly enhanced by platforms like HuggingFace, which host pre-trained model parameters (or `weights'), enabling researchers to download and utilise or fine-tune them without undertaking the computationally prohibitive initial pre-training \cite{wolf2019huggingface}. This project leverages such pre-trained models, specifically focusing on the Transformer family.

The Transformer architecture, introduced by (\citet{vaswani2017attention}), employ attention mechanisms, to compute pairwise token significance. That is, conceptually, attention allows the model - when processing a word in a sequence - to weigh the importance of other words in the sequence regardless of their distance to each other. This enables the model to capture long-range dependencies and understand context more effectively than earlier sequential models. While the detailed architecture involves components like multi-head attention and layer normalisation, the core idea is this context-aware weighting mechanism \cite{vaswani2017attention}. Given its foundational nature and detailed coverage in the literature, we refer the reader to the original paper for architectural specifics.

Building upon the Transformer, several influential PLMs were developed. We briefly discuss two foundational models, BERT and XLNet, as their concepts and limitations motivate the subsequent choice of models like MPNet used later in this study.

\subsubsection{BERT}
The Bidirectional Encoder Representations from Transformers (BERT) model was a landmark development by Google AI \cite{devlin2019bert}. Its key innovation was learning deep \textit{bidirectional} representations by considering both the left and right context simultaneously in all layers. This contrasts with earlier models that typically processed text in only one direction. BERT achieves this primarily through its Masked Language Modelling (MLM) pre-training objective. During MLM, a percentage of input tokens (15\%) are randomly masked (replaced with a special \texttt{[MASK]} token), and the model learns to predict these original tokens based on the surrounding unmasked context \cite{devlin2019bert}. This forces the model to develop a deep understanding of word relationships and context from both directions. While BERT also used a Next Sentence Prediction task, MLM is its core contextual learning mechanism \cite{song2020mpnet}. BERT demonstrated state-of-the-art performance on numerous NLP tasks after fine-tuning \cite{devlin2019bert}.

However, BERT has limitations. A notable issue is the pre-train-to-fine-tune discrepancy: the artificial \texttt{[MASK]} token used during pre-training is absent in downstream tasks (i.e. during fine-tuning), potentially hindering performance. Furthermore, by predicting masked tokens independently, BERT doesn't explicitly model the dependencies between the masked tokens themselves, which can be an oversimplification for complex language understanding, as pointed out by (\citet{schank1972conceptual}) \cite{yang2019xlnet}.

\subsubsection{XLNet}

% XLNet was proposed to address BERT's limitations while retaining the benefits of bidirectional context \cite{yang2019xlnet}. It employs an autoregressive (AR) approach, meaning it predicts tokens sequentially, but introduces Permutation Language Modelling. Conceptually, instead of processing the sentence in its original order, XLNet considers all possible permutations (shuffled orders) of the input sequence. For each permutation, it predicts a token based only on the tokens that came before it in that specific shuffled order. By maximizing the prediction likelihood across all permutations, the model implicitly learns context from both original directions (left and right) for every token \cite{yang2019xlnet}. This approach avoids the artificial \texttt{[MASK]} token and explicitly models dependencies between predicted tokens within a given permutation. It leverages the Transformer-XL architecture for improved handling of longer sequences \cite{dai2019transformer}.
%
% While XLNet offered improvements, there still existed various downsides of XLNet, including its inherent inability to learn the bidirectional context due to its AR modelling approach. Thus, the development of PLMs continued, leading to models like MPNet \cite{song2020mpnet}, which attempts to unify the strengths of both masked language modelling (like BERT) and permutation-based autoregressive modelling (like XLNet). This motivates MPNet's consideration in this project (see Chapter~\ref{chap:Methodology}).

To provide context for the MPNet model used later in this project, we briefly discuss XLNet \cite{yang2019xlnet}, a model proposed to address certain limitations of BERT while retaining the benefits of bidirectional context. XLNet employs an autoregressive (AR) approach using Permutation Language Modelling. Conceptually, instead of processing sentences in their original linear order, XLNet maximises the likelihood over all possible permutations (factorisation orders) of the input sequence. For any given permutation, it predicts a token based only on the tokens preceding it in that specific order, thereby implicitly learning context from both original directions (left and right) for every token \cite{yang2019xlnet}. This method avoids BERT's artificial \texttt{[MASK]} token and explicitly models dependencies between predicted tokens within a given permutation. It also leverages the Transformer-XL architecture for improved handling of longer sequences \cite{dai2019transformer}.

While XLNet offered improvements over BERT, the development of PLMs continued, leading to models like MPNet \cite{song2020mpnet}. MPNet attempts to unify the strengths of both masked language modelling (like BERT) and permutation-based autoregressive modelling (like XLNet), which motivates its consideration in this project (see Chapter~\ref{chap:Methodology}). Further technical details regarding XLNet's mechanisms, including the two-stream self-attention, can be found in Appendix~\ref{app:bert_xlnet_details}.

\subsubsection{Considerations for Using Pre-trained Models}

Finally, while leveraging PLMs significantly accelerates research, it introduces reliance on third-party models. As noted previously, users must trust that the pre-trained parameters are sound and haven't been subject to issues like data contamination or malicious alterations. Therefore, favouring models with transparently documented data sources and training procedures is generally preferred when possible.


% \todo{Finish this off (the caption)}
% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Mathematics of BERT (detailed in XLNet paper) - DONE
%         \item Mathematics of XLNet (detailed in XLNet paper) - DONE
%         \item Mathematics of MPNet (detailed in MPNet paper) - DONE
%         \item Autoregressive vs autoencoder - DONE
%         \item Architectural changes in XLNet - SKIPPED
%         \item Compare XLNet, MPNet and BERT.
%         \item Architectural differences from BERT to MPNet.
%     \end{itemize}
% }

% \section{Dimensionality Reduction}\label{sec:dimred}
%
% Embedding spaces can get large, often on the scale of hundreds or even thousands of embedding dimensions. For example, the BERT base architecture uses an embedding dimension of 768 \cite{devlin2019bert}. Therefore, in applications dealing with such embedding spaces, this motivates a technique that reduces the dimensions to 2- or 3-dimensions but reflects the high-dimensional characteristics of the dataset. 
% Dimensionality reduction exploits the redundancy of the dataset by finding a smaller set of new features (variables) that are representative of the original data \cite{sorzano2014survey}.
%
% It has been shown that many high-dimensional spaces show `counter-intuitive geometrical properties', impacting data analysis methods, such as clustering. This phenomenon is referred to as the `curse of dimensionality' \cite{verleysen2005curse, hinneburg1999optimal}. Further, most clustering algorithms (such as those described in Section~\ref{sec:clustering}) have been shown to be inefficient in high-dimensional spaces, due to this curse \cite{hinneburg1999optimal}. Thus, in order to apply clustering (as we will, later in Chapter~\ref{chap:Methodology}) dimensionality reduction is once again motivated.
%
% This technique is not new in Statistics, with one of the most widely used dimensionality reduction techniques, Principal Component Analysis (PCA), introduced in (\citet{pearson1901liii}). However, with the increase in the quantity of available data and computational resources, researchers in fields such as Machine Learning and Statistics have developed a wide array of techniques aimed at tackling the dimensionality reduction problem \cite{sorzano2014survey}.
%
% In this section, we cover the background required in understanding techniques used in Chapter~\ref{chap:Methodology}, Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP).
%
% % \todo[inline, caption={}] {
% %     \begin{itemize}
% %         \item Talk why dimensionality reduction is needed: visualisation, curse of dimensionality, clustering? Figure out how to structure this well for the story.
% %         \item Talk about PCA and UMAP.
% %     \end{itemize}
% % }
%
% \subsection{PCA}
% Principal Component Analysis (PCA), as mentioned before, was first introduced in (\citet{pearson1901liii}) but later independently developed and named in (\citet{hotelling1933analysis}). PCA computes principal components, or variables which linearly decompose the features of the original dataset.
% The core idea is to find directions (the principal components) in the data that capture the maximum amount of variance, essentially identifying the axes along which the data spreads out the most \cite{abdi2010principal}. Each subsequent principal component is computed to be orthogonal (that is, statistically uncorrelated) to the previous ones, ensuring that the new components capture different aspects of the data's variance.
%
% This process can be described as an optimisation problem. Given a data matrix $X \in \mathbb{R}^{m \times n}$ (where $m$ is the number of data samples and $n$ is the number of original features), we want to find a new, lower-dimensional linear subspace, defined by the principal components, such that when the data $X$ is projected onto this subspace, the variance of the projected data is maximised. Equivalently, this can be viewed as finding the linear subspace that minimises the reconstruction error, specifically, the sum of squared distances (least-squares distance) between the original data points and their projections onto the subspace \cite{udell2016generalized}.
%
% More formally, if we want to reduce the dimensionality from $n$ features down to $k$ principal components (for $k < n$), we seek a matrix $C \in \mathbb{R}^{n \times k}$ whose columns represent these top $k$ orthogonal component directions. The projected data in the lower $k$-dimensional space is given by $XC$. We can approximate the original data $X$ by projecting back from this lower dimension using $XCC^T$. PCA aims to find the matrix $C$ that minimises the difference between the original data $X$ and this reconstructed data $XCC^T$, subject to the constraint that the column-wise components are orthogonal ($C^TC = I$, for the identity matrix $I$) \cite{udell2016generalized}:
% \begin{align}
%     \min_C || X - XCC^T ||_F^2 
%     \label{eq:pca} % Kept original label
% \end{align}
%
% A standard and efficient method for solving this optimisation problem and finding the principal components is through the Singular Value Decomposition (SVD) of the data matrix $X$ \cite{steinberger2005text, udell2016generalized}. SVD is a fundamental matrix factorisation technique that decomposes $X$ into three matrices:
% \begin{align}
%     X = USV^T \label{eq:svd} % Changed label to svd for clarity
% \end{align}
% Here, $U \in \mathbb{R}^{m \times k}$ and $V \in \mathbb{R}^{n \times k}$ are matrices with orthogonal columns, $S \in \mathbb{R}^{k \times k}$ is a diagonal matrix (only the diagonal contains non-zero elements) which contains the non-negative singular values ($s_i$) sorted in descending order, and $k$ is the rank of matrix $X$.
%
% The crucial link to PCA is that the columns of the matrix $V$ are precisely the principal component directions we seek. That is, the matrix $C$ in Equation~\ref{eq:pca}, containing the top $k$ principal components, corresponds to the first $k$ columns of $V$.
%
% Furthermore, SVD connects directly to the covariance matrix of the data ($X^TX$, assuming $X$ is centered). The decomposition of the covariance matrix is given by:
% \begin{align}
%     X^TX &= (USV^T)^T (USV^T) \\ % Show transpose step
%          &= (VS^T U^T) (USV^T) \\ % Apply transpose rules
%          &= V S^T (U^T U) S V^T \\ % Associativity
%          &= V S^T I S V^T \quad \text{(since } U^T U = I \text{)} \\
%          &= V (S^T S) V^T \\
%          &= V S^2 V^T\quad \text{(since } S \text{ is a diagonal matrix)}
%          \label{eq:svd_covariance} % Changed label, S^T S = S^2 as S is diagonal
% \end{align}
%
% This is the eigen-decomposition of the covariance matrix $X^TX$. It shows that the columns of $V$ are the eigenvectors (the principal components), and the diagonal entries of $S^2$  are the corresponding eigenvalues (i.e. $s_i^2 = \lambda_i$). Since the eigenvalues represent the variance of the data along the eigenvectors, the squared singular values ($s_i^2$) directly quantify the variance captured by each principal component \cite{udell2016generalized}. Therefore, computing the SVD of the data matrix $X$ provides both the principal components (in $V$) and the measure of variance associated with each component (in $S^2$) without explicitly forming the covariance matrix. Using a truncated SVD (keeping only the top $k$ components corresponding to the largest singular values) gives us a reduction in the dimensions.
%
%
% % \todo[inline, caption={}] {
% %     \begin{itemize}
% %         \item Talk about PCA mathematics and why its used for dim reduction
% %         \item Talk about elbowing and finding the most optimal (?)
% %     \end{itemize}
% % }
%
% \subsection{UMAP}\label{sec:umap}
%
% Unlike PCA, Uniform Manifold Approximation and Projection (UMAP) is generally considered a less directly interpretable dimensionality reduction method in terms of linear components. UMAP is a non-linear, stochastic `manifold learning technique' designed for dimensionality reduction, with theoretical foundations in Riemannian geometry and algebraic topology \cite{mcinnes2018umap}. As it places no constraints on the dimensionality of the input space, it is viable as a general-purpose technique in machine learning, often used for visualisation or as a pre-processing step for other algorithms like clustering. For the reader interested in the rigorous mathematics underpinning these topics, we refer them to the following textbooks: (1) (\citet{lee2003smooth}) for an introduction to differentiable and Riemannian manifolds; (2) (\citet{lee2006riemannian}) for Riemannian geometry in greater detail; and (3) (\citet{willard2012general}) for fundamental concepts of general topology.
%
% The UMAP algorithm operates under the assumption that the high-dimensional data (the input embeddings in our case) lies on or near a lower-dimensional manifold embedded within the high-dimensional space. The algorithm then attempts to find a low-dimensional representation (typically 2- or 3-dimensional) that best models the essential structure of this assumed manifold. \\
%
% \noindent Conceptually, the UMAP algorithm achieves this in two main stages \cite{mcinnes2018umap}:
% \begin{enumerate}
%     \item High-dimensional graph construction - First, UMAP constructs a representation of the relationships between data points in the original high-dimensional space. It does this by finding the nearest neighbours for each point and constructing a weighted graph where edge weights represent the estimated probability or likelihood that two points are closely connected on the underlying manifold. This step focuses on capturing the local structure and connectivity of the data.
    % \item Low-dimensional layout optimisation - Second, UMAP initialises the data points in the target low-dimensional space randomly. It then iteratively adjusts the positions of these low-dimensional points to create a graph structure that is as similar as possible to the high-dimensional graph constructed in the first stage. This optimisation process aims to minimise the difference (often measured using cross-entropy, a measure of divergence between probability distributions \cite{zhang2018generalized}) between the edge weights (probabilities of connection) in the high-dimensional and low-dimensional graphs. In other words, it tries to arrange the points in the low dimension such that connected points in the high dimension remain connected, and disconnected points remain disconnected.
% \end{enumerate}
%
% The outcome is a low-dimensional embedding that aims to preserve both the local connectivity structure and, to some extent, the broader global structure of the data's manifold. 
%
% The output of the UMAP algorithm can be significantly influenced by several hyperparameters, which allow the user to tune the dimensionality reduction process \cite{mcinnes2018umap}. The two most commonly adjusted parameters are:
% \begin{itemize}
%     \item \texttt{n\_neighbors}: Determines the size of the local neighbourhood UMAP considers when initially modelling the manifold structure in the high-dimensional space. It controls the balance between capturing local detail versus preserving the global structure of the data. Lower values focus UMAP on very local structure, potentially isolating small, distinct groups but risking the loss of broader relationships. Higher values encourage UMAP to consider more points as neighbours, leading to embeddings that better reflect the overall global structure but may obscure fine-grained details or merge closely related clusters.
%     \item \texttt{min\_dist}: This parameter controls the minimum distance between points in the final low-dimensional embedding space. It primarily affects the visual density of the resulting plot. Lower values allow points to be packed very tightly together, emphasizing cluster separation and density. Higher values force points further apart, creating more dispersed embeddings where the structure within potential clusters might be more visible, but the clusters themselves may appear less distinct.
%     \item \texttt{n\_components}: This parameter specifies the target dimensionality of the reduced embedding space. It is commonly set to 2 or 3 for visualisation purposes, but can be set to higher values if the UMAP output is intended as input for subsequent machine learning tasks, such as clustering. Changing this directly alters the number of dimensions in the output array. 
%     \item \texttt{metric}: This parameter defines the distance metric used to measure similarity or dissimilarity between data points in the original high-dimensional input space when finding nearest neighbours. The default is typically `euclidean', but other metrics like `cosine' (cosine distance) or `manhattan' (Manhattan distance) can be specified. The choice of metric is crucial and depends heavily on the nature of the input data. For instance, `cosine' distance is often preferred for high-dimensional, sparse data such as text embeddings, as it measures orientation rather than magnitude \cite{mcinnes2018umap}. Using an inappropriate metric can significantly distort the perceived local structure and thus the final embedding.
% \end{itemize}
%
% Selecting appropriate values for these parameters often involves experimentation based on the dataset and the specific goals of the dimensionality reduction (i.e. visualisation vs. input for clustering) \cite{mcinnes2018umap}. Implementations of UMAP, such as the widely used \texttt{umap-learn} Python library, are noted for being computationally efficient and scalable relative to other non-linear techniques like t-distributed Stochastic Neighbour Embedding (t-SNE) \cite{cai2022theoretical}, making UMAP a practical choice for many machine learning applications \cite{mcinnes2018umap}.
%
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.6\textwidth]{./images/umapElephant.png}
%     \caption{An illustration of UMAP applied (right) on samples taken from the skeletal structure of an elephant in 3-dimensions (left) with the UMAP hyperparameter values displayed. Highlighted, a black sphere (left) and circle (right), shows a chosen point mapped from 3-dimensions to 2. Source: (\citet{umapelephant}).}\label{fig:umapElephant}
% \end{figure}
%
% While powerful for visualisation and revealing non-linear structures, the resulting axes in the UMAP embedding do not have the same direct interpretation as the variance-ordered principal components derived from PCA.
% An example illustration of UMAP applied to an elephant skeletal structure, which is considered structured data, can be seen in Figure~\ref{fig:umapElephant}.

% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Talk about UMAP, the algorithm, the assumptions, the constraints
%         \item Use the visualisation of the elephant here.
%     \end{itemize}
% }

\section{Dimensionality Reduction}\label{sec:dimred}

The sentence embedding techniques discussed previously often produce high-dimensional vector representations, frequently on the scale of hundreds or even thousands of dimensions (i.e. 768 dimensions for BERT base \cite{devlin2019bert}). Directly analysing or visualising data in such high-dimensional spaces is challenging. Furthermore, many data analysis methods, including the clustering algorithms discussed later (Section~\ref{sec:clustering}), can suffer from performance degradation in high dimensions due to phenomena collectively known as the `curse of dimensionality' \cite{verleysen2005curse, hinneburg1999optimal}. This `curse' refers to counter-intuitive geometric properties of high-dimensional spaces that can make concepts like distance and density less meaningful \cite{verleysen2005curse}.

Therefore, dimensionality reduction techniques are often employed. These methods aim to find a lower-dimensional representation (typically 2-dimensional or 3-dimensional for visualisation) that captures the most important structure or variance present in the original high-dimensional data \cite{sorzano2014survey}. By exploiting redundancy in the data, they project points into a smaller feature space suitable for visualisation or input to subsequent algorithms like clustering \cite{sorzano2014survey, hinneburg1999optimal}. While numerous techniques exist, as shown by (\citet{sorzano2014survey}), this section focuses on two widely used methods employed in this project: Principal Component Analysis (PCA) \cite{pearson1901liii, hotelling1933analysis} and Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap}.

\subsection{PCA}\label{sec:pca}

Principal Component Analysis (PCA) is a linear method for dimensionality reduction, originally developed by (\citet{pearson1901liii}) and (\citet{hotelling1933analysis}). It operates by finding orthogonal axes, known as principal components, that capture the directions of maximum variance within the dataset \cite{abdi2010principal}. The first principal component captures the largest amount of variance, the second captures the largest remaining variance while being orthogonal (that is, statistically uncorrelated) to the first, and so on. Projecting the original data onto the first few principal components creates a lower-dimensional representation that retains most of the original data's variance \cite{abdi2010principal}.

Conceptually, PCA seeks a linear projection that either maximizes the variance of the projected data or, equivalently, minimises the `reconstruction error' which is the squared distance between original points and their projections \cite{udell2016generalized}. While the formal solution involves finding eigenvectors of the data's covariance matrix, a standard and efficient method to compute the principal components and their associated variance relies on the Singular Value Decomposition (SVD) of the (mean centred) data matrix $X$ \cite{steinberger2005text, udell2016generalized}. Here, mean centred refers to $X - \frac{1}{nm}\sum_i^n \sum_j^m X_{ij}$ for $X \in \mathbb{R} ^{n\times m}$. The SVD factorizes $X$ as $X = USV^T$, where the columns of $V$ represent the principal component directions, and the squared diagonal elements of $S$ represent the variance captured by each component \cite{udell2016generalized}. By keeping only the components associated with the largest singular values (a truncated SVD), PCA achieves dimensionality reduction while preserving maximal variance in a linear sense. A formal treatment of this as an optimisation problem can be found at Appendix~\ref{app:pca}.

\subsection{UMAP}\label{sec:umap}

Uniform Manifold Approximation and Projection (UMAP) is a more recent, non-linear dimensionality reduction technique based on manifold learning principles \cite{mcinnes2018umap}. Unlike PCA's linear projections, UMAP aims to capture potentially complex, non-linear structures in the data. It operates under the assumption that the high-dimensional data lies on or near a lower-dimensional manifold embedded within the higher-dimensional space \cite{mcinnes2018umap}. The goal is to find a low-dimensional embedding that faithfully represents the topological structure of this presumed manifold. While grounded in advanced mathematical concepts \cite{lee2003smooth, lee2006riemannian, willard2012general}, the intuition behind the algorithm can be understood through its two main conceptual stages \cite{mcinnes2018umap}:

\begin{enumerate}
    \item \textbf{High-dimensional Graph Construction} - UMAP first builds a weighted graph representation of the data in the original high-dimensional space. It identifies nearest neighbours for each data point and assigns edge weights based on the likelihood that points are closely connected on the underlying manifold. This step primarily focuses on capturing the local connectivity and structure of the data.
    \item \textbf{Low-dimensional Layout Optimisation} - UMAP then seeks a corresponding low-dimensional embedding where the graph structure is as similar in structure as possible to the high-dimensional graph. It iteratively adjusts the positions of points in the low-dimensional space to minimise the difference (often measured using cross-entropy, a measure of divergence between probability distributions \cite{zhang2018generalized}) between the connectivity probabilities in the high- and low-dimensional representations. This optimisation aims to keep points connected in the low-dimensional space if they were connected in the high-dimensional space, and separated if they were not.
\end{enumerate}

The resulting low-dimensional embedding aims to preserve both local neighbourhood structure and, importantly, aspects of the data's broader global structure \cite{mcinnes2018umap}. UMAP's output is influenced by several key hyperparameters:
\begin{itemize}
    \item \textbf{\texttt{n\_neighbors}} - Controls the size of the local neighbourhood considered. Lower values emphasize local structure, potentially revealing fine clusters, while higher values capture more global structure, potentially merging related groups \cite{mcinnes2018umap}.
    \item \textbf{\texttt{min\_dist}} - Affects the minimum separation between points in the low-dimensional embedding. Lower values create tighter, denser clusters, while higher values produce more spread out embeddings where intra-cluster structure might be more visible \cite{mcinnes2018umap}.
    \item \textbf{\texttt{n\_components}} - Defines the dimensionality of the output embedding (typically 2- or 3-dimensions for visualisation).
    \item \textbf{\texttt{metric}} - Specifies the distance measure used in the high-dimensional space (i.e. `euclidean', `cosine'). The choice is crucial, `cosine' distance is often preferred for high-dimensional text embeddings as it measures orientation rather than magnitude \cite{mcinnes2018umap}.
\end{itemize}

Selecting appropriate hyperparameters often requires experimentation (as explored in Section~\ref{sec:hyperparameterOpt}). UMAP implementations are generally computationally efficient compared to other non-linear methods like t-SNE \cite{cai2022theoretical, mcinnes2018umap}, making it practical for many applications. Figure~\ref{fig:umapElephant} provides a visual intuition of UMAP applied to structured data (sampled points from the skeletal structure of an elephant). Notably, structure preservation can be visually observed, as points from a tusk represent a shape which mirrors a tusk shadow in the 2-dimensional down-projection. However, while powerful for revealing non-linear structures, the axes of a UMAP projection do not have the direct variance interpretation of PCA's principal components.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/umapElephant.png}
    \caption{An illustration of UMAP applied (right) on samples taken from the skeletal structure of an elephant in 3-dimensions (left) with the UMAP hyperparameter values displayed. Highlighted, a black sphere (left) and circle (right), shows a chosen point mapped from 3-dimensions to 2. Source: (\citet{umapelephant}).}\label{fig:umapElephant}
\end{figure}


\section{Clustering}\label{sec:clustering}

Clustering is an unsupervised machine learning task that aims to group data points such that items within a group (a cluster) are more similar to each other than to those in other clusters. Common approaches include partitioning methods, which divide the data into a predefined number of clusters, density-based methods, which identify clusters based on regions of high data point density, and hierarchical methods, which build a tree-like structure of nested clusters \cite{kaufman2009finding}.

This section provides the necessary background for the clustering techniques employed later in this project. We will cover k-Medoids \cite{kmedoids}, a partitioning algorithm; Density-Based Spatial Clustering of Applications with Noise (DBSCAN) \cite{ester1996density}, a density-based algorithm; and Hierarchical DBSCAN (HDBSCAN) \cite{campello2013density}, which builds upon density and hierarchical concepts.

% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Talk about what clustering does, i.e. looking at trying to categorise or something - DONE
%         \item Talk about the curse of dimensionality and why clustering does not perform well. Then talk about what people usually do (dimensionality reduction via feature selection or algorithms) - BEFORE
%         \item Talk about k-Medoids (supervised), DBSCAN and HDBSCAN (unsupervised). - DONE, but wrong classification here.
%     \end{itemize}
% }

\subsection{k-Medoids}\label{sec:kmedoids}
The k-Medoids algorithm is a partitioning-based clustering method that aims to divide a dataset into k distinct clusters. The k-Medoids algorithm uses actual data points, known as medoids, as the centres of its clusters. It aims to minimise a cost function based on the sum of pairwise dissimilarities (that is, distances) between each point and the medoid of the cluster it is labelled under \cite{kmedoids}.

A classical algorithm for implementing k-Medoids is Partitioning Around Medoids (PAM), introduced by (\citet{kmedoids}). PAM operates in two main phases:

\begin{enumerate}
    \item \textbf{The build phase} - An initial set of $k$ data points are selected as the starting medoids. This selection aims to establish a reasonable initial clustering by choosing points that are relatively central within potential groups that exists, minimising the initial total dissimilarity.
    \item \textbf{The swap phase} - This phase iteratively refines the set of medoids. For each currently selected medoid $m$ and each non-medoid point data point $x$, the algorithm computes the change in total dissimilarity that would result from swapping $m$ and $x$. That is, setting $x$ as that centroid of $m$'s cluster. The swap that yields the greatest reduction across all possible pairs is finally performed. This process repeats until no swap can further reduce the total dissimilarity which indicates a local optimum has been reached.
\end{enumerate}

The PAM algorithm employs a greedy search strategy during the swap phase, which means it may converge to a local rather than a global optimum \cite{kmedoids}. The computational complexity of the swap phase is often cited as $O(k(n - k)^2)$ per iteration, where $n$ is the number of data points and $k$ is the number of clusters \cite{kmedoids}.

Appendix~\ref{app:pam} provides a high-level pseudocode description of the relatively straightforward PAM process. It takes the desired number of clusters $k$ and the dataset $X$ as input, and outputs the final set of medoids $M$ and set of clusters $X'$. The fact that $k$ is a parameter to the algorithm means that either the number of clusters must be known beforehand or must be found through hyperparameter optimisation.

% --- The Algorithm ---

% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Talk about algorithm
%         \item Talk about the parameters any effects when tuning these parameters.
%         \item Talk about drawbacks and benefits of this approach, citations needed.
%     \end{itemize}
% }

\subsection{DBSCAN} \label{sec:dbscan}

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is density-based clustering algorithm, unlike partitioning methods like k-Medoids \cite{ester1996density}. Instead of partitioning data around centres, DBSCAN groups together points that are closely packed in high-density regions, separated by sparser areas. A key characteristic of this approach is its ability to discover clusters of arbitrary shapes and to automatically determine the number of clusters present, rather than requiring it as a predefined input \cite{ester1996density}. The core intuition is that points belonging to the same conceptual group or category tend to lie close to each other, forming denser regions than considered noise or outliers.

The algorithm's behaviour and the concept of density is controlled by two hyperparameters \cite{ester1996density}:
\begin{itemize}
    \item \textbf{\texttt{Eps}} - This parameter defines the maximum distance between two points for one to be considered as in the neighbourhood of the other. It essentially sets the radius for identifying neighbours. A smaller \texttt{Eps} value requires points to be closer together to form a dense region, potentially leading to smaller, tighter clusters and classifying more points as noise. Conversely, a larger \texttt{Eps} value allows clusters to span sparser regions and potentially merge groups that might otherwise be distinct.
    \item \textbf{\texttt{MinPts}} - This parameter specifies the minimum number of points (including the point itself) required within a point's Eps-neighbourhood for that point to be considered a core point (i.e. an interior point of a dense region). \text{MinPts} defines the threshold for density. A higher value requires more points to constitute a dense core, potentially resulting in fewer clusters and more points being classified as noise. A lower value allows sparser groupings to qualify as clusters, potentially resulting in more clusters.
\end{itemize}

Determining appropriate values for \texttt{Eps} and \texttt{MinPts} often requires a heuristic (such as the one described by (\citet{ester1996density})) or domain knowledge, as DBSCAN is an unsupervised machine learning algorithm \cite{ester1996density}.

To understand the algorithm's operation, several core concepts are essential \cite{ester1996density}
\begin{itemize}
    \item \textbf{Eps-neighbourhood} - The Eps-neighbourhood of a point is the area within a radius of length \texttt{Eps} from that point.
    \item \textbf{Density-reachable} - A point $p$ is density reachable from another core point, if there is a sequence of points from $p$ to $q$ where the distance between any two points is less than or equal to \texttt{Eps} and every point in this sequence except $q$ must be a core point. This defines a chain of density connecting $p$ to $q$.
\end{itemize}

Additionally, based on the hyperparameters, DBSCAN categorises each point in the dataset into one of the three types \cite{ester1996density}. 
\begin{itemize}
    \item \textbf{Core point} - A point whose Eps-neighbourhood contains at least \texttt{MinPts} points, including itself. Core points are considered to be in the interior of a dense cluster.
    \item \textbf{Border point} - A point that is not a core point itself, but falls within the Eps--neighbourhood of at least one core point. Border points lie on the border of a cluster.
    \item \textbf{Noise point} - A point that is neither a core point nor a border point. These points typically reside in low-density regions and are not assigned to any cluster.
\end{itemize}

The DBSCAN algorithm, formally defined in (\citet{ester1996density}), iterates through the data points. When it encounters an unvisited point, it checks if that point qualifies as a core point by examining the set of points Eps-neighbourhood. If it is indeed a core point, then a new cluster is formed. This cluster is then expanded by finding all density-reachable points, starting from this initial core points. This involves recursively exploring the Eps-neighbourhood of all core points found during the expansion process. Any point reachable through a chain of core points connected by their Eps-neighbourhood becomes part of the cluster. If an unvisited point examined is not a core point, it is initially marked as noise, although it may be later reclassified as a border point, if found within the Eps-neighbourhood of a core point belonging to an expanding cluster. Points identified as density-reachable but not core points become border points assigned to this cluster. This procedure continues until all points in the dataset have been visited and assigned to a cluster or designated as noise.

An important consideration arises when a border point falls within the Eps-neighbourhood of core points belonging to different, simultaneously expanding clusters. Standard DBSCAN implementations typically resolve this based on the order of processing: the border point is assigned to the cluster associated with the first core point that `discovers' it during expansion \cite{ester1996density}. Once assigned, it is generally not reassigned, even if subsequently found to be reachable from another cluster's core point. Thus, while the core cluster structures are stable, the assignment of some border points can be implementation-dependent.

In general, DBSCAN offers significant advantages: it can identify clusters of arbitrary shape, is inherently robust to noise (which it explicitly labels) and does not require the number of clusters to be specified beforehand. However, it also has limitations. The algorithm's performance is sensitive to the choice of \texttt{Eps} and \texttt{MinPts} and it can struggle to correctly identify clusters of significantly varying densities within the same dataset. Furthermore, the average runtime complexity is $O(n\log n)$ where $n$ is the number of points in the dataset. \cite{ester1996density, campello2013density}. Like many distance-based algorithms, its effectiveness can degrade in very high-dimensional spaces due to the `curse of dimensionality' (see Section~\ref{sec:dimred}) \cite{hinneburg1999optimal, verleysen2005curse}.


% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Talk about algorithm
%         \item Talk about the parameters any effects when tuning these parameters.
%         \item Talk about drawbacks and benefits of this approach, citations needed.
%     \end{itemize}
% }

\subsection{HDBSCAN}\label{sec:hdbscan}

Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN), proposed by (\citet{campello2013density}), is an advancement over DBSCAN that generates a hierarchical clustering structure. This approach addresses a key limitation of DBSCAN: its reliance on a single global density threshold $\epsilon$ (equivalent to \texttt{Eps} in DBSCAN), which makes it difficult to identify clusters of varying densities simultaneously. HDBSCAN overcomes this by effectively exploring all possible density thresholds ($\lambda = \frac 1 \epsilon$, for all $\epsilon \in \big[0, \infty \big)$) to build a cluster hierarchy \cite{campello2013density}.

The method introduces two main contributions: (1) the HDBSCAN algorithm itself, which constructs this complete density-based hierarchy, represented as a dendrogram (a hierarchical tree-based representation of the dataset points); and (2) a novel measure of cluster stability used to extract a simplified, flat partition containing only the most significant and persistent clusters from this hierarchy \cite{campello2013density}. This extraction is framed as an optimisation problem maximising the total stability of the selected clusters. A significant advantage of HDBSCAN is its reduced sensitivity to hyperparameters as it primarily requires only $m_{\text{pts}}$ (equivalent to \texttt{MinPts} in DBSCAN), a parameter whose effect as a density smoothing factor is generally well-understood \cite{campello2013density}.

Importantly, the remaining portion of this section assumes a reasonably basic level of understanding of graph theory. For a great primer, the reader is referred to (\citet{harris2008combinatorics}).

To establish a formal link between the flat clustering of DBSCAN and the hierarchy of HDBSCAN, the authors first revisit the DBSCAN concept (referred to as DBSCAN\* in the paper for clarity). In this view, clusters are defined as connected components of core objects within a graph where edges connect mutually reachable core points based on $\epsilon$ and $m_{\text{pts}}$. Non-core objects are considered noise, which includes border points. This perspective allows density-based clusters to be seen as connected components of a level set of density (i.e. here, given a density threshold $\lambda = \frac 1 \epsilon$, a level set of density can be thought of as a set with points with density above that value) \cite{campello2013density}.

HDBSCAN builds upon the defining concepts relative to $m_{\text{pts}}$, only allowing $\epsilon$ (or $\lambda$) to vary \cite{campello2013density}:
\begin{itemize}
    \item \textbf{Core distance ($d_{\text{core}}(x)$)} - For a data point $x$, this is the distance to its $m_{\text{pts}}$-th nearest neighbour (including itself). It serves as a measure of the local density around $x$. A smaller core distance implies a denser region.
    \item \textbf{Mutual reachability distance ($d_{\text{mreach}}(x_p, x_q)$)} - Defined for two data points $x_p, x_q$, this is the maximum of their respective core distances and their pairwise distance defined as:
        \[d_{\text{mreach}}(x_p, x_q) = \max \{ d_{\text{core}}(x_p), d_{\text{core}}(x_q), d(x_p, x_q) \}\]
    This adjusted distance measure ensures that points in sparse regions are effectively pushed further apart, reflecting density differences.
\end{itemize}

The HDBSCAN algorithm constructs the cluster hierarchy based on these concepts. Conceptually, it first computes the core distance for all points. Then, it builds a graph where all points are vertices, and the weight of the edge between any two points is their mutual reachability distance. The algorithm finds the Minimum Spanning Tree (MST) of this complete graph. The cluster hierarchy (dendrogram) is then extracted from this MST by considering edges in decreasing order of weight (distance). As edges are removed (corresponding to decreasing the density threshold $\lambda$ or increasing the distance threshold $\epsilon$), connected components in the remaining graph represent clusters at that specific density level. The hierarchy captures how these components merge as the density threshold decreases \cite{campello2013density}. This process can be implemented efficiently, with a typical time complexity of $O(n^2)$ if pairwise distances are precomputed or provided, or potentially $O(dn^2)$ depending on MST construction details if distances are computed on the fly from $d$-dimensional data \cite{campello2013density}.

To extract a meaningful flat partition from this hierarchy, HDBSCAN introduces the concept of cluster stability. This measure quantifies how persistent a cluster $C_i$ is across different density levels ($\lambda = \frac 1 \epsilon$). That is, as you vary the density level $\lambda$, this measures the longest period in which the cluster exists without disappearing or splitting. The stability $S(C_i)$ is calculated by summing, for each point $x_j$ belonging to cluster $C_i$, the range of density levels for which that point remains part of that specific cluster \cite{campello2013density}. More formally, using the notation from the paper where $\lambda_{\text{min}}(C_i)$ is the density level at which cluster $C_i$ appears (thought of as is `born') and $\lambda_{\text{max}}(x_j, C_i)$ is the density at which point $x_j$ leaves cluster $C_i$, either through becoming noise or part of some child cluster as density increases \cite{campello2013density}:

\begin{align}
S(C_i) = \sum_{x_j \in C_i} (\lambda_{\text{max}}(x_j, C_i) - \lambda_{\text{min}}(C_i))
\label{eq:hdbscan-stability}
\end{align}

Equation~\ref{eq:hdbscan-stability} adapts the concept of `excess of mass' (the total density contained in a cluster after it is formed) \cite{muller1991excess} to a set of discrete data points and density levels derived from the hierarchy. Clusters where points persist over a larger range of $\lambda$ values (i.e. are more `stable' across varying density thresholds) will have higher stability scores.

Using this concept of stability, HDBSCAN selects the optimal flat clustering by solving an optimisation problem: finding a maximal disjoint set of clusters from the hierarchy  such that the sum of stabilities $S(C_i)$ is maximised. Here a maximal disjoint set of clusters refers to the fact that no two cluster is an ancestor or descendant of another selected cluster, ensuring a flat partition covering all points down certain branches \cite{campello2013density}. The paper provides an efficient algorithm that traverses the simplified cluster tree bottom-up and then top-down to find this optimal set of stable clusters in linear time, relative to the size of the simplified tree. The simplified cluster tree is obtained after applying a minimum cluster size $m_{\text{dSize}}$, typically set equal to $m_{\text{pts}}$. This process effectively performs the optimal `local cuts' in the cluster dendrogram at different levels of hierarchy, based on this notion of cluster persistence.

% \section{Clustering Evaluation}\label{sec:clusteringEval}
% \todo[inline] {Maybe this can be moved to the methodology?}
% Furthermore, as evaluating the quality of unsupervised clustering results is crucial, we will also introduce several common clustering evaluation heuristics used in this work: Inertia, the Silhouette coefficient \cite{rousseeuw1987silhouettes}, the Davies-Bouldin Index \cite{davies1979cluster}, and the Calinski-Harabasz Index \cite{calinski1974dendrite}.
% \todo[inline, caption={}] {
%     Maybe this should be later on in the methodology? 
%     \begin{itemize}
%         \item Motivate the need for clustering evaluation.
%         \item Just highlight mathematics of each algorithm and mention the need for them.
%     \end{itemize}
% }
% \subsection{Inertia}
% \subsection{Silhouette}
% \subsection{Davies-Bouldin Index}
% \subsection{Calinski-Harabasz Index}
% \section{Maybe Optuna}

\chapter{Automatic Categorisation and Label Generation}\label{chap:Methodology}

\section{Overview}

% \todo{Make a diagram that showcases the process (and all the hyperparameter choices)}

The process of transforming an unlabelled, uninformative Operalog into an informed, induced-label tagged dataset can be broadly characterised into five major steps. These steps are: (1) initial pre-processing and normalisation of the \texttt{FaultDescription} natural language text to standardise text and reduce noise; (2) embedding the cleaned text into a high-dimensional embedding space to capture semantic meaning numerically; (3) low-dimensional projection of the embedding space while respecting the higher-dimensional topological manifold (that is, aiming to preserve the structure and relationships present in the original high-dimensional space) to enable effective clustering while preserving structure; (4) performing clustering on the low-dimensional data, optimising clustering using clustering metrics as heuristics for the `goodness' of a cluster to group similar fault descriptions automatically; and (5) using natural language processing to induce label names for each cluster and tagging each entry with a meaningful label identifying a fault category.  Here, we choose to solely consider the \texttt{FaultDescription} field as the vast majority of \texttt{FaultRepair} fields are either empty or less than or equal to 5 characters long. A diagram of the pipeline can be seen in Figure~\ref{fig:pipeline}, which illustrates the pipeline described above.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{pipeline.png}
    \caption{An overview of the end-to-end machine learning pipeline, outlining each key stage and highlighting which sections required hyperparameter optimisation.}\label{fig:pipeline}
\end{figure}

Several hyperparameters affect the performance of the label generating process. Examples include model or algorithm specific parameters (i.e. the dimensionality reduction or clustering algorithms) and clustering `goodness' heuristics used. These hyperparameters are tuned using Optuna \cite{akiba2019optuna}, an automatic optimisation framework specifically designed for machine learning applications and allows for multi-objective optimisation. This hyperparameter optimisation process determines `best' induced categories, and thus labels, for each issue type. 

Additionally, as this project is an early stage exploration of auto-categorisation for the ISIS Operalog, the focus is on entries of ion source \texttt{Equipment} type. This enables the project to focus on refining the approach in a scoped manner with a structured goal in mind - to progress the auto-categorisation effort on the ISIS Operalog. The subsequent sections of this chapter detail each stage of this end-to-end pipeline, developed with the goal of producing an effective auto-categorisation model for the Operalog data.

\section{Natural Language Pre-processing}\label{sec:preproc}

The aim of pre-processing the unstructured, English natural language text data from \\ \texttt{FaultDescription} is to normalise and prepare it for text embedding. Normalisation allows many syntactic permutations or augmentations of a text to be collapsed into one standardised form. This allows for consistent results as it ensures texts that are syntactically different but semantically similar will be mapped by embedding model to the same value. For pre-processing, and the rest of the pipeline, each log entry's \texttt{FaultDescription} is considered one text.

Tokenisation is one of the earliest stages of natural language processing \cite{tabassum2020survey, sammons2016edison}. To perform tokenisation, we use Tensorflow's \texttt{UnicodeScriptTokenizer} \cite{tensorflow2015whitepaper}. Tensorflow is `an end-to-end platform for machine learning', which provides efficient machine learning algorithm implementations. A tokeniser, such as this, splits the text into sub-units called `tokens' \cite{gefenstette1999tokenization}. These tokens are useful to pass to subsequent natural language processing stages which is, in our case, text embedding. \texttt{UnicodeScriptTokenizer}, a specialised version of Tensorflow's \texttt{Tokenizer}, tokenises `UTF-8 strings by splitting where there is a change in Unicode script' \cite{tensorflow2015whitepaper}.

Once tokenised, stop-word removal is performed. Stop-words are non-informative words such as articles, prepositions and pronouns. Therefore removal of these words tends to help models have access to informative contexts and reduce noise \cite{silva2003importance}. We consider the standard English stop-words from NLTK \cite{bird2009natural} and some context specific stop-words. This includes `ion source' which does not provide any informative contexts as we have already scoped our dataset to ion source \texttt{Equipment} and `breakdown' which appears in just over $50\%$ of entries. 

As mentioned previously, there are a few non-trivial abbreviations used by the operational crew when writing a log entry. These abbreviations are normalised into a standard English word or phrase (see Table~\ref{tab:abbrevCrew}).

Furthermore, punctuation tokens are converted into one of two categories: end-of-sentence (\texttt{EOS}) tokens or regular punctuation (\texttt{PUNC}) tokens, which are self-explanatory. The goal for this is to normalise punctuation to either signify sentence boundaries or collapse into one token. The majority of the \texttt{FaultDescription} text does not typically utilise punctuation to convey additional meaning. However, this is not trivial to see and thus the step was made parameterisable via a flag which decides whether punctuation mapping is enabled. Similarly, text casing normalisation (converting all text to lower-case) follows the same procedure. 

Just in terms of ion source \texttt{Equipment}, the pre-processing is applied across 1251 log entries, ranging from 2009 - 2023. Tokenisation and operating on tokens is computationally intensive, thus the solution operates on tensors, leveraging Tensorflow capabilities to perform GPU-based compute, which speeds up machine learning based applications \cite{tensorflow2015whitepaper, baldini2014predicting}.

To improve computational efficiency, Boolean flags controlling text casing and punctuation removal were excluded from the hyperparameter optimisation phase (Section~\ref{sec:hyperparameterOpt}). Incorporating them would require the computationally expensive pre-processing step to be repeated for each Optuna \cite{akiba2019optuna} trial, a hyperparameter framework used in Section~\ref{sec:hyperparameterOpt}, even for minor parameter adjustments unrelated to these flags. Given that there are only a few combinations for these Boolean settings, they are more efficiently managed as user-selectable options via the CLI application (Section~\ref{sec:CLI}), ensuring pre-processing is performed only once per selected configuration.

\rowcolors{2}{gray!10}{white}
\begin{table}[htbp]
    \fontsize{8}{12}\selectfont
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{c | c}
        \toprule
        \textbf{Abbreviation Regular Expression} & \textbf{Mapped word or phrase} \\
        \midrule
        {o/p} & {output} \\
        {i/s} & {ion source} \\
        {(b/down | break-down | b\textbackslash down | b/d)} & {breakdown} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Operational crew abbreviation mapping.}
    \label{tab:abbrevCrew}
\end{table}


\section{Text Embedding Model Selection and Application}
After pre-processing, the cleaned and normalised \texttt{FaultDescription} text requires transformation into a numerical format suitable for analysis. Sentence embedding (introduced in Section~\ref{sec:sentenceEmbedding}) provides this by mapping text into a high-dimensional vector space, where semantic relationships are preserved. This is crucial for the project's goal: automatically categorising log entries by grouping semantically similar entries, which corresponds to finding similar fault types. 

\subsection{Dataset and Hardware Driven Requirements}\label{sec:req}
Effective categorisation relies on an embedding model that can accurately capture the syntactic and semantic nuances of these technical descriptions. Figure~\ref{fig:ionSourceTokenCountDist} highlights that, although the average \texttt{FaultDescription} entry has roughly 7 tokens, in the dataset, over 60\% of entries have less than 5 tokens and around 10\% of the 1251 entries have over 20 tokens. This includes entries which stretch to multiple sentences. Additionally, the vocabulary is heavily domain specific (i.e. `ion source') and a single out-of-place adjective can reverse the meaning of a fault report (i.e. `beam loss' versus `no beam loss').  

Furthermore, the hardware available for research is detailed in Table~\ref{tab:researchHw}. Some models, such as large language models with a parameter counts in hundreds of millions or billions may exceed the 6GB GPU VRAM limit during inference. Therefore, we restrict consideration to models whose inference memory footprint and compute latency remain feasible on this hardware.

As a result, we require a model that (1) handles sequences that contain both long- and short-term dependencies but still establish semantic relationships effectively; (2) is able to capture the semantic contextual meaning from the entire sequence - that is, bidirectionally; and (3) is able to be feasibly loaded and run on the research hardware specified in Table~\ref{tab:researchHw}.

\rowcolors{2}{gray!10}{white}
\begin{table}[htbp]
    \fontsize{8}{12}\selectfont
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{p{3cm}|p{10cm}}
        \toprule
        \textbf{Hardware} & \textbf{Description} \\
        \midrule
        {CPU} & {11th Gen Intel i7-11800H (16 core) @ 4.600GHz} \\
        {RAM} & {64GB} \\
        {GPU} & {NVIDIA GeForce RTX 3060 Mobile / Max-Q} \\
        {GPU VRAM} & {6GB} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Research hardware description.}
    \label{tab:researchHw}
\end{table}


\subsection{Candidate Models}

With these constraints in mind, we look at the recent state-of-the-art embedding models and highlight models that were not in consideration. Candidate models were compared on the constraints above and selected based on pre-fine-tuned models published on HuggingFace (so we can use them directly, without re-training) \cite{wolf2019huggingface}. HuggingFace is an open-source platform for machine learning related development, where researchers can upload pre-trained and fine-tuned model parameters. The models that ranked high on clustering tasks on the Massive Text Embedding Benchmark (MTEB) leaderboard, which compares more than 100 text and image embedding models across 132 tasks of 9 categories - 17 of which are clustering tasks \cite{muennighoff2022mteb}. 

Two BERT-family sentence-embedding models satisfy the constraints above, MPNet and Nomic. Both models were used with their out-of-the-box HuggingFace settings (i.e. maximum token lengths of 384 and 8192, respectively and embedding dimension of 768) \cite{song2020mpnet, nussbaum2024nomic, nussbaum2024nomic1.5}.

\paragraph{MPNet - an overview}
512-token context window with a 768 embedding dimension. Unifies BERT's masked and XLNet's permutation language modelling pre-training objectives. This gives better positional awareness for tokens that are further away from each other \cite{song2020mpnet}. MPNet has around 109 million parameters, meaning it is able to load onto the research hardware. The HuggingFace model path is: \texttt{sentence-transformers/all-mpnet-base-v2} and achieves a clustering score of 40.77, ranking at 98 on the MTEB leaderboard, at time of writing.

\paragraph{Nomic - an overview}
Current state-of-the-art 8192-token context window with up to a 768 embedding dimension. Utilises rotary positional embeddings and flash attention that provide long-context support while remaining relatively small, with around 137 million parameters. Task prefixes (i.e. \texttt{classification:}, \texttt{clustering:}, etc.) encourage distinct semantic sub-spaces, thus we prepend all inputs passed to this model with `clustering:'. Specifically, we use version 1.5 which takes advantage of Matryoshka Representation Learning (MRL) and is able to encode coarse versus fine-grained semantic information in embedding vector sub-spaces \cite{nussbaum2024nomic, nussbaum2024nomic1.5}. The HuggingFace model path is: \texttt{nomic-ai/nomic-embed-text-v1.5} and achieves a clustering score of 41.55, ranking at 54 on the MTEB leaderboard, at time of writing.

\paragraph{Excluded Models}
The following embedding models were evaluated but ultimately excluded, either because they exceeded our 6GB VRAM budget or failed to meet clustering performance thresholds in pilot runs (see Table~\ref{tab:excludedModels}). Excluding these ensures all retained models both fit our hardware and deliver the minimum clustering quality we require.

\rowcolors{2}{gray!10}{white}
\begin{table}[htbp]
    \fontsize{8}{11}\selectfont
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{p{1.6cm} p{1.7cm} p{7.3cm} p{2.5cm}}
        \toprule
        \textbf{Model} & \textbf{Parameters} & \textbf{Exclusion Reason} & \textbf{Reference}\\
        \midrule
        Qwen2‑7B   & 7 billion   & Requires more than 6 GB VRAM at inference. & (\citet{yang2407qwen2}) \\
        Mistral‑7B & 7 billion   & Similar VRAM overrun on consumer GPUs. & (\citet{wang2023improving}) \\
        e5‑large   & 440 million & Scored only 32 points on MTEB’s clustering  benchmark. & (\citet{wang2024multilingual})\\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Models considered but excluded from the embedding pipeline.}
    \label{tab:excludedModels}
\end{table}


% \subsection{Detailed Rationale}
% In this section, we cover the selected models in a higher level of technical detail.
%
% \subsubsection{MPNet}\label{sec:rationalempnet}
%
% MPNet aims to combine both BERT's and XLNet's pre-training objectives to achieve the benefits without the drawbacks of each individual models. This hybrid approach is particularly attractive for this project because the Operalog dataset contains a mixture of short, concise entries and longer, more descriptive entries detailing causal factors or sequences of events. Effectively understanding these requires capturing both fine-grained token relationships and broader contextual meaning.
%
% While detailed descriptions of BERT and XLNet are in Section~\ref{sec:BERT}, the key challenge MPNet addresses is that masked language modelling (BERT) assumes independence between masked tokens, potentially losing valuable dependency information, and permutation language modelling (XLNet), while capturing dependencies, can have inconsistencies regarding positional information between pre-training and fine-tuning \cite{song2020mpnet, yang2019xlnet}.
%
% MPNet proposes a 'unification' solution that addresses these issues. It achieves this by intelligently permuting the order of input tokens and applying masking in a way that allows the model to learn bidirectional context (like BERT) while also modelling the dependencies between tokens (like XLNet) \cite{song2020mpnet}.
% This capability is crucial for the Operalog data, where understanding the relationship between specific technical terms, even if separated by other words, is key to identifying the true nature of a fault.
% This unified objective function is formally defined in Appendix~\ref{app:mpnet_maths} (Equation~\ref{eq:mpnet_objective_appendix}).
%
% To facilitate this objective, MPNet structures the input by rearranging tokens based on the permutation and partitioning them into non-predicted and predicted segments, using appropriate masking (conceptually illustrated in Figure~\ref{fig:mpnet}). This allows the model to condition its predictions effectively. The precise mechanism for structuring the input is detailed with an example in Appendix~\ref{app:mpnet_maths} (Equation~\ref{eq:mpnet_input_appendix}).
%
% Similarly to XLNet, MPNet also employs a two‑stream self‑attention mechanism (content vs
% query) but modifies the attention mask to enable the model to see the entire input sequence. The modified attention masks allow the query stream to access the
% entire sequence during pre‑training, which better aligns with downstream (fine-tune) tasks \cite{song2020mpnet}. This also matters in our use-case as, using a pre-fine-tuned model, we want a pre-training objective that reflects inference usage, rather than focus on fine-tuning discrepancies.
% Because many Operalog entries reiterate initial fault cues later in the same sentence (e.g. describing a corrective action by name), allowing the query stream full sequence visibility helps MPNet embed those long‑range dependencies.
%
% Similarly to XLNet, MPNet employs a two-stream self-attention mechanism (distinguishing content and query information). However, MPNet modifies the attention masks to allow the query stream to access the entire input sequence during pre-training \cite{song2020mpnet}. This modification better aligns the pre-training conditions with how the model is used during inference for downstream tasks like embedding generation.
% This is relevant for our use-case as Operalog entries sometimes re-iterate initial fault symptoms or mention corrective actions later in the description (i.e. `ion source unstable ... source change required.'). Allowing the model full sequence visibility helps MPNet embed those long-range dependencies, leading to more accurate embeddings for capturing the overall semantic meaning.
%
% Using these changes, (\citet{song2020mpnet}) has quantitatively shown that MPNet leverages 92.5\% tokens (as opposed to 85\% by BERT) and 100\% of positions (as opposed to 92.5\% by XLNet) of input sequences (under the assumption they predict the same 15\% masked token amounts). In addition, they have also shown that MPNet outperforms the state-of-the-art, at the time of publishing, in almost all GLUE benchmark tasks (an industry standard language model benchmark, with 9 natural language understanding tasks) and various other benchmarks \cite{wang2018glue}. 
%
% For further information on the MPNet architecture and pre-training, the reader is referred to the original paper by (\citet{song2020mpnet}).
%
% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.493\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{MPNetMLM.png}
%         \caption{Masked language modelling (left) compared to the permuted order (right, unified view).}
%         \label{fig:MPNetMLM}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}[b]{0.455\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{MPNetPLM.png}
%         \caption{Permutation language modelling (left) compared to the permuted order (right, unified view).}
%         \label{fig:MPNetPLM}
%     \end{subfigure}
%     \caption{Comparison of both masked and permutation language modelling pre-training objectives. Here, tokens are denoted $X_i$, masked tokens are denoted \texttt{[M]} and positional encoding is denoted $P_i$. Source: (\citet{song2020mpnet}).}
%     \label{fig:mpnet}
% \end{figure}
%
%
% \subsubsection{Nomic}\label{sec:nomic}
%
% The second model evaluated is Nomic. This model, released in 2024, represents a state-of-the-art, open-source text embedding model specifically designed for long-context understanding and reproducibility \cite{nussbaum2024nomic, nussbaum2024nomic1.5}. Version 1.5, used in this project, builds upon the initial release with features like Matryoshka Representation Learning (MRL) and multi-modal support.
%
%
% The relevant adaptions and justifications are listed below.
% \begin{itemize}
%     \item Long context handling via RoPE - Nomic utilise Rotary Positional Embeddings (RoPE) instead of traditional positional embeddings \cite{su2024roformer, nussbaum2024nomic, vaswani2017attention}. RoPE encodes positional information using rotations, which has been shown to improve model generalisation to sequences of lengths not seen during pre-training (whether longer or shorter) \cite{su2024roformer}. This is a significant advantage for the Operalog dataset, where \texttt{FaultDescription} entries vary greatly in length (Figure~\ref{fig:hists}), helping ensure effective semantic understanding by the model.
%     \item Computational efficiency - Despite its capabilities, Nomic v1.5 maintains a relatively small parameter count of around 137 million compared to other leading models, whose parameter counts typically exceed 1 billion significantly. It also incorporates architectural optimisations like FlashAttention \cite{dao2022flashattention}, an I/O-aware attention mechanism that reduces memory usage and latency during inference \cite{nussbaum2024nomic}. These factors were significant in selecting Nomic, as they allow the model to run efficiently within the constraints of the available 6GB GPU VRAM (Table~\ref{tab:researchHw}), unlike larger models such as Qwen2 or Mistral-7B (Table~\ref{tab:excludedModels}).
%     \item Task-specific optimisation - Nomic supports task-specific prefixes prepended to the input text. For this project, the `clustering:' prefix was used for all \texttt{FaultDescription} entries fed into the Nomic model. This technique enables the model to produce embeddings specifically optimised for clustering tasks, by leveraging distinct semantic embedding subspaces learned during pre-training \cite{nussbaum2024nomic}. Utilising this prefix aims to enhance the quality of the embedding space specifically for the goal of grouping similar fault types, potentially leading to more coherent and meaningful clusters.
%     \item Matryoshka Representation Learning (MRL) - Nomic version 1.5 incorporates MRL, which trains the model to produce nested embeddings where shorter prefixes of the full embedding vector still form meaningful representations \cite{kusupati2022matryoshka, nussbaum2024nomic1.5}. While the full 768-dimension embedding was used for this project, with MRL creating a structured embedding space, it opens up the possibility of detecting patterns that aid fault categorisation  more easily.
%     \item Modern Architecture Choices - Nomic also incorporates other modern design choices adapted from BERT, such as removing the next sentence prediction pre-training objective, which simplifies pre-training and allowing for longer contiguous text processing, and using 0\% dropout, reflecting findings on training with very large datasets \cite{nussbaum2024nomic, liu2019roberta, xue2023repeat}. These contribute to its overall efficiency and performance.
%     \item Benchmark Performance - Nomic version 1.5 demonstrates strong performance on the MTEB leaderboard, particularly for its size, ranking well on clustering tasks and providing empirical validation for its selection \cite{muennighoff2022mteb, nussbaum2024nomicelo}.
% \end{itemize}
%
% For further technical details on the Nomic architecture, pre-training and MRL, readers are referred to the relevant publications \cite{nussbaum2024nomic, nussbaum2024nomic1.5, kusupati2022matryoshka}.

\subsubsection{Detailed Rationale}\label{sec:rationale_mpnet_nomic} % 

In this section, we elaborate on the rationale for selecting MPNet and Nomic as the primary embedding models for evaluation, connecting their specific properties to the requirements identified in Section~\ref{sec:req} and the conceptual background from Chapter~\ref{chap:nlpbg}.

\paragraph{MPNet Justification}
MPNet \cite{song2020mpnet} was for its hybrid pre-training approach, combining masked and permutation language modelling benefits to potentially mitigate drawbacks of earlier methods like BERT \cite{devlin2019bert} and XLNet \cite{yang2019xlnet}. This hybrid nature seemed particularly well-suited to the Operalog dataset for several reasons.

\begin{enumerate}
    \item MPNet's unified objective learns bidirectional context while modelling token dependencies \cite{song2020mpnet}, which appeared suitable for the Operalog's mix of short and long entries (Section~\ref{sec:req}). This capability is potentially advantageous for capturing relationships between technical terms that might be separated within longer \texttt{FaultDescription}s. The formal definition of MPNet's objective function and an example of its input structuring are provided in Appendix~\ref{app:mpnet_maths} for completeness (see Equations~\ref{eq:mpnet_objective_appendix} and \ref{eq:mpnet_input_appendix}). 

    \item MPNet's modified attention mechanism aims to reduce the pre-train and fine-tune discrepancy, compared to earlier models \cite{song2020mpnet}. Allowing the model component responsible for prediction, query stream (see Appendix~\ref{sec:twostream}), access to the full sequence during pre-training potentially aligns better with its use for direct embedding generation as an off-the-shelf model in this project. This will aid long-range dependency capture.

    % \item MPNet addresses potential discrepancies between pre-training and downstream tasks. While XLNet's permutation approach avoids BERT's reliance on the artificial \texttt{[MASK]} token, MPNet further refines the mechanism by modifying the two-stream attention masks \cite{song2020mpnet}. These modifications allow the model components responsible for prediction (the query stream) to access information from the entire sequence during pre-training, better mirroring how the model is used during inference for embedding generation \cite{song2020mpnet}. This alignment is advantageous when using an off-the-shelf pre-trained (and further fine-tuned) model directly for embedding, as done in this project. This full sequence visibility aids in capturing long-range dependencies potentially present in Operalog entries where initial symptoms might be reiterated or linked to actions later in the description \cite{song2020mpnet}.

    \item MPNet demonstrated strong performance on various NLP benchmarks at the time of its release, including the GLUE benchmark \cite{wang2018glue, song2020mpnet}, suggesting a robust general language understanding capability applicable to the technical language in the Operalog. Its parameter count (approximately 109 million) also made it feasible for the available hardware (Table~\ref{tab:researchHw}).
\end{enumerate}

\paragraph{Nomic Justification}
Nomic \texttt{nomic-embed-text-v1.5} \cite{nussbaum2024nomic, nussbaum2024nomic1.5}, was selected as a recent state-of-the-art open-source embedding models, offering several potentially beneficial features:

\begin{enumerate}
    % \item A key advantage is its explicit design for long-context understanding. It utilizes Rotary Positional Embeddings (RoPE) \cite{su2024roformer} instead of traditional absolute or relative positional embeddings \cite{vaswani2017attention}. RoPE has been shown to improve generalisation to sequence lengths not seen during training \cite{su2024roformer}. Given the significant variation in the length of \texttt{FaultDescription} entries in the Operalog (Figure~\ref{fig:hists}), Nomic's ability to handle contexts up to 8192 tokens effectively was a major consideration - potentially offering better semantic representation for longer, more detailed entries compared to models with shorter context windows like MPNet (512 tokens).
    
    \item Its design for long contexts (up to 8192 tokens), using Rotary Positional Embeddings (RoPE) \cite{su2024roformer} is advantageous given the significantly variable lengths of Operalog entries (Figure~\ref{hists}), potentially offering better representation for detailed logs compared to MPNet's 512-token window \cite{song2020mpnet}. 

    % \item Computational efficiency was another critical factor. Despite its long-context capability, Nomic v1.5 has a relatively low parameter count (approximatly 137 million) and incorporates optimisations like FlashAttention \cite{dao2022flashattention}, designed to reduce memory usage and latency \cite{nussbaum2024nomic}. This made it feasible to run inference within the project's 6GB GPU VRAM constraint (Table~\ref{tab:researchHw}), unlike significantly larger multi-billion parameter models considered (Table~\ref{tab:excludedModels}).

    \item Despite its long context, Nomic's computational efficiency, achieved through a moderate parameter count (roughly 137 million) and optimisations like FlashAttention \cite{dao2022flashattention} (reducing memory usage and latency), ensured feasibility within the project's 6GB VRAM constraint (Table~\ref{tab:researchHw}), unlike large models (Table~\ref{tab:excludedModels}).

    \item Nomic also offers task-specific optimisation via input prefixes \cite{nussbaum2024nomic}. By prepending `\texttt{clustering:}' to each \texttt{FaultDescription} entry, we aimed to leverage specific semantic subspaces within the model potentially optimised during its pre-training for clustering tasks. This was hypothesised to generate embeddings more suitable with the primary goal of grouping similar fault types compared to using a generic embedding.

    \item Nomic v1.5 incorporates Matryoshka Representation Learning (MRL) \cite{kusupati2022matryoshka, nussbaum2024nomic1.5}. While this project utilised the full 768-dimension embeddings, MRL imbues the embedding space with a nested structure where shorter prefixes remain meaningful. This structured property might implicitly aid downstream tasks like clustering by organizing semantic information hierarchically, although exploring different MRL embedding lengths was outside the scope of this project.

    \item Nomic's demonstrated strong performance for its size on benchmarks like MTEB, including clustering tasks, provided empirical validation for its consideration \cite{muennighoff2022mteb, nussbaum2024nomicelo}. 
\end{enumerate}

\subsection {Section Summary}

By evaluating both MPNet (representing a mature, unified BERT/XLNet approach) and Nomic (representing recent advancements in long-context efficiency and specialised embeddings), which meet the requirements criteria set in Section~\ref{sec:req}, this project aimed to explore different effective strategies for representing the challenging Operalog text data. 

\section{Embedding Dimensionality Reduction}

% Motivated by the need for visualisation of the high-dimensional embedding space we explore dimensionality reduction techniques. Specifically we justify the applicability of two reduction techniques, Principal Component Analysis (PCA) \cite{pearson1901liii, hotelling1933analysis} and Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap}. Through justification from visualisation and literature, we decide to continue using UMAP in our pipeline. Apart from these two techniques, there was also consideration of t-distributed Stochastic Neighbour Embedding (t-SNE). However, as UMAP is known to be most consistent across runs and tends to better preserve the global structure of the data and due to the time constrained nature of this project, we opted to not explore t-SNE. Below, we show the justification in using UMAP for our dimensionality reduction use-case and explore the various hyperparameter applications to our dataset.

The high dimensionality (768 dimensions) of the text embeddings necessitates dimensionality reduction, initially to enable visualisation and subsequently to aid clustering (referring to the `curse of dimensionality' \cite{verleysen2005curse}, see Section~\ref{sec:dimred} for more details). This section explores and justifies the applicability of two techniques for this purpose: Principal Component Analysis (PCA) \cite{pearson1901liii, hotelling1933analysis}, a linear method, and Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap}, a non-linear manifold learning approach. While another non-linear technique, t-distributed Stochastic Neighbour Embedding (t-SNE), was initially considered, we focused on the evaluation of PCA and UMAP. This decision was based on UMAP's reported advantages in preserving global data structure more effectively than t-SNE, alongside its generally greater computational efficiency and run-to-run consistency \cite{mcinnes2018umap}. Given the project's scope, these practical benefits drew us away from t-SNE. The following subsections present a comparison between PCA and UMAP based on visualisations, justify the subsequent selection of UMAP for our pipeline; and detail its hyperparameter application to the Operalog dataset.

\subsection{Visualisation}

An example visualisation comparing the 3-dimensional projection, obtained using both PCA and UMAP in both MPNet and Nomic embedding spaces, is shown in Figure~\ref{fig:dimred-compare}. Since the original data resides in a 768-dimension embedding space, direct visualisation is impossible. Projecting this data down to the three dimensions enables visualisation but potentially loses information, making purely visual comparisons between the method outputs subjective.

To supplement the visual assessment with a quantitative measure, we evaluate the spread within these 3-dimensional projections using normalised variance. Normalised variance, defined for a data matrix $X$ in Equation~\ref{eq:normvar}, serves as a metric to quantify the overall dispersion of the points in the low-dimensional space.
Observing Figure~\ref{fig:dimred-compare}, the UMAP projections visually appear to exhibit more defined structures, such as clearer separations and groupings of points. Comparatively, the corresponding PCA projections look more dispersed and do not show any `clusters' or groupings of points. This visual impression is supported quantitatively by the normalised variance metric, calculated for these specific runs. The values for the UMAP projections ($0.90$ for MPNet, $2.27$ for Nomic) are substantially lower and closer to 1 than those for PCA (approximately $1.0 \times 10^7$ for MPNet, $-2.4 \times 10^9$ for Nomic). In this context, the lower normalised variance for UMAP suggests a projection that arranges the data more compactly while potentially preserving meaningful neighbourhood relationships, consistent with UMAP's goal of modelling the data manifold.

\begin{align}
    \text{Normalised Variance}(X) = \frac{\text{Var}(X)}{\text{Mean}(X)}
    \label{eq:normvar}
\end{align}

However, as discussed in Section~\ref{sec:umap}, UMAP is a stochastic algorithm. That is, due to an element of algorithmic randomness, its output naturally varies between runs \cite{mcinnes2018umap}. To account for this variability, Figure~\ref{fig:normvarumap} visualises the distribution of normalised variance results obtained over 100 independent UMAP runs for both embedding spaces. While variation exists, these distributions reaffirm the concept that UMAP consistently produces projections with significantly lower normalised variance than those produced by PCA. The distributions exhibit some right-skewness, particularly for the MPNet embeddings but remain concentrated in a range indicative of more structured projections than those from PCA.

Based on the combination of visually apparent characteristics in the example projections Figure~\ref{fig:dimred-compare} and the consistently lower (although variable) normalised variance scores, indicative of the better preservation of manifold structure, in Figure~\ref{fig:normvarumap}, UMAP is selected as the dimensionality reduction technique for the remainder of this pipeline. It appears more likely, than PCA, to produce a low-dimensional representation that retains structural information beneficial for the next clustering stage. 



\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{distUMAP.png}
\caption{Histogram plot of 100 UMAP runs' normalised variance (Equation~\ref{eq:normvar}) is shown for both MPNet and Nomic embedding spaces of the Operalog \texttt{FaultDescription} entries. UMAP parameters: \texttt{min\_dist} $=0.1$, \texttt{n\_neighbors} $=15$, \texttt{metric} $= \text{cosine}$.}
\label{fig:normvarumap}
\end{figure}
\begin{figure}[htbp]
  \centering
  % Row 1: MPNet
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{pcaTestMPNet.png}
    \caption{MPNet --- PCA}
    \label{fig:pcaTest-mpnet}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{umapTestMPNet.png}
    \caption{MPNet --- UMAP}
    \label{fig:umapTest-mpnet}
  \end{subfigure}
  \vspace{1em} % space between rows
  % Row 2: Nomic
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{pcaTestNomic.png}
    \caption{Nomic --- PCA}
    \label{fig:pcaTest-nomic}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{umapTestNomic.png}
    \caption{Nomic --- UMAP}
    \label{fig:umapTest-nomic}
  \end{subfigure}
  \caption{Comparison of PCA versus UMAP dimensionality reduction on the MPNet (top row) and Nomic (bottom row) embedding spaces on Operalog \texttt{FaultDescription} entries Normalised variance (Equation~\ref{eq:normvar}) is shown. UMAP parameters: \texttt{min\_dist} $=0.1$, \texttt{n\_neighbors} $=15$, \texttt{metric} $= \text{cosine}$.}
  \label{fig:dimred-compare}
\end{figure}

% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Talk about the visualisation mpnet vs nomic
%         \item {Talk about all the hyperparameters of the library, and add all the images generated for nomic and mpnet to the appendix. highlight that this not the end, right now we have no quantitative measure of deciding what is correct and what is not. So look forward to the clustering and hyperparameter section.}
%     \end{itemize}
% }

\subsection{UMAP Hyperparameters}\label{sec:umaphyperparameters}

% As mentioned in Section~\ref{sec:umap}, UMAP has 4 main hyperparameters, described below. Evidently, from the previous section, it is unintuitive to decide which embedding space is `better' or even what values to set the model parameters. 
%
% Here we touch on how the parameters may affect the Operalog data.
%
% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item justify the tuning 
%         \item introduce the experimentation
%         \item connect main text to appendix
%         \item summarise findings / selection
%         % \item Talk about how each hyperparameter works, and what we did to tune this.
%         % \item Talk about how they affect.
%     \end{itemize}
% }


As discussed in Section~\ref{sec:umap}, % Adjust reference to where UMAP is introduced
tuning UMAP's hyperparameters is crucial for obtaining a meaningful low-dimensional representation of the high-dimensional Operalog text embeddings. The choice of parameters like \texttt{n\_neighbors} and \texttt{min\_dist}, along with the distance \texttt{metric}, significantly influences the resulting structure.

Based on the nature of the Operalog data, adjusting \texttt{n\_neighbors} involves a trade-off. Lower values might isolate highly specific or rare fault types but could cause fragmentation of broader categories. Higher values might group related issues more globally but risk merging unique sub-types. Given the mix of common and rare faults, exploring a range of neighbour values is necessary. 

Similarly, \texttt{min\_dist} affects the visual density. Here, lower values may create tightly packed, visually distinct clusters suitable for identifying major categories. However, higher values might reveal finer intra-cluster variations relevant for understanding subtypes, but could make overall separation less clear. This parameter will affect clustering algorithms, such as DBSCAN as it essentially determines potential cluster density \cite{ester1996density}.
% [*Optional: Add a sentence about the potential impact on density-based clustering if applicable.*]
The choice of \texttt{metric} (i.e. `cosine' vs. `euclidean') is also critical, as `cosine' is often better suited for high-dimensional text embeddings like those used here \cite{baba2017plagiarism, cao2024recent}.

% \todo{Fix this... mention cosine but this discussion links to a non-existent result...}
To gain an initial understanding of suitable UMAP parameters for the Operalog data, a naive grid search was performed using both MPNet and Nomic embeddings across the parameter ranges displayed in Table~\ref{tab:umapgrid}. The resulting visualisations for each parameter combination are presented in Appendix~\ref{app:umaphypgrid}. However, visual inspection of these results highlighted a surprising finding: contrary to common practice for text embeddings, cosine distance did not appear to yield projections demonstrably better at preserving structure more faithfully than Euclidean distance in this preliminary exploration. These inconclusive and unexpected outcomes underscore the need for a more systematic hyperparameter optimisation phase, which is later detailed in Section~\ref{sec:hyperparameterOpt}.

\begin{table}[htbp]
    \fontsize{8}{11}\selectfont % Kept your font size setting
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase vertical spacing between rows slightly
    \rowcolors{2}{gray!10}{white} % Keep your row coloring
    \begin{tabular}{p{3cm}|p{8.5cm}} % Use left-aligned columns, no vertical rule
        \toprule % Top rule from booktabs
        \textbf{Hyperparameter} & \textbf{Values Explored} \\ % Column headers
        \midrule % Middle rule from booktabs
        \texttt{n\_neighbors} & $\big\{$ 5, 10, 13, 15, 20, 25 $\big\}$ \\ % Plain text list
        \texttt{min\_dist}    & $\big\{$ 0.0125, 0.05, 0.1, 0.15, 0.2 $\big\}$\\ % Plain text list
        \texttt{metric}       & $\big\{$ \texttt{'euclidean'}, \texttt{'cosine'} $\big\}$\\ % Keep texttt for strings
        \bottomrule % Bottom rule from booktabs
    \end{tabular}
    \renewcommand{\arraystretch}{1.0} % Reset arraystretch to default
    \caption{UMAP hyperparameter values explored in naive grid search} % Correct caption
    \label{tab:umapgrid} % Kept your label
\end{table}


% To empirically determine suitable parameters for the Operalog data using both MPNet and Nomic embeddings, a grid search was performed across a range of values for \texttt{n\_neighbors} and \texttt{min\_dist}, using the
% [State the metric you used, e.g., 'cosine'] distance metric and projecting to [State dimensions, e.g., 3] dimensions. The resulting visualisations for each combination are presented in [Reference Appendix and Figures, e.g., Appendix~B, Figures~B.1 through B.12].

% Visual inspection of these results
% [Reference Appendix again, e.g., in Appendix~B]
% revealed that
% [Summarise key observations from your 12 plots. E.g., "lower values of n\_neighbors (<10) tended to produce fragmented structures, while values above 50 overly smoothed distinct groups. A min\_dist below 0.1 created overly dense clusters where internal structure was lost, whereas values above 0.5 made separation difficult. The Nomic embeddings generally showed clearer structure across parameters compared to MPNet." - **REPLACE THIS WITH YOUR ACTUAL OBSERVATIONS**].

% Based on this analysis, the UMAP parameter configuration selected for subsequent clustering analysis (Section~\ref{sec:clustering_methods}) % Adjust reference if needed
% was \texttt{n\_neighbors=[Your chosen value]}, \texttt{min\_dist=[Your chosen value]}, and \texttt{metric='[Your chosen metric]'}. This configuration was chosen because it appeared to offer [State your justification based on observations. E.g., "the most informative balance between preserving local cluster separation (identifying distinct fault types) and representing the broader relationships between fault categories for both embedding models, based on the visual evidence in the appendix." - **REPLACE THIS WITH YOUR SPECIFIC JUSTIFICATION**].


\subsection{Section Summary}

Based on the comparison in the previous subsections, UMAP was selected over PCA as the preferred dimensionality reduction technique for the MPNet and Nomic embedding spaces. The preliminary visualisations also highlighted that UMAP's output is sensitive to its hyperparameters, indicating that a systematic approach is required to find optimal settings for revealing structure within the Operalog data. Directly evaluating the 'quality' of different UMAP projections is challenging therefore, to assess the effectiveness of various projections (resulting from different hyperparameter settings), we adopt an indirect evaluation strategy by applying clustering algorithms to the low-dimensional UMAP outputs. The quality of the resulting clusters, measured using specific clustering evaluation metrics (introduced in the subsequent Section~\ref{sec:unsupcat}), will serve as a proxy for how well the UMAP projection preserved the relevant structure for categorisation. The next section introduces the clustering algorithms and metrics chosen for this purpose

% \todo[inline, caption={}]{
%     \begin{itemize}
%         \item motivate this with the curse of dimensionality and state the embedding spaces of the models - DONE
%         \item Here, we put some effort into visualisation of the embedding dimension with MPNET and with Nomic. Show images - DONE
%         \item Note that only unique sentences are considered for embedding (we have a one to one mapping backwards anyways). - DONE
%         \item In literature, there exists PCA and t-SNE and UMAP. Explain why we did not choose t-SNE and stuck with comparing UMAP and PCA. - DONE
%         \item Reason why UMAP is fine to use here, the topological manifold assumption (hard). - DONE
%         \item Talk about selecting UMAP and why over PCA. - DONE
%         \item Talk about UMAP parameters and how changing them affects things. - DONE
%         \item Talk about UMAP librarys and just lightly motivate this as an optimal  implementation. - SORT OF
%     \end{itemize}
% }

\section{Unsupervised Categorisation Through Clustering}\label{sec:unsupcat}

Following dimensionality reduction via UMAP, the resulting low-dimensional embedding space is prepared for the core task of clustering. As outlined in Section~\ref{sec:clustering}, clustering is an unsupervised machine learning approach used to group similar data points. Within this project's context, the goal is to group the embeddings of Operalog \texttt{FaultDescription} entries such that each resulting cluster corresponds to a distinct, underlying fault category. Ideally, the clustering process should automatically discover these latent categories from the data structure.

To explore different clustering paradigms, three algorithms were selected for evaluation based on their characteristics described in Section~\ref{sec:clustering}:

\begin{itemize}
    \item \textbf{k-Medoids \cite{kmedoids}} - A partitioning-based algorithm that groups data around actual data points (medoids).
    \item \textbf{DBSCAN \cite{ester1996density}} - A density-based algorithm that identifies clusters based on regions of high point density.
    \item \textbf{HDBSCAN \cite{campello2013density}} - An algorithm combining density-based principles with hierarchical clustering to identify stable clusters across varying densities.
\end{itemize}

Notably, k-Means \cite{macqueen1967some}, another common partitioning algorithm, was considered but excluded due to its known sensitivity to initialisation and potential instability compared to k-Medoids \cite{zhang2008improved}.

A fundamental challenge in unsupervised clustering is evaluating the quality of the resulting clusters without ground truth labels. To quantitatively assess and compare the different clustering outcomes, three established internal evaluation metrics (heuristics) are employed: the Silhouette coefficient \cite{rousseeuw1987silhouettes}, the Davies-Bouldin index \cite{davies1979cluster}, and the Calinski-Harabasz index \cite{calinski1974dendrite}.

This section details the rationale for selecting the specific clustering algorithms and introduces the evaluation heuristics used in the subsequent hyperparameter optimisation phase.

\subsection{Clustering Candidates}

The suitability of each clustering algorithm depends on the structure of the data and the nature of the desired clusters. Below, we discuss the characteristics of each chosen algorithm in the context of the Operalog embedding data.

\paragraph{k-Medoids} As described in Section~\ref{sec:kmedoids}, the k-Medoids algorithm (implemented via the PAM algorithm in common libraries like scikit-learn \cite{scikit-learn}) requires only one primary hyperparameter: $k$, the desired number of clusters. While determining the optimal $k$ for the unknown Operalog fault categories is non-trivial, having a single parameter simplifies the optimisation process. Furthermore, $k$ has an intuitive interpretation:  higher $k$ might isolate rate but specific ion source faults, while lower $k$ might group broader issues such as `ion source failure'. 
However, k-Medoids assumes clusters are roughly spherical and aims to minimise distances to central medoids \cite{kmedoids}. Visual inspection of the UMAP projections (Figure~\ref{fig:dimred-compare} and Appendix~\ref{app:umaphypgrid}) suggests the Operalog embeddings form complex, non-spherical structures. Therefore, k-Medoids might struggle to accurately capture the boundaries of these potential fault categories. 

\paragraph{DBSCAN} This algorithm (Section~\ref{sec:dbscan}) uses two key hyperparameters, \texttt{Eps} and \texttt{MinPts}, to define local density. Its main advantages are the ability to find arbitrarily shaped clusters and explicitly identify noise points, which could be valuable for isolating unusual or poorly described Operalog entries \cite{ester1996density}. However, DBSCAN's performance relies heavily on selecting appropriate \texttt{Eps} and \texttt{MinPts} values. Its use of a single, global density threshold makes it potentially sensitive to datasets containing clusters of varying densities \cite{campello2013density}. Assessing whether the Operalog embeddings exhibit such density variations is difficult visually. The IsoScore metric \cite{rudman2021isoscore}, which measures the uniformity of space utilisation by the projected points, provides some insight. Figure~\ref{fig:isoscore} shows average IsoScores of 0.653 (MPNet) and 0.710 (Nomic) for the UMAP projections. While these values indicate reasonable space utilisation, they are not close to 1, suggesting potential density variations that might pose a challenge for DBSCAN's global threshold approach. We expect some very short \texttt{FaultDescription} entries which are common, leading to dense clusters and longer entries, which are more rare, leading to sparse clusters. This insight would support the concern about the varying densities.

\paragraph{HDBSCAN} As an extension of DBSCAN (Section~\ref{sec:hdbscan}), HDBSCAN aims to overcome the sensitivity to Eps and the varying density problem. It achieves this by building a cluster hierarchy and using a stability measure to extract clusters that persist across a range of density levels \cite{campello2013density}. It requires only one primary hyperparameter, $m_{\text{pts}}$ (equivalent to \texttt{MinPts} in DBSCAN), making it simpler to tune than DBSCAN. Its ability to handle varying densities and its robustness (consistent results between runs) make it a strong candidate for the Operalog data, where the true number and density of fault categories are unknown. As mentioned before, since we expect a clusters of varied densities, being able to find stable clusters in this space is desirable for discovering potentially diverse fault types in the Operalog.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/isoscore.png}
    \caption{IsoScore comparison of MPNet versus Nomic embeddings over 100 runs of UMAP. UMAP parameters: \texttt{min\_dist} $=0.1$, \texttt{n\_neighbors} $=15$, \texttt{metric} $= \text{cosine}$.}\label{fig:isoscore}
\end{figure}

Based on these characteristics, we hypothesise that HDBSCAN might be best suited for this task, followed by DBSCAN, with k-Medoids likely performing worst due to the non-spherical nature of the projected data. However, empirical evaluation across all three algorithms is necessary to validate these expectations and determine the most effective approach for automatically categorising the Operalog fault descriptions. The subsequent hyperparameter optimisation (Section~\ref{sec:hyperparameterOpt}) and evaluation will compare their performance quantitatively using the clustering heuristics.

\subsection{Clustering heuristics} \label{sec:clusterheuristics}

Motivated by the necessity to quantitatively evaluate the quality of different clustering results in this unsupervised setting, three well-understood, internal clustering evaluation metrics (heuristics) are utilised. These metrics assess the `goodness-of-fit' based on the properties of the clusters themselves:
\begin{itemize}
    % \item Inertia - Also known as the within-cluster sum-of-squares, this metric measures cluster compactness by summing the squared distances of each sample to its closest cluster centre (medoid in the case of k-Medoids) \cite{macqueen1967some}. Lower inertia values generally indicate denser, more compact clusters. However, inertia tends to decrease as the number of clusters increases, making it less useful for comparing clusterings with different numbers of clusters (k).

    \item \textbf{Silhouette coefficient} - This metric evaluates how well-separated clusters are by comparing each point's average distance to points in its own cluster (cohesion) with its average distance to points in the nearest neighbouring cluster (separation) \cite{rousseeuw1987silhouettes}. Scores range from -1 to 1, where values closer to 1 indicate well-separated, dense clusters, values near 0 suggest overlapping clusters, and negative values indicate potential misclassifications.

    \item \textbf{Davies-Bouldin index} - This index quantifies the average `similarity' between each cluster and its most similar neighbour, where similarity is defined as the ratio of within-cluster scatter to between-cluster separation \cite{davies1979cluster}. Lower Davies-Bouldin scores indicate better clustering, implying clusters are compact and well-separated from their nearest neighbours.

    \item \textbf{Calinski-Harabasz index} - Measures the ratio of the sum of between-cluster dispersion to the sum of within-cluster dispersion for all clusters \cite{calinski1974dendrite}. Higher Calinski-Harabasz scores generally indicate better defined clusters. That is, clusters are dense and well-separated.
\end{itemize}

Furthermore, each of these metrics captures a different aspect of clustering quality (compactness, separation and variance ratios). As it is unclear which single metric best reflects the desired clustering structure for a specific dataset and task, particularly in an exploratory context like identifying Operalog fault categories, all three metrics were used collectively during the hyperparameter optimisation phase (Section~\ref{sec:hyperparameterOpt}) to provide a more comprehensive assessment of the different clustering solutions.

% \todo[inline, caption={}]{
%     \begin{itemize}
%         \item mention k-medoids sklearn implementation uses PAM algorithm. Touch on the fact it only has one parameter, which is significant for our usecase
%         \item motivate the clustering - DONE ISH
%         \item talk about clustering algorithms we did not consider, with research - DONE ISh
%         \item explain the 3 clustering methods we have chosen to use and the hyperparameters in a table
%         \item show test clustering 
%         \item this section is not really quantitative, highlight how we have to introduce clustering heuristics 
%     \end{itemize}
% }

\section{Hyperparameter optimisation}\label{sec:hyperparameterOpt}

As established in previous sections, achieving effective unsupervised categorisation requires careful selection of parameters for both the dimensionality reduction (UMAP, Section~\ref{sec:umap}) and clustering stages (Section~\ref{sec:clustering}). Given the interplay between these stages and the sensitivity of algorithms like UMAP and DBSCAN to their settings, a systematic approach to find optimal parameter combinations is necessary. Furthermore, as discussed in Section~\ref{sec:clusterheuristics} different internal clustering evaluation metrics capture distinct aspects of cluster quality, and it is unclear which single metric best reflects the desired outcome for a specific dataset.   

To address these challenges, this project employed the Optuna framework for automatic hyperparameter optimisation \cite{akiba2019optuna}. Optuna was selected over alternatives like Hyperband \cite{li2018hyperband} primarily due to its robust support for multi-objective optimisation \cite{akiba2019optuna}. This capability is crucial here, as the goal was to simultaneously optimise the clustering results based on multiple evaluation heuristics: the Silhouette coefficient, Davies-Bouldin index, and Calinski-Harabasz index. The optimisation aims to maximise the Silhouette and Calinski-Harabasz scores while minimising the Davies-Bouldin score \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}. A practical advantage of Optuna noted during this work was its ability to save and resume optimisation states, improving research efficiency \cite{akiba2019optuna}.   

An Optuna optimisation session, termed a `study', iteratively evaluates different hyperparameter configurations (`trials') \cite{akiba2019optuna}. For each trial, Optuna suggests parameter values based on a chosen sampler algorithm. This study utilised the Tree-structured Parzen Estimator sampler, which is Optuna's default algorithm known for efficiently exploring complex search spaces \cite{watanabe2023tree}. The core components of the Optuna study setup were:   

\begin{itemize}
    \item \textbf{Objective function} - A function designed to take a set of suggested hyperparameters, execute the relevant parts of the pipeline (UMAP transformation and clustering), calculate the three chosen clustering heuristic scores, and return these scores to Optuna. This function defines the search space by specifying the range or set of possible values for each hyperparameter using Optuna's library semantics. The specific hyperparameters optimised included shared UMAP settings and algorithm-specific parameters, with ranges informed by preliminary experimentation as shown in Table \ref{tab:hypreal}. To handle potential errors arising from UMAP's stochastic nature or problematic parameter combinations leading to clustering failures, the objective function incorporated a retry mechanism (up to 10 attempts). If errors persisted, the trial was assigned extremely poor heuristic scores to guide the optimiser away from that configuration region.
    \item \textbf{Number of trials} - An upper limit defining the maximum number of configurations to evaluate within the study.
\end{itemize}

\begin{table}[htbp]
    \fontsize{8}{11}\selectfont % Font size setting
    \centering
    \renewcommand{\arraystretch}{1.2} % Row spacing
    \rowcolors{2}{gray!10}{white} % Row coloring
    % Use tabularx to fit the text width. 'X' column automatically adjusts and wraps text.
    \begin{tabularx}{\textwidth}{l l X}
        \toprule % Top rule from booktabs
        \textbf{Scope} & \textbf{Hyperparameter} & \textbf{Range / Set} \\ % Column headers - slightly adjusted last header
        \midrule % Middle rule from booktabs
        General       & \texttt{n\_components}       & Integer range $[2, 6]$ \\
        General       & \texttt{n\_neighbors}        & Integer range $[3, 15]$ \\
        General       & \texttt{min\_dist}           & Float range $(0, 0.2)$ \\
        General       & \texttt{metric}              & Set $\{$ \texttt{`euclidean'}, \texttt{`cosine'} $\}$ \\
        General       & \texttt{clustering\_method}  & Set $\{$ \texttt{`k-Medoids'}, \texttt{`DBSCAN'}, \texttt{`HDBSCAN'} $\}$ \\
        k-Medoids     & \texttt{n\_clusters}         & Integer range $[3, 30]$ \\
        DBSCAN        & \texttt{eps}                 & Float range $[0.1, 1.0]$ \\
        DBSCAN        & \texttt{min\_cluster\_size}  & Integer range $[3, 12]$ \\
        HDBSCAN       & \texttt{min\_pts}            & Integer range $[3, 30]$ \\
        \bottomrule % Bottom rule from booktabs
    \end{tabularx}
    \renewcommand{\arraystretch}{1.0} % Reset arraystretch to default
    \caption{Optuna hyperparameter search space. k-Medoids hyperparameter $k$ was renamed to \texttt{n\_clusters}; DBSCAN hyperparameter \texttt{MinPts} was renamed to \texttt{min\_cluster\_size}; and HDBSCAN hyperparameter $m_{\text{pts}}$ was renamed to \texttt{min\_pts}.} % Caption
    \label{tab:hypreal}
\end{table}

Upon completion of the specified number of trials, the study yields a set of optimal configurations, those for which no single objective (i.e. heuristic) score can be improved without worsening at least one other objective. Since multiple objectives often lead to several such `best' configurations, a post-optimisation filtering step was applied using rank aggregation \cite{li2019comparative}. For each heuristic, the optimal configurations were ranked, and each configuration received points equal to its inverse rank. For example, for 10 configurations, rank 1 gets 10 points, rank 2 gets 9, ..., rank 10 gets 1 point. These points were summed across all three heuristics for each configuration. The configuration(s) with the highest total score were selected as the final `best' performing set of hyperparameters, representing a balanced choice across the different measures of clustering quality. The quantitative results of this optimisation are presented and discussed in Chapter~\ref{chap:results}.

\section{Label generation}\label{sec:labelgen}

After achieving the `most optimal’ clusters through the hyperparameter optimisation process described in Section~\ref{sec:unsupcat}, the final step involves automatically inferring meaningful names or labels for these discovered groups. This automatic label generation was performed leveraging the natural language processing capabilities of the SpaCy library \cite{spacy2}. The main idea is to capture the essence of the text in each cluster into a precise but representative phrase.

The process begins by examining all the \texttt{FaultDescription} sentences assigned to a particular cluster. For each sentence, SpaCy is employed to perform detailed linguistic analysis, including part-of-speech tagging and dependency parsing \cite{tabassum2020survey}. This helps identify key components of the grammar and determine their relationships within the sentence.

To focus on the most informative words, several filtering (pre-processing) steps are applied during this analysis. Common English stop-words, along with project-specific stop-words (like `ion source' or `breakdown', as discussed in Section~\ref{sec:preproc}), are disregarded \cite{tabassum2020survey}. Furthermore, tokens that are non-alphanumeric or consist of fewer than two characters are excluded to reduce noise from punctuation or trivial entries.

From the remaining tokens, the system specifically identifies and collects verbs (particularly the root verb of sentences), direct objects (identified via dependency relations), nouns, and adjectives. To ensure consistency and group related word forms, lemmatisation is applied, reducing words to their base or dictionary form. For example, `changed' and `changing' become 'change' \cite{tabassum2020survey}.

Building the label then involves constructing a naive phrase by combining the most frequently occurring elements identified across all sentences in the cluster. Specifically, the system determines:
\begin{itemize}
    \item The single most common verb (identified as the root of sentences).
    \item The single most common direct object (based on dependency parsing).
    \item The top two most common nouns (excluding any words already selected as the verb or direct object to ensure variety).
\end{itemize}

These selected components are then combined into a single, underscore-separated string to form the cluster's generated label. This approach incorporates a mechanism to handle cases where fewer than the desired number of unique terms are available within a cluster's vocabulary, ensuring a label is still generated. The selection process considers a configurable maximum number of top nouns to examine, ensuring efficiency while capturing the primary themes.

Finally, this label generation process is integrated into the data handling workflow. It operates on the clustered data, mapping the generated underscore-separated label back to each original log entry belonging to that cluster.

The intuition behind this heuristic approach is to capture the core semantic meaning of the fault descriptions within a cluster by focusing on the primary action (verb), the entity acted upon (direct object), and the key subjects or items involved (nouns), thereby creating a succinct, machine-generated descriptor for each automatically identified fault category.

\section{CLI Application}\label{sec:CLI}

To facilitate the practical application and reproducibility of the methodology developed in this project, a Command Line Interface (CLI) tool was created, accompanying the code submission. This application encapsulates the end-to-end pipeline, providing users with a means to apply the automatic labelling process to Operalog data.

The primary functions offered by the CLI include managing the hyperparameter optimisation process, allowing users to initiate new optimisation runs or resume existing ones, and executing the label inference step on clustered data. Upon completion, the tool generates two key output artefacts: an updated version of the Operalog dataset, augmented with columns indicating the assigned numerical cluster identifier (\text{label}) and the inferred natural language label (\texttt{real\_label}) for each entry, and a structured JSON file. This JSON file contains lists of the original sentences grouped by their assigned cluster, alongside a mapping from each cluster ID to its generated natural language label.

To offer flexibility, the CLI exposes several parameters for user configuration. The primary parameters are listed below:
\begin{itemize}
    \item \textbf{Model} - The HuggingFace model path.
    \item \textbf{Keep punctuation} - Boolean toggle which keeps punctuation from being stripped during the text pre-processing stage.
    \item \textbf{Ignore case} - Boolean toggle which ignores casing during the text pre-processing stage.
    \item \textbf{Clustering methods} - Define one or more clustering methods to use for the hyperparameter optimisation stage. 
    \item \textbf{Heuristic methods} - Define one or more heuristic methods to use for the hyperparameter optimisation stage.
\end{itemize}

The design of the CLI application and its underlying codebase explicitly considers future development. Its structure allows users to experiment with embedding models beyond those researched in this project by simply providing a different model path. Furthermore, the modular organisation of the codebase aims to simplify the future integration of additional clustering algorithms or evaluation heuristics, enhancing the tool's potential utility for ongoing research or operational use.

% \chapter{Results and Discussion}\label{chap:results}
%
% As mentioned in Section~\ref{sec:hyperparameterOpt}, we perform hyperparameter optimisation on parameters, as shown in Table~\ref{tab:hypreal}. Three heuristics (Silhouette coefficient, Davies-Bouldin index and Calinski-Harabasz index) for the three clustering methods (k-Medoids, DBSCAN and HDBSCAN). To initially get an idea of the clustering, we fix the clustering methods to less than three and analyse the clusters. Then, with the insights, we introduce another metric, the `unclustered\_percentage' \todo{maybe I wont say this sentence}. Finally, the three clustering methods are ran against each other over \todo{set thsi once done} $400$ trials toggling the two boolean flags (keep punctuation and ignore case), resulting in four runs, and the result with the most informative clustering labels are chosen. This is chosen subjectively. \todo{point out this is is not good and we want to move away from subjectiveness in the future.} \todo{piont out the min,max of each clustering method we are using.}
%
% Note that this $400$ was chosen due to the short-term instability in results. This is mainly introduced by the stochastic process of the optimisation pipeline (i.e. the dimensionality reduction with UMAP and clustering methods).
%
% Call the phases: (1) Head-To-Head (H2H) phase; (2) Free-For-All (F4A) phase (and make abbreviatiosn for this todo)
%
% \section{Experiments and Results}
%
% \subsection{Head-to-Head}
%
% Each head to head study was run for $400$ trials on both the MPNet and Nomic embedding spaces. This section details the results.
%
% The chosen ran were:
% \todo{Make this as a table and label each one (A), (B), (C) so we can refer to it as run A, run B etc.}
% \begin{itemize}
%     \item k-Medoids, DBSCAN 
%     \item DBSCAN, HDBSCAN
%     \item only HDBSCAN optimisation, as this was hypothesised as the best, we explored whether having the other clustering methods was drawing away unnecessary attention during optimisation.
% \end{itemize}
%
% These checks ignorecase and ignore punctuation.
%
% \todo{Explain why I am looking at cluster sizes}
%
% Cluster sizes are a good indication of the distribution of labels and how general a label is. We generally want, run-to-run, for a model to produces clusters of a consistent size.
%
% Figures \ref{fig:mpnet_h2h}~and~\ref{fig:nomic_h2h}, show the rank aggregation result, both MPNet and Nomic, for the top 3 configurations and their heuristic scores for k-Medoids versus DBSCAN (run (A)), and DBSCAN versus HDBSCAN (run (B)) as a 3-dimensional plot. Additionally, Figures \ref{fig:mpnet_h2h_whisker} and \ref{fig:nomic_h2h_whisker} highlight the distribution of cluster sizes, if each configuration was used for clustering.
%
% Notably, for MPNet, DBSCAN showed the widest whisker plots, indicating a higher variance and thus, more unstable results. Further, one source of instability comes from stochastic nature of UMAP (see Section~\ref{sec:umap}), and thus for some configuration recognised as `good', multiple runs may be required to re-produce the `good' results. Taking this into account, the results shown in the Figures \ref{fig:mpnet_h2h_whisker} and \ref{fig:nomic_h2h_whisker} is the entire distribution of cluster sizes over 10 runs, for the configuration over. 
%


% \pagebreak
% \todo{fix this}
% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvsdbscan_whisker.png}
%         \caption{k-Medoids versus DBSCAN}
%         \label{fig:mpnet_kmedoidsvsdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvshdbscan_whisker.png}
%         \caption{k-Medoids versus HDBSCAN}
%         \label{fig:mpnet_kmedoidsvshdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/mpnet_dbscanvshdbscan_whisker.png}
%         \caption{DBSCAN versus HDBSCAN}
%         \label{fig:mpnet_dbscanvshdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/mpnet_hdbscan_whisker.png}
%         \caption{HDBSCAN}
%         \label{fig:mpnet_hdbscan}
%     \end{subfigure}
%     \caption{MPNet: Whisker plots of the distribution of the sizes of clusters for each configuration returned by Optuna. Empty means there were no optimal configurations using that clustering method.}
%     \label{fig:mpnet_h2h_whisker}
% \end{figure}

% \pagebreak
% \todo{fix this}
% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvsdbscan_whisker.png}
%         \caption{k-Medoids versus DBSCAN}
%         \label{fig:nomic_kmedoidsvsdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvshdbscan_whisker.png}
%         \caption{k-Medoids versus HDBSCAN}
%         \label{fig:nomic_kmedoidsvshdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/nomic_dbscanvshdbscan_whisker.png}
%         \caption{DBSCAN versus HDBSCAN}
%         \label{fig:nomic_dbscanvshdbscan}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/nomic_hdbscan_whisker.png}
%         \caption{HDBSCAN}
%         \label{fig:nomic_hdbscan}
%     \end{subfigure}
%     \caption{Nomic: Whisker plots of the distribution of the sizes of clusters for each configuration returned by Optuna. Empty means there were no optimal configurations using that clustering method.}
%     \label{fig:nomic_h2h_whisker}
% \end{figure}
%
% \subsection{Free-For-All}
%
% In this section, all clustering methods were compared against each other for $400$ trials, for all heuristics but over the boolean toggles from before, with each embedding space (MPNet and Nomic).  The results can be seen in (TODO). This will ideally plot the number of the clustesr. Think abuot how to lay this out.
%
% \section{Discussion}
%
% From the results, it is evident that DBSCAN is the most volatile and produces the most varied number of clusters. However, interestingly, HDBSCAN is the next most varied, with k-medoids being the most stable. This would make sense, though, as the number of clusters is set as a hyperparameter during optimisation, so there is only a fixed range of cluster sizes it can be. Disregarding this, HDBSCAN appears to be the most stable, given it has no prior information on the number of clusters. Additionally, Figure~\ref{fig:mpnet_dbscanvshdbscan}, DBSCAN got dominated by HDBSCAN to the point that there were no contending configurations found over $400$ trials. 
%
%
% \todo[inline, caption={}] {
%     \begin{itemize}
%         \item Rank aggregation - DONE
%         \item plot whiskers (or maybe in results?). acc this should be in results as it makes sense that is quantitative evaluation of results. - DONE ish
%         \item results should also talk about qualitatively and motivate the conclusion, this is a step in the right direction but not 100\%. 
%     \end{itemize}
% }
%
% \todo[inline,caption={}]{
% Describe the results and analyse the results
% \begin{itemize}
%     \item Analyse the sentence embedding results.
%     \item Anaylse the UMAP hyperparameter optimisation qualitatively, mention that we use Optuna.
%     \item Next steps: Fine tuning the PLM (Nomic or MPNet). Using MOE nomic (v2). Rent higher computation, then you can use top performing PLMS. Using t-SNE to see differences.
% \end{itemize}
% }

\chapter{Results and Discussion}\label{chap:results}

This chapter presents the results obtained from the hyperparameter optimisation process detailed in Section~\ref{sec:hyperparameterOpt}. Recall that the optimisation sought to find the best parameters for the UMAP dimensionality reduction (Section~\ref{sec:umap}) and subsequent clustering stage (Section~\ref{sec:clustering}) by evaluating three clustering algorithms (k-Medoids, DBSCAN, HDBSCAN) across both MPNet and Nomic embedding spaces (Figure~\ref{fig:pipeline}). The optimisation used three internal clustering heuristics (Silhouette coefficient, Davies-Bouldin index, Calinski-Harabasz index) as objectives \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}, maximising the Silhouette and Calinski-Harabasz scores while minimising the Davies-Bouldin score. 

The optimisation was conducted in two main phases:
\begin{enumerate}
    \item \textbf{Head-to-Head (H2H) Phase} - Pairwise comparisons of clustering algorithms, plus an HDBSCAN-only run, were performed to gain initial insights into their relative performance and stability under specific conditions (detailed in Section~\ref{sec:h2h_results}). These runs used default text pre-processing (ignoring case and ignoring punctuation).
    \item \textbf{Free-For-All (F4A) Phase} - All three clustering algorithms were compared simultaneously, exploring the impact of different text pre-processing flags (toggling \texttt{ignore\_case} and \texttt{keep\_punctuation}) to identify robust, high-performing configurations across various settings (detailed in Section~\ref{sec:f4a_results}).
\end{enumerate}

For each phase, Optuna \cite{akiba2019optuna} studies were run for 400 trials per configuration. This relatively high number of trials was chosen to account for the inherent stochasticity in both the UMAP algorithm (Section~\ref{sec:umap}) and the Optuna sampler \cite{watanabe2023tree}, ensuring a more robust exploration of the hyperparameter space defined in Table~\ref{tab:hypreal}. The best configurations from each study were identified using the rank aggregation method described in Section~\ref{sec:hyperparameterOpt}.

The subjective choice of the final label is based on a qualitative assessment of the labels generated from the quantitatively best-performing cluster configuration identified via rank aggregation. Improving the quantitative evaluation of generated label quality remains an area for future work. 

\section{Experiments and Results}\label{sec:exp_results}

% Suggestion: Move detailed figures referenced below to an Appendix B.

\subsection{Head-to-Head (H2H) Phase}\label{sec:h2h_results}

In the H2H phase, four distinct optimisation studies were conducted for both MPNet and Nomic embeddings, each running for 400 trials, as summarised in Table~\ref{tab:h2h_summary}. These runs used the default pre-processing settings (\texttt{ignore\_case} = \texttt{True}, \texttt{keep\_punctuation} = \texttt{False}).

% \begin{itemize} % \todo{Made the list into a table-like structure}
%     \item \textbf{Run A} - Comparing k-Medoids vs. DBSCAN.
%     \item \textbf{Run B} - Comparing k-Medoids vs. DBSCAN.
%     \item \textbf{Run C} - Comparing DBSCAN vs. HDBSCAN.
%     \item \textbf{Run D} - Optimising HDBSCAN only (to assess if competition from other methods hindered its optimisation).
% \end{itemize}

\begin{table}[H] 
    \centering
    \rowcolors{2}{gray!10}{white}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l l l l}
        \toprule
        \textbf{Run ID} & \textbf{Description} & \textbf{Optimal Config. Plots} & \textbf{Cluster Size Plots} \\
        \midrule
        Run A & k-Medoids vs. DBSCAN & Fig.~\ref{fig:mpnet_kmedoids_vs_dbscan_h2h_config} (MPNet) & Fig.~\ref{fig:mpnet_h2h_runa} (MPNet) \\
              &                      & Fig.~\ref{fig:nomic_kmedoids_vs_dbscan_h2h_config} (Nomic) & Fig.~\ref{fig:nomic_h2h_runa} (Nomic) \\
        Run B & k-Medoids vs. HDBSCAN & Fig.~\ref{fig:mpnet_kmedoids_vs_hdbscan_h2h_config} (MPNet) & Fig.~\ref{fig:mpnet_h2h_runb} (MPNet) \\
              &                       & Fig.~\ref{fig:nomic_kmedoids_vs_hdbscan_h2h_config} (Nomic) & Fig.~\ref{fig:nomic_h2h_runb} (Nomic) \\
        Run C & DBSCAN vs. HDBSCAN    & Fig.~\ref{fig:mpnet_dbscan_vs_hdbscan_h2h_config} (MPNet) & Fig.~\ref{fig:mpnet_h2h_runc} (MPNet) \\
              &                       & Fig.~\ref{fig:nomic_dbscan_vs_hdbscan_h2h_config} (Nomic) & Fig.~\ref{fig:nomic_h2h_runc} (Nomic) \\
        Run D & HDBSCAN Only          & Fig.~\ref{fig:mpnet_hdbscan_only_h2h_config} (MPNet) & Fig.~\ref{fig:mpnet_h2h_rund} (MPNet) \\
              &                       & Fig.~\ref{fig:nomic_hdbscan_only_h2h_config} (Nomic) & Fig.~\ref{fig:nomic_h2h_rund} (Nomic) \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.5em} % Add a small space after the table
    \caption{Summary of H2H optimisation runs (default pre-processing).
    All H2H runs used default pre-processing settings (\texttt{ignore\_case=True}, \texttt{keep\_punctuation=False}). Appendix references point to Appendix~\ref{app:exp_results_plots}.}
    \label{tab:h2h_summary}
\end{table}


\noindent Key performance indicators included the heuristic scores achieved by the top configurations (selected via rank aggregation) and the stability of the clustering results, assessed by examining the distribution of cluster sizes produced by the highlighted top configurations over multiple independent runs. In this case, 10 runs were used here to capture variability stemming from UMAP's stochasticity. Cluster size distribution provides insight into whether an algorithm consistently finds a similar number of clusters of similar sizes, which is desirable for reproducible categorisation. 

% Detailed visualisations comparing the heuristic scores for the top-ranked H2H configurations for MPNet and Nomic embeddings are presented in Appendix~\ref{sec:h2h_config_appendix}. These plots show the trade-offs between the different heuristics for the best solutions found.
% \todo{Think about whether to move some examples up here}

Detailed visualisations comparing the heuristic scores for the top-ranked H2H configurations for MPNet and Nomic embeddings are presented in Appendix~\ref{sec:h2h_config_appendix} (Figures~\ref{fig:mpnet_h2h_config_appendix} and \ref{fig:nomic_h2h_config_appendix}). These plots show the trade-offs between the different heuristics for the best solutions found.
The stability analysis, showing the distribution of cluster sizes over 10 runs for the best configurations identified in the H2H phase, is presented in Appendix~\ref{sec:h2h_whisker_appendix} (Figures~\ref{fig:mpnet_h2h_whisker_appendix} and \ref{fig:nomic_h2h_whisker_appendix}).

% Observing these distributions (particularly for MPNet, Figure~\ref{fig:mpnet_h2h_whisker_appendix}), several trends emerged:
% \begin{itemize}
%     \item DBSCAN often exhibited the widest variance in cluster sizes and sometimes failed to find optimal configurations which is indicated by the wide range (1 to around 500) of in MPNet Run A (Figure~\ref{fig:mpnet_h2h_runa}), suggesting higher instability or sensitivity to hyperparameters within this specific data context compared to the other methods. For example, in Nomic Run A (Figure~\ref{fig:nomic_h2h_runa}), DBSCAN produced only 3 clusters as the highest value, whereas, in the Nomic Run C (Figure~\ref{nomic_h2h_runc}), it has runs with up to 40 clusters.
%     \item k-Medoids generally produced stable cluster counts, which is expected as the number of clusters ($k$) is a fixed hyperparameter determined during optimisation within a defined range (Table~\ref{tab:hypreal}). However, MPNet Run B (Figure~\ref{fig:mpnet_h2h_runb}) shows k-Medoids configurations getting dominated by HDBSCAN, suggesting that HDBSCAN clustering out-performs k-Medoids.
%     \item HDBSCAN, despite not having the number of clusters predefined, demonstrated reasonable stability in the number and size of clusters identified across runs, particularly when optimised alone (Run C, Figures~\ref{fig:mpnet_h2h_runc} and \ref{fig:nomic_h2h_runc}). In the DBSCAN vs. HDBSCAN comparison (Run B, Figures~\ref{fig:mpnet_h2h_runb} and \ref{fig:nomic_h2h_runb}), HDBSCAN configurations showed the highest range of results, indicating that DBSCAN introduces noise when optimising using Optuna. 
% \end{itemize}


Observing these distributions (e.g., MPNet results in Figure~\ref{fig:mpnet_h2h_whisker_appendix}), several trends emerged:
\begin{itemize}
    \item DBSCAN often exhibited wide variance in cluster sizes (i.e. a range from 1 to nearly 500 clusters found across runs for optimal configurations in MPNet Run A, Figure~\ref{fig:mpnet_h2h_runa}) and sometimes failed to identify competitive optimal configurations (indicated by empty plots in the appendix, i.e. Figure~\ref{fig:mpnet_h2h_runc} for MPNet Run C). This suggests higher instability or sensitivity to hyperparameters within this specific data context compared to the other methods. For instance, Nomic Run A (Figure~\ref{fig:nomic_h2h_runa}) configurations using DBSCAN yielded at most 3 clusters, whereas Nomic Run C (Figure~\ref{fig:nomic_h2h_runc}) showed DBSCAN configurations producing up to 40 clusters, highlighting its variable behaviour.
    \item k-Medoids generally produced stable cluster counts, as expected since the number of clusters ($k$) is a hyperparameter optimised within a defined range (Table~\ref{tab:hypreal}). However, in comparisons like Run B (i.e. MPNet, Figure~\ref{fig:mpnet_kmedoids_vs_hdbscan_h2h_config}), k-Medoids configurations were often dominated by HDBSCAN in the optimal set, suggesting HDBSCAN found better heuristic trade-offs.
    \item HDBSCAN demonstrated reasonable stability in the number and size of clusters identified across runs, particularly when optimised alone (Run D, Figures~\ref{fig:mpnet_h2h_rund} and \ref{fig:nomic_h2h_rund}). In the DBSCAN vs. HDBSCAN comparison (Run C, Figures~\ref{fig:mpnet_dbscan_vs_hdbscan_h2h_config} and \ref{fig:nomic_dbscan_vs_hdbscan_h2h_config}), HDBSCAN configurations clearly dominated the optimal set, reinforcing its superior performance in this dataset. 
\end{itemize}


These H2H results provide preliminary evidence supporting the initial hypothesis (Section~\ref{sec:unsupcat}), that density-based methods capable of handling varying densities (i.e. HDBSCAN) might be better suited to this dataset than the global density-based DBSCAN or the partition-based k-Medoids. Although k-Medoids offers predictable cluster counts, it got dominated by HDBSCAN, indicating that HDBSCAN may be the superior choice on this dataset. 

\subsection{Free-For-All (F4A) Phase}\label{sec:f4a_results}

% Building on the H2H insights, the F4A phase involved four distinct optimisation studies, each comparing \textbf{all three} clustering methods (k-Medoids, DBSCAN, HDBSCAN) simultaneously for 400 trials per study. These studies explored the impact of the text pre-processing flags (Section~\ref{sec:preproc}) on the final clustering outcome:

Building on the H2H insights, the F4A phase involved four distinct optimisation studies, each comparing all three clustering methods (k-Medoids, DBSCAN, HDBSCAN) simultaneously for 400 trials per study. These studies explored the impact of the text pre-processing flags (Section~\ref{sec:preproc}) on the final clustering outcome, as summarised in Table~\ref{tab:f4a_summary}.

% \todo{make this into a table}
% \begin{itemize}
%     \item \textbf{Run 1} - \texttt{ignore\_case=False}, \texttt{keep\_punctuation=False} (Figures~\ref{fig:mpnet_ff_f4a_config}, \ref{fig:nomic_ff_f4a_config}, \ref{fig:mpnet_ff_f4a_whisker}, \ref{fig:nomic_ff_f4a_whisker})
%     \item \textbf{Run 2} - \texttt{ignore\_case=False}, \texttt{keep\_punctuation=True} (Figures~\ref{fig:mpnet_ft_f4a_config}, \ref{fig:nomic_ft_f4a_config}, \ref{fig:mpnet_ft_f4a_whisker}, \ref{fig:nomic_ft_f4a_whisker})
%     \item \textbf{Run 3} - \texttt{ignore\_case=True},  \texttt{keep\_punctuation=False} (Default) (Figures~\ref{fig:mpnet_tf_f4a_config}, \ref{fig:nomic_tf_f4a_config}, \ref{fig:mpnet_tf_f4a_whisker}, \ref{fig:nomic_tf_f4a_whisker})
%     \item \textbf{Run 4} - \texttt{ignore\_case=True},  \texttt{keep\_punctuation=True} (Figures~\ref{fig:mpnet_tt_f4a_config}, \ref{fig:nomic_tt_f4a_config}, \ref{fig:mpnet_tt_f4a_whisker}, \ref{fig:nomic_tt_f4a_whisker})
% \end{itemize}

\begin{table}[H]
    \centering
    \rowcolors{2}{gray!10}{white}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l l l l l}
        \toprule
        \textbf{Run ID} & \textbf{\texttt{ignore\_case}} & \textbf{\texttt{keep\_punc}} & \textbf{Optimal Config. Plots} & \textbf{Cluster Size Plots} \\
        \midrule
        Run 1           & False   & False & Fig.~\ref{fig:mpnet_ff_f4a_config} (MPNet) & Fig.~\ref{fig:mpnet_ff_f4a_whisker} (MPNet) \\
                        &         &                             & Fig.~\ref{fig:nomic_ff_f4a_config} (Nomic) & Fig.~\ref{fig:nomic_ff_f4a_whisker} (Nomic) \\
        Run 2           & False   & True & Fig.~\ref{fig:mpnet_ft_f4a_config} (MPNet) & Fig.~\ref{fig:mpnet_ft_f4a_whisker} (MPNet) \\
                        &         &                             & Fig.~\ref{fig:nomic_ft_f4a_config} (Nomic) & Fig.~\ref{fig:nomic_ft_f4a_whisker} (Nomic) \\
        Run 3 (Default) & True    & False & Fig.~\ref{fig:mpnet_tf_f4a_config} (MPNet) & Fig.~\ref{fig:mpnet_tf_f4a_whisker} (MPNet) \\
                        &         &                              & Fig.~\ref{fig:nomic_tf_f4a_config} (Nomic) & Fig.~\ref{fig:nomic_tf_f4a_whisker} (Nomic) \\
        Run 4           & True    & True & Fig.~\ref{fig:mpnet_tt_f4a_config} (MPNet) & Fig.~\ref{fig:mpnet_tt_f4a_whisker} (MPNet) \\
                        &         &                                                    & Fig.~\ref{fig:nomic_tt_f4a_config} (Nomic) & Fig.~\ref{fig:nomic_tt_f4a_whisker} (Nomic) \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    % \vspace{0.5em} % Add a small space after the table
    \caption{Summary of Free-For-All (F4A) Optimisation Runs. Appendix references point to Appendix~\ref{app:exp_results_plots}. Pre-processor settings \texttt{ignore\_case} (normalise text casing) and \texttt{keep\_punc} (keep punctuation) are highlighted. See Section~\ref{sec:preproc} for more information. }
    \label{tab:f4a_summary}
\end{table}

\noindent These runs were performed for both MPNet and Nomic embeddings. The goal was to identify the overall best-performing and most robust configurations considering both the clustering algorithm and the pre-processing variations.

Results from the F4A phase provided insights into the influence of pre-processing choices. Comparing the cluster size distributions in Appendix~\ref{sec:f4a_whisker_appendix} (Figures~\ref{fig:mpnet_f4a_whisker_appendix} and \ref{fig:nomic_f4a_whisker_appendix}), Run 2 (\texttt{ignore\_case=False}, \texttt{keep\_punc=True}) appeared to perform poorly, in both MPNet and Nomic, where only DBSCAN produced an optimal configuration finding just one clustering (Figure~\ref{fig:mpnet_ft_f4a_whisker}).

Runs 1 and 3, both featuring \texttt{keep\_punc=False}, generally exhibited lower variance in cluster sizes compared to Runs 2 and 4 (\texttt{keep\_punc=True}), especially visible in the DBSCAN results (known for variability from Section~\ref{sec:h2h_results}). This suggests that removing punctuation (\texttt{keep\_punc=False}) is beneficial, likely because punctuation tokens introduced noise which was detrimental to the embedding models' semantic representations for this dataset. Comparing Run 1 (\texttt{ignore\_case=False}) and Run 3 (\texttt{ignore\_case=True}), both with punctuation removed, Run 3 (ignoring case) often showed slightly more consistent results (especially in the Nomic embeddings), suggesting that ignoring case standardisation also helps.

However, HDBSCAN configurations still produced some outliers in terms of cluster size across most runs, indicating sensitivity in certain parameter regions. However, these outliers may be more useful configurations, as HDBSCAN tends to produce very conservative configurations with low cluster sizes of around 3 to 5.

The extreme outlier cluster size (>400) found in MPNet Run 1 (Figure~\ref{fig:mpnet_ff_f4a_whisker}) might highlight a specific sensitivity or edge case within the optimisation process for that particular setting. Overall, the F4A results reinforce the preference for HDBSCAN and suggest that the default pre-processing (Run 3: \texttt{ignore\_case=True}, \texttt{keep\_punc=False}) provides a good balance of performance and stability.

% The results results from the F4A phase indicated that Run 2 was the worst configuration, as it produced only one optimal configuration (DBSCAN) of a very low cluster size (1 for MPNet, 4 for Nomic). The runs with the lowest variation were 1 and 3 for both Nomic and MPNet, indicated by the DBSCAN runs (which were shown to be the most variable in Section~\ref{sec:h2h_results}). This suggests that, indicated by the common \texttt{keep\_punct = False}, punctuation tokens introduced a large amount of noise to the embedding models and the pre-processing stage helped. Additionally, HDBSCAN produces outliers in all runs (except those where it is not present), indicating that, due to this instability, the `best' configurations are likely not yet found. Moreover, both in MPNet and Nomic embeddings, \texttt{ignore\_case = True} consistently produces clusters, albeit it does not guarantee stability. Also, Figure~\ref{fig:mpnet_ff_f4a_whisker} shows an extreme outlier configuration, which produced a cluster size of over 400. This might indicate that in the general case, this setting is stable and might highlight an issue with the hyperparameter optimisation process.
 
% Figures for F4A could include:
% - Plots comparing best heuristic scores (e.g., box plots) for each algorithm under each flag setting.
% - 3D heuristic plots for the overall best configurations found in F4A.
% - Whisker plots of cluster sizes for the overall best configurations.
% - Plot showing the number of clusters found by DBSCAN/HDBSCAN in F4A runs.
% \todo{Add relevant summary figures/tables here and ensure all detailed supporting figures are placed in Appendix B and correctly referenced.}

\section{Distribution of Hyperparameters}\label{sec:dist-hyp}

\todo{clean up wording in this section and think about how it will change the abstract, the introduction}

% The aggregated hyperparameter distributions of all optimal configurations (see Table~\ref{tab:hypreal}) found across H2H and F4A phases are presented in Appendix~\ref{sec:agg_config_dist}. Additionally the distribution of the hyperparameters for the top 5 configurations, selected via rank aggregation (Section~\ref{sec:hyperparameterOpt}), is also presented. These distributions are presented overall, for MPNet embeddings and for Nomic embeddings, as detailed in Table~\ref{tab:dist-hyp}.

To gain further insight into the parameter settings favoured by the optimisation process, this section examines the distribution of hyperparameters corresponding to the optimal configurations identified by Optuna across the H2H and F4A phases. Appendix~\ref{sec:agg_config_dist} presents visualisations of these distributions, referencing the hyperparameters defined in Table~\ref{tab:hypreal}. Specifically, it shows the aggregated distributions for:
\begin{itemize}
    \item All optimal configurations found by Optuna before rank aggregation.
    \item The top 5 configurations selected from each study via rank aggregation (as detailed in Section~\ref{sec:hyperparameterOpt}).
\end{itemize}
These distributions are provided separately for the combined results ('Overall'), for runs using MPNet embeddings, and for runs using Nomic embeddings, as summarised in Table~\ref{tab:dist-hyp}.


\begin{table}[H]
    \centering
    \rowcolors{2}{gray!10}{white}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l l l l l}
        \toprule
        \textbf{Aggregation Type} & \textbf{Number of Config.} & \textbf{Optimal Config. Plots} & \textbf{Top 5 Plots} \\
        \midrule
        Overall        & 288 & Fig.~\ref{fig:overall_param_plot} & Fig.~\ref{fig:overall_param_plot_top5} \\
        MPNet          & 144 & Fig.~\ref{fig:mpnet_param_plot} & Fig.~\ref{fig:mpnet_param_plot_top5} \\
        Nomic          & 144 & Fig.~\ref{fig:nomic_param_plot} & Fig.~\ref{fig:nomic_param_plot_top5} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    % \vspace{0.5em} % Add a small space after the table
    \caption{Summary of Optuna optimal hyperparameter plots. The top 5 hyperparameter plots are selected via rank aggregation. Appendix references point to Appendix~\ref{app:exp_results_plots}. Parameters correspond to Table~\ref{tab:hypreal} See Section~\ref{sec:hyperparameterOpt} for more information.}
    \label{tab:dist-hyp}
\end{table}

% Visual inspection of the figures listed in Table~\ref{tab:dist-hyp} indicates, as noted in Section~\ref{sec:umaphyperparameters}, the `cosine' metric does not out-perform the `euclidean' metric. In fact, in the top 5 configurations, the `euclidean' metric is used by over 60\% of the overall top 5 configurations, per run. Additionally, Nomic (Figure~\ref{fig:overall_param}) tends to prefer `euclidean' distance, with around 60\% of all configurations using Nomic used the distance metric and just under 80\% of the top 5 configurations using it. Additionally, all top 5 configurations preferred a higher number of components (5) during UMAP dimensionality reduction. Further, MPNet and Nomic disagreed on the best clustering algorithm, with MPNet preferring DBSCAN in over 40\% of runs and Nomic preferring HDBSCAN in around 40\%. The left-skewed \texttt{min\_dist} distribution was not significantly different between the embeddings indicating that the search space for this parameter was too large. k-Medoids, ignoring some outliers and across both embedding spaces, tended to prefer clusters from 18 to 21 in the top 5 configurations. MPNet tended to select very low \texttt{min\_pts}, which may indicate the reason for poor DBSCAN performance. On the other hand, Nomic preferred \texttt{min\_pts} in the range of 7-8.


Analysis of the hyperparameter distributions (Figures~\ref{fig:overall_param} to \ref{fig:nomic_param}) reveals several notable trends among the optimal configurations:
\begin{itemize}
    \item \textbf{Distance Metric} - Confirming preliminary observations (Section~\ref{sec:umaphyperparameters}), the `euclidean' distance metric was frequently preferred over `cosine', particularly for Nomic embeddings. Over 60\% of the overall top 5 configurations utilised `euclidean' distance, rising to nearly 80\% among the top 5 configurations specifically for Nomic (Figure~\ref{fig:nomic_param_plot_top5}). This contradicts the common heuristic favouring cosine distance for text embeddings \cite{baba2017plagiarism, cao2024recent}.
    \item \textbf{UMAP Components} - Optimal solutions consistently favoured a higher number of UMAP components, typically 5 or 6 (the upper end of the search range), suggesting that retaining slightly higher dimensionality post-UMAP aids clustering performance for this dataset.
    \item \textbf{Clustering Algorithm Preference} - The preferred clustering algorithm differed between embedding models in the top 5 configurations. MPNet runs frequently favoured DBSCAN (over 40\%), whereas Nomic runs showed a preference for HDBSCAN (around 40\%) (Figures~\ref{fig:mpnet_param_plot_top5} and \ref{fig:nomic_param_plot_top5}).
    \item \textbf{Density Parameters (\texttt{MinPts}/$m_{pts}$)} - For DBSCAN, MPNet's optimal runs often selected very low \texttt{MinPts} values, which might contribute to the observed instability and poorer performance of DBSCAN with MPNet embeddings. Conversely, Nomic's optimal DBSCAN runs preferred \texttt{MinPts} values around 7-8. For HDBSCAN, preferred $m_{pts}$ values varied but often fell within the lower to mid-range of the search space [3, 30].
    \item \textbf{k-Medoids Clusters} - Ignoring outliers, the top 5 k-Medoids configurations tended to favour cluster counts ($k$) in the range of 18 to 21, aligning reasonably well with the broader 5-25 range estimated from HDBSCAN stability.
    \item \textbf{UMAP \texttt{min\_dist}} - The distribution for \texttt{min\_dist} was heavily left-skewed towards lower values (closer to 1) across both embeddings and both optimal sets (all vs. top 5). This strong preference suggests the defined search space $(0, 0.2)$ might have been slightly too broad, as the optimizer rarely favoured values near the upper bound.
\end{itemize}

\section{Discussion}\label{sec:discussion} 

The analysis of cluster sizes across the H2H and F4A experiments (Appendices~\ref{sec:h2h_whisker_appendix} and \ref{sec:f4a_whisker_appendix}) suggests a plausible range for the number of distinct fault categories within the ion source data. Excluding extreme outliers and configurations resulting in very few clusters (often associated with DBSCAN instability or less optimal methods), the more stable HDBSCAN configurations frequently identified between approximately 5 and 25 clusters. This range may represent a reasonable estimate for the number of operationally relevant fault categories inferable from this dataset using the current methodology, although further validation is required.


% \todo{incorporate the new results from aggregated}
% The experimental results presented, combining insights from both the H2H and F4A phases, highlight several key aspects of applying unsupervised clustering to the ISIS Operalog data.

The experimental results, combining insights from the H2H and F4A phases and the hyperparameter distribution analysis (Section~\ref{sec:dist-hyp}), highlight several key aspects of applying unsupervised clustering to the ISIS Operalog data:


% \begin{enumerate}
%     \item The instability observed with DBSCAN, particularly regarding the variance in cluster sizes and its occasional failure to find competitive solutions during optimisation (summarised in Tables~\ref{tab:h2h_summary} and \ref{tab:f4a_summary} referencing appendix figures), suggests that its reliance on a single global density threshold (\texttt{Eps}) is a limitation for this dataset. This aligns with the expectation that the Operalog entries, varying significantly in length and specificity, likely form clusters of differing densities (Section~\ref{sec:unsupcat}).
%     \item HDBSCAN emerged as a more robust density-based method, consistently finding `good' solutions according to the heuristics and demonstrating better stability than DBSCAN across runs, even without a predefined cluster count. Its ability to handle varying densities via its hierarchical approach and stability-based extraction \cite{campello2013density} appears advantageous here.
%     \item While k-Medoids provided stable cluster counts due to its nature, its suitability is questionable given the likely non-spherical shapes of clusters in the UMAP-projected embedding space (Figure~\ref{fig:dimred-compare}). The optimal configurations found via heuristics might not correspond well to the true underlying fault categories if the shape assumption is violated.
%     \item the F4A phase demonstrated that pre-processing choices significantly impacted the quality of the resulting clusters. Specifically, removing punctuation (\texttt{keep\_punc=False}) consistently yielded better and more stable results across algorithms and embedding models (comparing Runs 1\&3 vs. 2\&4 in Table~\ref{tab:f4a_summary}), suggesting punctuation added noise. Ignoring case (\texttt{ignore\_case=True}) also appeared beneficial for stability compared to maintaining case sensitivity when punctuation was removed (comparing Run 3 vs. Run 1).
%     \item There was no clear difference between the MPNet and Nomic. However, Optuna tended to produce configurations within the inferred true cluster size range more often in Nomic (Figure~\ref{fig:nomic_f4a_whisker_appendix}. However, in both H2H and F4A, HDBSCAN run on Nomic embeddings resulted in more conservative cluster sizes, with few outliers beyond that range.
%     \item Comparing the embedding models, neither MPNet nor Nomic demonstrated a dramatically superior performance across all heuristic trade-offs and stability plots (Appendices~\ref{sec:h2h_config_appendix}-\ref{sec:f4a_whisker_appendix}). However, optimal configurations derived from Nomic embeddings potentially aligned more frequently with the inferred reasonable cluster size range (5-25 clusters discussed earlier), particularly in the F4A phase (Figure~\ref{fig:nomic_f4a_whisker_appendix}). Furthermore, when using HDBSCAN, Nomic embeddings tended to yield cluster size distributions with fewer extreme outliers compared to MPNet (compare Figures~\ref{fig:mpnet_h2h_whisker_appendix} and \ref{fig:nomic_h2h_whisker_appendix}, and Figures~\ref{fig:mpnet_f4a_whisker_appendix} and \ref{fig:nomic_f4a_whisker_appendix}).
%     \item the process highlighted the importance of using multiple clustering heuristics. Different configurations optimised differently across the Silhouette, Davies-Bouldin, and Calinski-Harabasz indices, necessitating the rank aggregation approach to select balanced solutions. However, the ultimate validation of the `best' clustering lies in the semantic and operational utility of the resulting categories and their generated labels. While this pipeline provides a systematic, quantitatively-guided method for finding promising clusterings, the qualitative assessment of the generated labels remains crucial and identifies a key area for future refinement. Although, another interpretation of this could mean that the clustering heuristics are not specialised enough to the dataset, possibly motivating a re-evaluation of what a `good' clustering is.
% \end{enumerate}


\begin{enumerate}
    \item \textbf{DBSCAN Instability} - The observed instability of DBSCAN, particularly its variance in cluster counts and occasional failure to find competitive solutions, suggests its reliance on a single global density threshold (\texttt{Eps}) is a limitation for the Operalog's likely varying cluster densities. The preference for very low \texttt{MinPts} in optimal MPNet runs might exacerbate this issue.

    \item \textbf{HDBSCAN Robustness} - HDBSCAN emerged as a more robust density-based method, consistently finding `good' solutions and demonstrating better stability. Its ability to handle varying densities \cite{campello2013density} appears advantageous, aligning with its preference among top configurations for Nomic embeddings.

    \item \textbf{k-Medoids Suitability} - While providing stable cluster counts (often favouring 18-21 clusters in top runs), k-Medoids' appropriateness remains questionable given the non-spherical cluster shapes suggested by UMAP projections (Figure~\ref{fig:dimred-compare}). The optimal heuristic scores might not perfectly reflect meaningful operational categories if the shape assumption is violated.

    \item \textbf{Pre-processing Impact} - The F4A phase confirmed that removing punctuation (\texttt{keep\_punc=False}) and ignoring case (\texttt{ignore\_case=True}) generally yielded more stable and quantitatively better clustering results, likely by reducing noise in the embeddings.

    \item \textbf{Embedding Model Nuances} - While neither MPNet nor Nomic showed dramatic overall superiority, Nomic embeddings potentially aligned more frequently with the inferred 5-25 cluster range and yielded fewer extreme outliers with HDBSCAN. Furthermore, the analysis revealed distinct preferences - Nomic strongly favoured the `euclidean' distance metric and HDBSCAN clustering in its top configurations, whereas MPNet showed a greater tendency towards the `cosine' metric and DBSCAN.

    \item \textbf{UMAP Parameter Insights} - Optimal configurations consistently preferred higher dimensionality (5-6 components) after UMAP reduction and strongly favoured lower \texttt{min_dist} values, suggesting denser packing in the reduced space aids the clustering algorithms used. The preference for the `euclidean' metric over `cosine', especially for Nomic, is a noteworthy finding that warrants further investigation, potentially indicating specific characteristics of the Nomic embedding space or the UMAP implementation.

    \item \textbf{Heuristic-Based Optimisation} - The process confirmed the value of multi-objective optimisation using several heuristics, as configurations balanced trade-offs differently. However, the ultimate validation requires assessing the semantic and operational utility of the resulting categories and labels, representing a crucial area for future work. The potential need for more domain-specific or task-aware evaluation metrics beyond standard internal heuristics could also be considered.
\end{enumerate}


% \noindent Finally, selecting the single definitive configuration for final label generation presents challenges. While Optuna and rank aggregation identify a set of quantitatively `optimal' configurations (i.e. the top 5), choosing among these requires further consideration. The stability analysis, examining cluster size distributions over multiple runs (Appendices~\ref{sec:h2h_whisker_appendix} and \ref{sec:f4a_whisker_appendix}), provides valuable insights into robustness, but ambiguity can remain in selecting the best trade-off between heuristic scores, stability, and the interpretability of the resulting clusters (especially for methods like HDBSCAN which handles noise and varying densities differently depending on parameters). The inherent stochasticity within the pipeline (such as from UMAP) adds another layer of complexity, as slightly different cluster assignments might arise even from the same `best' configuration parameters across independent runs. This reinforces the point that while this pipeline offers a systematic approach, the ultimate validation relies on qualitative assessment and operational utility. Therefore, refining the criteria for selecting the final configuration and developing more robust methods for quantitatively assessing the generated label quality are important directions for future work.

\noindent Finally, selecting the single definitive configuration for final label generation presents challenges. While Optuna and rank aggregation identify quantitatively `optimal' configurations, choosing among the top set requires balancing heuristic scores, stability (cluster counts), parameter plausibility (i.e. reasonable \texttt{MinPts}), and ultimately, the interpretability of the resulting clusters. The inherent stochasticity within the pipeline (UMAP) adds complexity, reinforcing that validation relies on qualitative assessment and operational utility. Therefore, refining selection criteria and developing methods for quantitatively assessing label quality are important future directions.

% \todo[inline, caption={}]{Original TODOs moved here for review/removal:
%     \begin{itemize}
%         \item Rank aggregation - DONE (covered in methodology and applied)
%         \item plot whiskers (or maybe in results?). acc this should be in results as it makes sense that is quantitative evaluation of results. - DONE (Moved to Appendix, referenced from text)
%         \item results should also talk about qualitatively and motivate the conclusion, this is a step in the right direction but not 100\%. - DONE (Addressed in Discussion, linking quantitative results to qualitative label assessment)
%         \item Describe the results and analyse the results - DONE (Integrated into Results/Discussion)
%         \item Analyse the sentence embedding results. - DONE (Implicitly done by running pipeline on both MPNet/Nomic, specific comparisons belong in F4A results)
%         \item Analyse the UMAP hyperparameter optimisation qualitatively, mention that we use Optuna. - DONE (Covered in Methodology/Results intro)
%         \item Next steps: Fine tuning the PLM (Nomic or MPNet). Using MOE nomic (v2). Rent higher computation, then you can use top performing PLMS. Using t-SNE to see differences. - DONE (These belong in Chapter 5: Conclusion/Future Work)
%     \end{itemize}
% }

\chapter{Conclusion}\label{chap:conclusion}

This project successfully developed and evaluated an end-to-end unsupervised machine learning pipeline for the automatic categorisation of unstructured operational log entries from the ISIS Neutron and Muon Source's Operalog, focusing specifically on \texttt{FaultDescription} data related to the ion source equipment. Recognising the open-ended nature of this task and the lack of established industry benchmarks. The primary contribution lies in demonstrating the feasibility of applying modern NLP techniques, specifically sentence embeddings combined with dimensionality reduction and clustering, to extract meaningful structure from complex, domain-specific textual data.

The core methodology involved pre-processing the text, generating embeddings using MPNet \cite{song2020mpnet} and Nomic \cite{nussbaum2024nomic} models, reducing dimensionality with UMAP \cite{mcinnes2018umap}, performing clustering using k-Medoids \cite{kmedoids}, DBSCAN \cite{ester1996density}, and HDBSCAN \cite{campello2013density}, and systematically optimising hyperparameters via Optuna \cite{akiba2019optuna} based on multiple internal clustering heuristics \cite{rousseeuw1987silhouettes, davies1979cluster, calinski1974dendrite}. The experimental results presented in Chapter~\ref{chap:results} and Appendix~\ref{app:exp_results_plots} provided valuable insights. Notably, HDBSCAN \cite{campello2013density} consistently emerged as a highly applicable clustering algorithm, effectively handling varying cluster densities and demonstrating superior stability compared to DBSCAN and k-Medoids. The hyperparameter analysis revealed a preference for the `euclidean' distance metric in UMAP, particularly with Nomic embeddings, and indicated that retaining slightly higher dimensionality (5-6 components) post-reduction aids clustering. Furthermore, optimal pre-processing involved removing punctuation and ignoring text case. While both MPNet and Nomic embeddings enabled effective clustering, Nomic showed potential advantages, aligning more frequently with the inferred range of 5-25 distinct categories and exhibiting fewer extreme outliers when paired with HDBSCAN, which was its preferred algorithm in top configurations (unlike MPNet which favoured DBSCAN, shown to be unstable). A naive, heuristic-based method for automatically generating cluster labels using SpaCy \cite{spacy2} was implemented, and the entire process was encapsulated within a reusable CLI tool (Section~\ref{sec:CLI}) to facilitate further experimentation.

Despite the successful demonstration of the pipeline's capability, several challenges and limitations remain, as discussed in Section~\ref{sec:discussion}. Selecting the single best configuration from the set of optimal solutions produced by multi-objective optimisation is non-trivial, requiring careful consideration of the trade-offs between quantitative heuristics, parameter plausibility, and qualitative assessment of cluster interpretability. The inherent stochasticity, particularly from UMAP, also adds complexity to achieving perfectly reproducible cluster assignments. Furthermore, the current label generation method requires significant refinement to ensure the labels are truly informative and operationally useful.

These limitations naturally lead to several avenues for future work:
\begin{enumerate}
    \item \textbf{Fine-tuning PLMs} - The performance of the pipeline could potentially be improved by fine-tuning the embedding models (MPNet or Nomic) specifically on the Operalog data or related technical documentation from ISIS. This could adapt the models more closely to the domain-specific language and nuances, potentially leading to more discriminative embeddings and possibly reconciling differing algorithmic preferences.
    \item \textbf{Exploring Alternative Models \& Metrics} - Given the rapid pace of development in NLP, investigating newer embedding models could yield benefits, perhaps including those specifically optimised for Euclidean space if this metric preference persists. However, this may necessitate access to greater computational resources. Further investigation into the unexpected preference for `euclidean' over `cosine' distance is also warranted.
    \item \textbf{Hyperparameter Search Refinement} - The hyperparameter search space could be further refined based on the observed distributions, for example, by narrowing the range for UMAP's \texttt{min\_dist} or exploring different ranges for density parameters like \texttt{MinPts}.
    \item \textbf{Refined Label Generation} - The current naive label generation (Section~\ref{sec:labelgen}) is a key area for improvement. More sophisticated techniques, such as leveraging topic modelling \cite{curiskis2020evaluation} within clusters or using abstractive summarization models \cite{nallapati2016abstractive}, should be explored. Evaluating label quality quantitatively also remains an open challenge.
    \item \textbf{Broader Data Scope} - The analysis should be extended beyond the `ion source' equipment type to encompass the full range of systems documented in the Operalog, assessing the pipeline's generalisability.
    \item \textbf{Integration with Structured Data} - Combining the insights from this text-based analysis with findings from the partner project focusing on structured ion source logs could yield a more holistic understanding of fault patterns and potentially improve predictive maintenance models.
\end{enumerate}

Finally, it is crucial to consider the ethical implications inherent in using complex machine learning models, particularly PLMs derived from external sources. As discussed in Section~\ref{sec:BERT}, relying on pre-trained models introduces a dependency on the practices of the model creators \cite{wolf2019huggingface}. There are potential risks associated with models trained on vast, often opaque datasets, including the possibility of inheriting societal biases present in the training data, or potential data contamination issues that might affect model behaviour in unforeseen ways \cite{min2023recent}. While less likely with reputable sources, the theoretical risk of maliciously manipulated models also exists. Therefore, a commitment to using models from transparent sources, critically evaluating their outputs, and understanding their limitations is paramount. Furthermore, handling operational data like the Operalog requires adherence to appropriate data privacy and confidentiality protocols, ensuring sensitive information is protected throughout the analysis pipeline. \\

\noindent In conclusion, this project has successfully demonstrated the potential of applying an unsupervised NLP pipeline, leveraging state-of-the-art embedding and clustering techniques, to automatically categorise unstructured operational logs from the complex ISIS facility. While further refinements, particularly in label generation and configuration selection, are necessary for direct operational deployment, the developed methodology, incorporating detailed hyperparameter analysis and encapsulated in a CLI tool, provides a valuable foundation. This work represents significant progress towards unlocking the rich information contained within textual logs, paving the way for improved diagnostics, data-driven insights, and ultimately, more effective predictive maintenance strategies at large-scale scientific facilities.

% \todo[inline, caption={}]{
%     \begin{itemize}
%         \item Definitely talk about clear and transparent PLMs and malicious stuff (see BERT section).
%     \end{itemize}
% }

% \todo[inline,caption={}]{
% Summarize your findings and suggest areas for future work.
% }

% -------------------------
% References
% -------------------------
% \bibliographystyle{plainnat}
\clearpage
\begin{multicols}{2} 
\small
\bibliographystyle{plainnat}
\bibliography{refs}  % Uses the external file refs.bib
\end{multicols}


\appendix

\chapter{Results and Figures}


\todo{fix this and talk about this in results.}

\section{UMAP Hyperparameter Search} \label{app:umaphypgrid}
Figures~\ref{fig:mpnet1} through \ref{fig:nomic3} in this appendix illustrate the results of the naive grid search performed over the UMAP hyperparameters listed in Table~\ref{tab:umapgrid}, applied to both MPNet and Nomic embeddings of the Operalog \texttt{FaultDescription} entries. These visualisations demonstrate the effects of varying parameters like \texttt{n\_neighbors}, \texttt{min\_dist} and \texttt{metric} as described conceptually in Section~\ref{sec:umap}. Notably, visual inspection suggests that, for the goal of creating a low-dimensional representation that isolates structural groupings in this specific dataset, the `cosine' distance metric did not produce significantly different or clearly superior results compared to the `euclidean' metric. This outcome is somewhat surprising as cosine distance is frequently recommended for text embedding applications in the literature \cite{cao2024recent, baba2017plagiarism}. This observation, combined with the difficulty in determining the optimal settings purely through visual inspection of numerous plots, makes it evident that this naive grid search approach is insufficient. It therefore motivates the need for the systematic hyperparameter optimisation process detailed in Section~\ref{sec:hyperparameterOpt}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/mpnethyp1.png}
    \caption{MPNet, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{metric} and \texttt{min\_dist}}
    \label{fig:mpnet1}
\end{figure}%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/mpnethyp2.png}
    \caption{MPNet, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{min\_dist} and \texttt{n\_neighbors}}
    \label{fig:mpnet2}
\end{figure}%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/mpnethyp3.png}
    \caption{MPNet, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{metric} and \texttt{n\_neighbors}}
    \label{fig:mpnet3}
\end{figure}%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/nomichyp1.png}
    \caption{Nomic, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{metric} and \texttt{min\_dist}}
    \label{fig:nomic1}
\end{figure}%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/nomichyp2.png}
    \caption{Nomic, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{min\_dist} and \texttt{n\_neighbors}}
    \label{fig:nomic2}
\end{figure}%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./images/nomichyp3.png}
    \caption{Nomic, grid search for UMAP hyperparameter (see Table~\ref{tab:umapgrid}) over the parameters \texttt{metric} and \texttt{n\_neighbors}}
    \label{fig:nomic3}
\end{figure}%

\clearpage

\section{Experimental Results Plots}\label{app:exp_results_plots}

This appendix section presents supplementary visualisations supporting the hyperparameter optimisation results discussed in Chapter~\ref{chap:results}. The plots detail outcomes for both the Head-to-Head (H2H) and Free-For-All (F4A) experimentation phases using Optuna \cite{akiba2019optuna}.

Section~\ref{sec:agg_config_dist} illustrates the distribution of hyperparameter values across the configurations identified as optimal by Optuna during the H2H and F4A runs, corresponding to Table~\ref{tab:hypreal}. It presents these distributions first for all optimal configurations found across the various studies, and then specifically for the top 5 configurations selected via rank aggregation (as described in Section~\ref{sec:hyperparameterOpt}) for each study. These plots offer insights into which parameter settings were frequently favoured by the optimisation process across different embedding models (Overall, MPNet, Nomic).

Sections~\ref{sec:h2h_config_appendix} and \ref{sec:f4a_config_appendix} display the results of the multi-objective optimisation studies. These 3-dimensional plots show the trade-offs between the Silhouette coefficient (higher is better), Calinski-Harabasz index (higher is better), and Davies-Bouldin index (lower is better) for the evaluated configurations. The best configuration found by Optuna, identified via rank aggregation (Section~\ref{sec:hyperparameterOpt}), are highlighted with red markers (top 5, or top 1 if fewer than 5 non-dominated solutions were found). Markers closer to the ideal top-left-front corner represent better solutions across the three heuristics; marker opacity indicates distance in the 3D space (more opaque is closer).

Sections~\ref{sec:h2h_whisker_appendix} and \ref{sec:f4a_whisker_appendix} present whisker plots illustrating the distribution of cluster sizes obtained from the optimal configurations identified in the corresponding optimisation runs. These plots show the stability and typical scale of clusters produced by different pipeline settings over 10 independent runs (to account for UMAP stochasticity). Note that empty plots indicate that no competitive configurations using that specific clustering method were found among the optimal set for that particular Optuna study.

\subsection{Aggregated Configuration Distributions}\label{sec:agg_config_dist}
\clearpage
\subsubsection{Overall}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/overall_param_plot.png}
        \caption{All optimal configurations}
        \label{fig:overall_param_plot} 
    \end{subfigure}
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/overall_param_plot_top5.png}
        \caption{Top 5 configurations (Rank Aggregated)}
        \label{fig:overall_param_plot_top5}
    \end{subfigure}
    \caption{Overall distribution of hyperparameter values across all optimal configurations identified by Optuna in H2H and F4A studies (top), and for the top 5 configurations selected via rank aggregation from each study (bottom).}
    \label{fig:overall_param}
\end{figure}

\subsubsection{MPNet}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_param_plot.png}
        \caption{All optimal configurations (MPNet)}
        \label{fig:mpnet_param_plot}
    \end{subfigure}
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_param_plot_top5.png}
        \caption{Top 5 configurations (MPNet, Rank Aggregated)}
        \label{fig:mpnet_param_plot_top5}
    \end{subfigure}
    \caption{Distribution of hyperparameter values for MPNet embeddings across all optimal configurations identified by Optuna in H2H and F4A studies (top), and for the top 5 configurations selected via rank aggregation from each study (bottom).}
    \label{fig:mpnet_param}
\end{figure}

\subsubsection{Nomic}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_param_plot.png}
        \caption{All optimal configurations (Nomic)}
        \label{fig:nomic_param_plot}
    \end{subfigure}
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_param_plot_top5.png}
        \caption{Top 5 configurations (Nomic, Rank Aggregated)}
        \label{fig:nomic_param_plot_top5}
    \end{subfigure}
    \caption{Distribution of hyperparameter values for MPNet embeddings across all optimal configurations identified by Optuna in H2H and F4A studies (top), and for the top 5 configurations selected via rank aggregation from each study (bottom).}
    \label{fig:nomic_param}
\end{figure}

\subsection{H2H Optimal Configuration Plots}\label{sec:h2h_config_appendix}
\subsubsection{MPNet}

\begin{figure}[H]
    \centering
    % Subfigures as before...
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvsdbscan.png}
        \caption{Run A: k-Medoids vs. DBSCAN}
        \label{fig:mpnet_kmedoids_vs_dbscan_h2h_config} % Adjusted label
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvshdbscan.png}
        \caption{Run B: k-Medoids vs. HDBSCAN}
        \label{fig:mpnet_kmedoids_vs_hdbscan_h2h_config} % Adjusted label
    \end{subfigure}
    \\
    \vspace{1em}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_dbscanvshdbscan.png}
        \caption{Run C: DBSCAN vs. HDBSCAN}
        \label{fig:mpnet_dbscan_vs_hdbscan_h2h_config}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_hdbscan.png}
        \caption{Run D: HDBSCAN Only}
        \label{fig:mpnet_hdbscan_only_h2h_config}
    \end{subfigure}
    % Revised main caption
    \caption{Heuristic trade-offs for optimal configurations from H2H Optuna runs using MPNet embeddings. Best solutions highlighted in red.}
    \label{fig:mpnet_h2h_config_appendix}
\end{figure}

\subsubsection{Nomic}

\begin{figure}[H]
    \centering
    % Subfigures as before...
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvsdbscan.png}
        \caption{Run A: k-Medoids vs. DBSCAN}
        \label{fig:nomic_kmedoids_vs_dbscan_h2h_config} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvshdbscan.png}
        \caption{Run B: k-Medoids vs. HDBSCAN}
        \label{fig:nomic_kmedoids_vs_hdbscan_h2h_config}
    \end{subfigure}
    \\
    \vspace{1em}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_dbscanvshdbscan.png}
        \caption{Run C: DBSCAN vs. HDBSCAN}
        \label{fig:nomic_dbscan_vs_hdbscan_h2h_config} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_hdbscan.png}
        \caption{Run D: HDBSCAN Only}
        \label{fig:nomic_hdbscan_only_h2h_config} 
    \end{subfigure}
    % Revised main caption
    \caption{Heuristic trade-offs for optimal configurations from H2H Optuna runs using Nomic embeddings. Best solutions highlighted in red.}
    \label{fig:nomic_h2h_config_appendix} % Main figure label
\end{figure}

% --- H2H Cluster Sizes ---
\subsection{H2H Cluster Size Distributions}\label{sec:h2h_whisker_appendix}

\subsubsection{MPNet}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvsdbscan_whisker.png}
        \caption{Run A: k-Medoids vs. DBSCAN}\label{fig:mpnet_h2h_runa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_kmedoidsvshdbscan_whisker.png}
        \caption{Run B: k-Medoids vs. HDBSCAN}\label{fig:mpnet_h2h_runb}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
    \ContinuedFloat
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_dbscanvshdbscan_whisker.png}
        \caption{Run C: DBSCAN vs. HDBSCAN}\label{fig:mpnet_h2h_runc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_hdbscan_whisker.png}
        \caption{Run D: HDBSCAN Only}\label{fig:mpnet_h2h_rund}
    \end{subfigure}
    \caption{Cluster size distributions over 10 runs for optimal H2H configurations using MPNet embeddings. Empty plots indicate no optimal configurations were found for that method.}
    \label{fig:mpnet_h2h_whisker_appendix}
\end{figure}

\subsubsection{Nomic}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvsdbscan_whisker.png}
        \caption{Run A: k-Medoids vs. DBSCAN}\label{fig:nomic_h2h_runa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_kmedoidsvshdbscan_whisker.png}
        \caption{Run B: k-Medoids vs. HDBSCAN}\label{fig:nomic_h2h_runb}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
    \ContinuedFloat
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_dbscanvshdbscan_whisker.png}
        \caption{Run C: DBSCAN vs. HDBSCAN}\label{fig:nomic_h2h_runc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_hdbscan_whisker.png}
        \caption{Run D: HDBSCAN Only}\label{fig:nomic_h2h_rund}
    \end{subfigure}
    \caption{Cluster size distributions over 10 runs for optimal H2H configurations using Nomic embeddings. Empty plots indicate no optimal configurations were found for that method.}
    \label{fig:nomic_h2h_whisker_appendix}
\end{figure}

% --- F4A Configuration Plots ---
\subsection{F4A Optimal Configuration Plots}\label{sec:f4a_config_appendix} % Renamed section title

\subsubsection{MPNet}

\begin{figure}[H]
    \centering
    % Subfigures arranged 2x2
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_false-false.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=False}}
        \label{fig:mpnet_ff_f4a_config} % Adjusted label
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_false-true.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=True}}
        \label{fig:mpnet_ft_f4a_config} % Adjusted label
    \end{subfigure}
    \\
    \vspace{1em}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_true-false.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=False}}
        \label{fig:mpnet_tf_f4a_config} % Adjusted label
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_true-true.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=True}}
        \label{fig:mpnet_tt_f4a_config} % Adjusted label
    \end{subfigure}
    % Revised main caption
    \caption{Heuristic trade-offs for optimal configurations from F4A Optuna runs using MPNet embeddings, varying pre-processing flags. Best solutions highlighted in red.}
    \label{fig:mpnet_f4a_config_appendix} % Main figure label
\end{figure}

\subsubsection{Nomic}

\begin{figure}[H]
    \centering
    % Subfigures arranged 2x2
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_false-false.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=False}}
        \label{fig:nomic_ff_f4a_config} % Adjusted label
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_true-false.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=True}}
        \label{fig:nomic_ft_f4a_config} % Adjusted label
    \end{subfigure}
    \\
    \vspace{1em}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_false-true.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=False}}
        \label{fig:nomic_tf_f4a_config} % Adjusted label
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_true-true.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=True}}
        \label{fig:nomic_tt_f4a_config} % Adjusted label
    \end{subfigure}
    % Revised main caption
    \caption{Heuristic trade-offs for optimal configurations from F4A Optuna runs using Nomic embeddings, varying pre-processing flags. Best solutions highlighted in red.}
    \label{fig:nomic_f4a_config_appendix} % Main figure label
\end{figure}

% --- F4A Cluster Sizes ---
\subsection{F4A Cluster Size Distributions}\label{sec:f4a_whisker_appendix}

\subsubsection{MPNet}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_false-false_whisker.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=False}} \label{fig:mpnet_ff_f4a_whisker}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_false-true_whisker.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=True}} \label{fig:mpnet_ft_f4a_whisker}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
    \ContinuedFloat
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_true-false_whisker.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=False}} \label{fig:mpnet_tf_f4a_whisker}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/mpnet_true-true_whisker.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=True}} \label{fig:mpnet_tt_f4a_whisker}
    \end{subfigure}
    \caption{Cluster size distributions over 10 runs for optimal F4A configurations using MPNet embeddings, varying pre-processing flags.}
    \label{fig:mpnet_f4a_whisker_appendix}
\end{figure}

\subsubsection{Nomic}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_false-false_whisker.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=False}} \label{fig:nomic_ff_f4a_whisker}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_true-false_whisker.png}
        \caption{\texttt{ignore\_case=False}, \texttt{keep\_punct=True}} \label{fig:nomic_ft_f4a_whisker}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
    \ContinuedFloat
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_false-true_whisker.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=False}} \label{fig:nomic_tf_f4a_whisker}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/nomic_true-true_whisker.png}
        \caption{\texttt{ignore\_case=True}, \texttt{keep\_punct=True}} \label{fig:nomic_tt_f4a_whisker}
    \end{subfigure}
    \caption{Cluster size distributions over 10 runs for optimal F4A configurations using Nomic embeddings, varying pre-processing flags.}
    \label{fig:nomic_f4a_whisker_appendix}
\end{figure}


\pagebreak
\chapter{Supplementary Technical Details}
\section{BERT and XLNet Pre-training}\label{app:bert_xlnet_details}

This section provides supplementary technical details on the pre-training objectives and mechanisms of the BERT and XLNet models, as discussed conceptually in Section~\ref{sec:BERT}.

\subsection{BERT Pre-training Tasks}

The BERT model \cite{devlin2019bert} employs two pre-training objectives during its pre-training phase:

\begin{itemize}
    \item \textbf{Masked Language Modelling} - During training, a portion of the input tokens are `masked' and the model predicts these masked tokens based on the remaining context, using cross-entropy loss \cite{zhang2018generalized}. In the literature, this is referred to as a Cloze task \cite{taylor1953cloze}. In the experiments conducted by (\citet{devlin2019bert}) 15\% of tokens in each sequence are randomly selected for masking. To mitigate the mismatch between pre-training and fine-tuning phases where the \texttt{[MASK]} token does not appear, the training data generator replaces each chosen masked token with: (1) the special token \texttt{[MASK]} 80\% of the time; (2) a random token 10\% of the time; and (3) the original token itself 10\% of the time \cite{devlin2019bert}.

    \item \textbf{Next Sentence Prediction} - In this task, the model is trained on a binary classification task predicting the relationship between sentence pairs. Given two sentences \texttt{A} and \texttt{B} either (1) sentence \texttt{B} is the actual sentence that follows \texttt{A} 50\% of the time or (2) sentence \texttt{B} is randomly selected from the corpus 50\% of the time \cite{devlin2019bert}. This task aimed to help BERT understand sentence relationships, though later models like RoBERTa and MPNet found it might not always be beneficial \cite{liu2019roberta, song2020mpnet}.
\end{itemize}

Figure~\ref{fig:bert1_appendix} illustrates the overall pre-training and fine-tuning workflow for BERT.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{BERT1.png} 
    \caption[BERT Pre-training and Fine-tuning Phases (Appendix)]{The overall pre-training and fine-tuning phases for BERT. For different down-stream tasks, notice the same architecture, except output layer, is used. Source: (\citet{devlin2019bert}). (Figure originally appeared as Figure 2.2)}
    \label{fig:bert1_appendix} 
\end{figure}

\subsection{XLNet Technical Details}\label{app:xlnet_maths}

XLNet \cite{yang2019xlnet} introduced mechanisms to overcome limitations observed in BERT, particularly concerning the pre-train-to-fine-tune discrepancy and the independence assumption in MLM.

\subsubsection*{Auto-regressive vs. Auto-encoding Perspectives}
According to (\citet{yang2019xlnet}), pre-training objectives can be broadly categorized as auto-regressive (AR) or auto-encoding (AE). AR language modelling, given a sequence of tokens $X = (x_1, ..., x_T)$, aims to factorize the likelihood of a token into either a forward or backward product:
\begin{align}
    p(x) &= \prod_{i=1}^{T} p(x_i | X_{<i}) \label{eq:fwdprod_appendix} \\
    p(x) &= \prod_{i=T}^{i} p(x_i | X_{>i}) \label{eq:bckprod_appendix}
\end{align}
where $X_{<i}$ denotes tokens before position $i$. As standard AR models only encode context unidirectionally, their effectiveness can be limited \cite{yang2019xlnet}. AE models like BERT reconstruct original data from corrupted input (i.e. masked tokens), allowing bidirectional context but potentially suffering from issues like the pretrain-finetune discrepancy and independence assumptions \cite{yang2019xlnet}.

\subsubsection*{Permutation Language Modeling Formalism}
XLNet employs Permutation Language Modelling as an AR objective that captures bidirectional context. It maximizes the expected log-likelihood over all possible factorisation orders (permutations) of the input sequence \cite{yang2019xlnet}. More formally:
\begin{equation}
    \max_{\theta} \text{  } \mathbb{E}_{z\in\mathcal{Z}_T} \bigl[ \sum_{i=1}^{T} \log p_{\theta} \bigl(x_{z_i}\mid x_{z_{<i}}\bigr) \bigr], \label{eq:plm_appendix} % Relabeled
\end{equation}
where $\mathcal{Z_T}$ is the set of all $T!$ permutations of the index list $[1, ... , T]$. For a given permutation $z = (z_1, ..., z_T)$, $X_{z_{<i}}$ denotes the tokens appearing before $z_i$ in that permutation. $\theta$ represents the shared model parameters across all permutations \cite{yang2019xlnet}. By training across all permutations, each position learns to utilise context from all other positions.

\subsubsection*{Two-Stream Self-Attention Mechanism}\label{sec:twostream}
To implement permutation language modelling while preventing the model from seeing the token it needs to predict, XLNet uses a two-stream self-attention mechanism \cite{vaswani2017attention, yang2019xlnet}. It maintains two hidden states per position at each layer:
\begin{enumerate}
    \item \textbf{Content Stream ($h$)} - Encodes both the content and position of a token, similar to standard transformer hidden states. It can access contextual information up to and including the current position in the permutation.
    \item \textbf{Query Stream ($g$)} - Encodes contextual information and the current position $z_i$, but, importantly, not the content $x_{z_i}$ of the token at the current position. It is used to gather information from the context $x_{z_{<i}}$ to predict $x_{z_i}$.
\end{enumerate}
At layer $m$ and target position $z_i$, the streams are updated using attention mechanisms, conceptually as follows \cite{vaswani2017attention, yang2019xlnet}:
\begin{align}
    g_{z_i}^{(m)} &\leftarrow \text{Attention} (Q = g_{z_i}^{(m - 1)}, KV = h_{z_{<i}}^{(m - 1)}) \label{eq:xlnet_g_update_appendix} \\ % Relabeled
    h_{z_i}^{(m)} &\leftarrow \text{Attention} (Q = h_{z_i}^{(m - 1)}, KV = h_{z_{\le i}}^{(m - 1)}) \label{eq:xlnet_h_update_appendix} % Relabeled
\end{align}
The query stream $g_{z_i}^{(m)}$ attends only to the content representations $h$ of tokens preceding $z_i$ in the current permutation. The content stream $h_{z_i}^{(m)}$ calculates an attention distribution over tokens up to and including $z_i$. After the final layer $M$, the query stream representation $g_{(z_i)}^{(M)}$ is used to compute the probability distribution over the vocabulary for predicting $x_{z_i}$ via a soft-max layer \cite{gao2017properties, yang2019xlnet}. This mechanism allows XLNet to predict tokens using contextual information gathered auto-regressively based on the permutation, without directly accessing the target token's content representation.

\section{MPNet Mathematical Formulations} \label{app:mpnet_maths}

This section provides supplementary mathematical details for the MPNet model discussed in Section~\ref{sec:rationale_mpnet_nomic}.

\subsection{MPNet Pre-training Objective}

The pre-training objective for MPNet aims to maximize the expected log-likelihood of predicting tokens based on a permuted context that includes information about masked tokens later in the sequence. Formally, let $\mathcal{Z}_{T}$ be the set of all $T!$ permutations of the index list $[1,...,T]$. For a given permutation $z=(z_{1},...,z_{T})$, let $x_{z_{i}}$ denote the token at original position $z_i$, and $x_{z<i}$ denote the sequence of tokens preceding $z_i$ in the permutation $z$. Let $M$ be the set of indices corresponding to masked tokens, and $M_{z>c}$ be the subset of masked token indices appearing after position $c$ in the permutation $z$. The objective is defined as \cite{song2020mpnet}:

\begin{equation} \label{eq:mpnet_objective_appendix}
\max_{\theta} \mathbb{E}_{z\in\mathcal{Z}_{T}} \left[ \sum_{i=1}^{T} \log p_{\theta}(x_{z_{i}} \mid x_{z<i}, M_{z>c}) \right]
\end{equation}

where $\theta$ represents the shared model parameters, fixed for each permutation $z$.


\subsection{MPNet Input Structuring Example}

To implement the objective (Equation~\ref{eq:mpnet_objective_appendix}), the input sequence is rearranged based on the permutation $z$ and a split point $c$. The sequence is divided into non-predicted tokens ($x_{z_{\le c}}$) and predicted tokens ($x_{z_{>c}}$). The input representation fed to the model concatenates the non-predicted part, mask token placeholders corresponding to the predicted part ($M_{z>c}$), and the actual predicted tokens (used only for loss calculation). For an example input sequence $(x_1, ..., x_6)$, a permutation $z=(x_3, x_5, x_2, x_1, x_4, x_6)$, and a split point $c=3$, the structured input is represented as \cite{song2020mpnet}:

\begin{equation} \label{eq:mpnet_input_appendix}
\langle x_{z_{\le c}}; M_{z>c}; x_{z_{>c}} \rangle = \langle x_3, x_5, x_2; [M], [M], [M]; x_1, x_4, x_6 \rangle
\end{equation}

where $[M]$ denotes the mask token placeholder.

% \subsection{MPNet Two-Stream Self-Attention Architecture}
%
% \begin{figure}[htbp]
%     \begin{center}
%         \includegraphics[width=0.5\textwidth]{MPNetTwoStreamSelfAttention.png}
%     \end{center}
%     \caption{The MPNet two-stream self-attention architecture illustration. Here, the query stream reuses the hidden value from the content stream for its key-value input. Source: (\citet{song2020mpnet})}\label{fig:MPNetTwoStreamSelfAttention}
% \end{figure}
% \begin{figure}[htbp]
%     \begin{center}
%         \includegraphics[width=0.95\textwidth]{MPNetTwoStreamSelfAttention1.png}
%     \end{center}
%     \caption{Illustration of the attention masks for an example input with 3 masked tokens. Left shows the transformer architecture with light grey attention masks Source: (\citet{song2020mpnet})}\label{fig:MPNetTwoStreamSelfAttention1}
% \end{figure}

% %It was shown in (\citet{song2020mpnet}) that under this unified view, masked language modelling and permutation language modelling both are able to represented in this unified view. As an illustration, take Figure~\ref{fig:mpnet} which shows how, for masked language modelling (Figure~\ref{fig:MPNetMLM}), it is equivalent first permute the input sequence and then mask a portion of the right-most tokens (both $x_2$ and $x_4$). Moreover, for permutation language modelling in Figure~\ref{fig:MPNetPLM} the input sequence is permuted, choosing the right most tokens as the predicted tokens \cite{song2020mpnet}. In either case, the tokens being predicted is the right of some index $c$ (the dotted line in the illustration). 
%
% % Formalising, let $\mathcal{Z_T}$ be some $T$-length set of all $T!$ permutations of the index list $[1,...,T]$. For a given permutation $z = (z_1, ..., z_T)$ we say $x_{(z_i)}$ denotes a token at position $z_i$ (and we denote $z_{<i}$ as all positions before some $1 \le i \le T$). Further, we let $M$ be a set of masked tokens, where $M_{z>c}$ denotes the masked tokens (\texttt{[M]} in Figure~\ref{fig:mpnet}) past the index $c$, mentioned before. Then, for some fixed model parameters $\theta$ we show the objective function in Equation~\ref{eq:mpnet} \cite{song2020mpnet}.
% %
% % \begin{align}
% %     \max_{\theta} \text{  } \mathbb{E}_{z\in\mathcal{Z}_T} \bigl[ \sum_{i=1}^{T} \log p_{\theta} \bigl(x_{z_i}\mid x_{z_{<i}}, M_{z_{>c}}\bigr) \bigr], \label{eq:mpnet}
% % \end{align}
%
% Formalising, let $\mathcal{Z}_T$ be the set of all $T!$ permutations of the
% index list $[1,\dots,T]$.  For a given permutation
% $z=(z_1,\dots,z_T)$ we denote $x_{(z_i)}$ the token at position $z_i$ and
% $z_{<i}$ the prefix positions.  Let $M$ be the set of masked tokens with
% $M_{z>c}$ the subset to the right of index $c$.
% The pre‑training objective is
% \begin{align}
%   \max_{\theta}\,
%   \mathbb{E}_{z\in\mathcal{Z}_T}
%   \Bigl[\sum_{i=1}^{T}\log p_{\theta}(x_{z_i}\mid x_{z_{<i}},M_{z_{>c}})\Bigr],
%   \label{eq:mpnet}
% \end{align}
% identical to Eq. (3) in \citet{song2020mpnet}.  
%
% To illustrate, a 6‑token input
% $(x_1,\dots,x_6)$ permuted as $(x_3,x_5,x_2,x_1,x_4,x_6)$ and masked after
% $c{=}3$ becomes
% $\langle x_3,x_5,x_2,[M],[M],[M],x_1,x_4,x_6\rangle$
% (see Fig.~\ref{fig:mpnet}).  % 🟥 6
% % ➜  Figure can stay if space allows; otherwise appendix.
%
% Clearly, this objective function looks very similar to that of the permutation language modelling. The main difference is the ability to capture the conditioning on the masked tokens, past some index $c$. To demonstrate this, take the following input 6 token input sequence $(x_1,x_2,...,x_6)$. After applying a random permutation, assume we have $(x_3, x_5, x_2, x_1, x_4, x_6)$ and are going to be masking 3 tokens (i.e. $c = 3$). A new input sequence is generated from concatenation of two parts: the non-predicted ($x_{z_{\le c}}$) and predicted ($x_{z_{>c}}$) parts. These parts are concatenated, joined with $M_{z_{>c}}$ masked tokens in between resulting in Equation~\ref{eq:concatmpnet} \cite{song2020mpnet}. \todo{Comment on this and what it basically means.}
% \begin{align}
%     \langle &x_{z_{\le c}}; M_{z_{>c}}; x_{z_{>c}} \rangle \\
%     = \langle &x_3, x_5, x_2, [M], [M], [M], x_1, x_4, x_6 \rangle
%     \label{eq:concatmpnet}
% \end{align}


\subsection{PCA Formalism and SVD Derivation}\label{app:pca}

This appendix provides the formal mathematical details underpinning the conceptual description of Principal Component Analysis (PCA) presented in Section~\ref{sec:pca}, including its optimisation objective and connection to Singular Value Decomposition (SVD) \cite{steinberger2005text}. \\

\noindent If we want to reduce the dimensionality from $n$ features down to $k$ principal components (for $k < n$), we seek a matrix $C \in \mathbb{R}^{n \times k}$ whose columns represent these top $k$ orthogonal component directions (i.e. columns are statistically uncorrelated). The projected data in the lower $k$-dimensional space is given by $XC$. We can approximate the original data $X$ by projecting back from this lower dimension using $XCC^T$. PCA aims to find the matrix $C$ that minimises the difference between the original data $X$ and this reconstructed data $XCC^T$, subject to the constraint that the column-wise components are orthogonal ($C^TC = I$, for the identity matrix $I$) \cite{udell2016generalized}:
\begin{align}
    \min_C \| X - XCC^T \|_F^2
    \label{eq:pca_appendix} % Suggestion: Relabel for appendix context
\end{align}

A standard and efficient method for solving this optimisation problem and finding the principal components is through the Singular Value Decomposition (SVD) of the data matrix $X$ \cite{steinberger2005text, udell2016generalized}. SVD is a fundamental matrix factorisation technique that decomposes $X$ into three matrices:
\begin{align}
    X = USV^T \label{eq:svd_appendix} % Suggestion: Relabel for appendix context
\end{align}
Here, $U \in \mathbb{R}^{m \times k}$ and $V \in \mathbb{R}^{n \times k}$ are matrices with orthogonal columns, $S \in \mathbb{R}^{k \times k}$ is a diagonal matrix (only the diagonal contains non-zero elements) which contains the non-negative singular values ($s_i$) sorted in descending order, and $k$ is the rank of matrix $X$.

The crucial link to PCA is that the columns of the matrix $V$ are precisely the principal component directions we seek. That is, the matrix $C$ in Equation~\ref{eq:pca_appendix}, containing the top $k$ principal components, corresponds to the first $k$ columns of $V$. 

Furthermore, SVD connects directly to the covariance matrix of the data ($X^TX$, assuming $X$ is centered). The decomposition of the covariance matrix is given by:
\begin{align}
    X^TX &= (USV^T)^T (USV^T) \\ % Show transpose step
         &= (VS^T U^T) (USV^T) \\ % Apply transpose rules
         &= V S^T (U^T U) S V^T \\ % Associativity
         &= V S^T I S V^T \quad \text{(since } U^T U = I \text{)} \\
         &= V (S^T S) V^T \\
         &= V S^2 V^T\quad \text{(since } S \text{ is a diagonal matrix)}
         \label{eq:svd_covariance_appendix} % Suggestion: Relabel for appendix context
\end{align}

This is the eigen-decomposition of the covariance matrix $X^TX$. It shows that the columns of $V$ are the eigenvectors (the principal components), and the diagonal entries of $S^2$  are the corresponding eigenvalues (i.e. $s_i^2 = \lambda_i$). Since the eigenvalues represent the variance of the data along the eigenvectors, the squared singular values ($s_i^2$) directly quantify the variance captured by each principal component \cite{udell2016generalized}. Therefore, computing the SVD of the data matrix $X$ provides both the principal components (in $V$) and the measure of variance associated with each component (in $S^2$) without explicitly forming the covariance matrix. Using a truncated SVD (keeping only the top $k$ components corresponding to the largest singular values) gives us a reduction in the dimensions.


\clearpage
\section{k-Medoids - The PAM Algorithm}\label{app:pam}
This section provides a high-level pseudocode of the Partitioning Around Medoids (PAM) algorithm, the core algorithm of the k-Medoids clustering method, as discussed in Section~\ref{sec:kmedoids} \cite{kmedoids}.

\begin{algorithm}[caption={Partitioning Around Medoids (PAM).}, label={alg:pam}]
input: Dataset $X = \{x_1, ..., x_n\}$, integer $k$ (number of clusters)
output: Set of $k$ medoids $M$, Partition $X' = \{C_1, ..., C_k\}$ where $C_j$ is the set of points assigned to medoid $m_j$

begin
    # --- Build Phase (Conceptual) ---
    # Select an initial set of k medoids M from X
    # (i.e. using a heuristic that minimises initial total dissimilarity)
    $M \gets$ SelectInitialMedoids($X, k$)

    # --- Swap Phase ---
    repeat
        best_cost_reduction $\gets 0$
        best_swap $\gets \emptyset$ 
        total_cost $\gets$ ComputeTotalDissimilarity($X, M$) # Cost based on current M

        # Consider swapping each medoid m with each non-medoid x
        foreach $m \in M$ do
            foreach $x \in X \setminus M$ do
                # Compute cost if m is replaced by x
                $M_{swap} \gets (M \setminus \{m\}) \cup \{x\}$
                cost_after_swap $\gets$ ComputeTotalDissimilarity($X, M_{swap}$)
                cost_reduction $\gets$ total_cost - cost_after_swap

                # Check if this swap is the best found so far
                if cost_reduction > best_cost_reduction then
                    best_cost_reduction $\gets$ cost_reduction
                    best_swap $\gets (m, x)$ # Store the potential swap pair
                end
            end
        end

        # Perform the best swap found in this iteration, if any cost reduction occurred
        if best_cost_reduction > 0 then
            ($m_{out}, x_{in}$) $\gets$ best_swap
            $M \gets (M \setminus \{m_{out}\}) \cup \{x_{in}\}$ # Update medoid set
            improved $\gets$ true
        else
            improved $\gets$ false # No cost reduction possible
        end
    until not improved # Stop when no swap improves the total cost

    # Assign points to final medoids to form clusters X'
    $X' \gets$ AssignPointsToClusters($X, M$)

    return M, X'
end
\end{algorithm}

\end{document}
